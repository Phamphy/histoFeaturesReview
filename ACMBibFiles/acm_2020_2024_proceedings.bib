@inproceedings{10.1145/3604078.3604094,
author = {Zhang, Jianyu and Hu, Hexuan and Yang, Tianjin and Hu, Qiang and Yu, Yufeng and Huang, Qian},
title = {HR-ASPP: An improved semantic segmentation model of cervical nucleus images with accurate spatial localization and better shape feature extraction based on Deeplabv3+},
year = {2023},
isbn = {9798400708237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604078.3604094},
doi = {10.1145/3604078.3604094},
abstract = {Cervical cancer, the fourth most common fatal cancer in women, has a considerably increased cure rate if identified and treated at early stages. Computer-aided diagnosis technology is the key for scaling up cervical cancer screening. Computer-processed images of nuclei can help doctors better analyze and diagnose the extent of cancer lesions. However, the lesion characteristics of cervical cancer are mainly reflected in the cell nuclei, which will be significantly larger, with distorted boundary shapes and deepened colors. What's worse is that the nuclei of diseased cells may be hidden in cell clusters, making it difficult for existing medical image segmentation models to achieve satisfactory results. In this paper, based on Deeplabv3+, we propose a HR-ASPP model with accurate spatial localization and better shape feature extraction to segment cervical nuclei. Firstly, HR-ASPP uses HRNet as the backbone network, which has good feature extraction ability for small targets. The structure of parallel multi-resolution and repeated multi-scale fusion makes the target information avoid losing in the deep network, so as to better extract the location information of cell nuclei. Secondly, we devise a lighter weight deformable convolution. The proposed HR-ASPP replaces part of the convolution in HRNet with a lighter weight deformable convolution to learn the edge shape of aberrant nuclei better while reducing the network computation. In addition, HR-ASPP incorporate CARAFE, which is able to aggregate information within a large receptive field and dynamically adapt to the content of a particular instance. The HR-ASPP method proposed in this paper achieves about 50.64%, 84.98% intersection over union (IoU) respectively on our dataset and the public datasets-ISBI. The results indicate that the proposed method achieves the best performance when compared with state-of-the-art approaches for pathologic image segmentation.},
booktitle = {Proceedings of the 15th International Conference on Digital Image Processing},
articleno = {16},
numpages = {8},
location = {<conf-loc>, <city>Nanjing</city>, <country>China</country>, </conf-loc>},
series = {ICDIP '23}
}

@inproceedings{10.1007/978-3-031-43987-2_64,
author = {Sun, Mengxue and Huang, Wenhui and Zheng, Yuanjie},
title = {Instance-Aware Diffusion Model for&nbsp;Gland Segmentation in&nbsp;Colon Histology Images},
year = {2023},
isbn = {978-3-031-43986-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43987-2_64},
doi = {10.1007/978-3-031-43987-2_64},
abstract = {In pathological image analysis, determination of gland morphology in histology images of the colon is essential to determine the grade of colon cancer. However, manual segmentation of glands is extremely challenging and there is a need to develop automatic methods for segmenting gland instances. Recently, due to the powerful noise-to-image denoising pipeline, the diffusion model has become one of the hot spots in computer vision research and has been explored in the field of image segmentation. In this paper, we propose an instance segmentation method based on the diffusion model that can perform automatic gland instance segmentation. Firstly, we model the instance segmentation process for colon histology images as a denoising process based on the diffusion model. Secondly, to recover details lost during denoising, we use Instance Aware Filters and multi-scale Mask Branch to construct global mask instead of predicting only local masks. Thirdly, to improve the distinction between the object and the background, we apply Conditional Encoding to enhance the intermediate features with the original image encoding. To objectively validate the proposed method, we compared state-of-the-art deep learning model on the 2015 MICCAI Gland Segmentation challenge (GlaS) dataset and the Colorectal Adenocarcinoma Gland (CRAG) dataset. The experimental results show that our method improves the accuracy of segmentation and proves the efficacy of the method.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part VI},
pages = {662–672},
numpages = {11},
keywords = {Gland segmentation, Diffusion model, Colon histology images},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43987-2_57,
author = {Chen, Shengcong and Ding, Changxing and Tao, Dacheng and Chen, Hao},
title = {DARC: Distribution-Aware Re-Coloring Model for&nbsp;Generalizable Nucleus Segmentation},
year = {2023},
isbn = {978-3-031-43986-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43987-2_57},
doi = {10.1007/978-3-031-43987-2_57},
abstract = {Nucleus segmentation is usually the first step in pathological image analysis tasks. Generalizable nucleus segmentation refers to the problem of training a segmentation model that is robust to domain gaps between the source and target domains. The domain gaps are usually believed to be caused by the varied image acquisition conditions, e.g., different scanners, tissues, or staining protocols. In this paper, we argue that domain gaps can also be caused by different foreground (nucleus)-background ratios, as this ratio significantly affects feature statistics that are critical to normalization layers. We propose a Distribution-Aware Re-Coloring (DARC) model that handles the above challenges from two perspectives. First, we introduce a re-coloring method that relieves dramatic image color variations between different domains. Second, we propose a new instance normalization method that is robust to the variation in foreground-background ratios. We evaluate the proposed methods on two H &amp;E stained image datasets, named CoNSeP and CPM17, and two IHC stained image datasets, called DeepLIIF and BC-DeepLIIF. Extensive experimental results justify the effectiveness of our proposed DARC model. Codes are available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part VI},
pages = {591–601},
numpages = {11},
keywords = {Domain Generalization, Nucleus Segmentation, Instance Normalization},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43907-0_25,
author = {Yu, Zhimiao and Lin, Tiancheng and Xu, Yi},
title = {SLPD: Slide-Level Prototypical Distillation for&nbsp;WSIs},
year = {2023},
isbn = {978-3-031-43906-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43907-0_25},
doi = {10.1007/978-3-031-43907-0_25},
abstract = {Improving the feature representation ability is the foundation of many whole slide pathological image (WSIs) tasks. Recent works have achieved great success in pathological-specific self-supervised learning (SSL). However, most of them only focus on learning patch-level representations, thus there is still a gap between pretext and slide-level downstream tasks, e.g., subtyping, grading and staging. Aiming towards slide-level representations, we propose Slide-Level Prototypical Distillation (SLPD) to explore intra- and inter-slide semantic structures for context modeling on WSIs. Specifically, we iteratively perform intra-slide clustering for the regions (4096&nbsp;\texttimes{}&nbsp;4096 patches) within each WSI to yield the prototypes and encourage the region representations to be closer to the assigned prototypes. By representing each slide with its prototypes, we further select similar slides by the set distance of prototypes and assign the regions by cross-slide prototypes for distillation. SLPD achieves state-of-the-art results on multiple slide-level benchmarks and demonstrates that representation learning of semantic structures of slides can make a suitable proxy task for WSI analysis. Code will be available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part I},
pages = {259–269},
numpages = {11},
keywords = {Computational pathology, Whole slide images(WSIs), Self-supervised learning},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-12053-4_57,
author = {Zhu, Zheyu and Deng, Ruining and Liu, Quan and Asad, Zuhayr and Cui, Can and Yao, Tianyuan and Huo, Yuankai},
title = {Large-Scale Patch-Wise Pathological Image Feature Dataset with&nbsp;a&nbsp;Hardware-agnostic Feature Extraction Tool},
year = {2022},
isbn = {978-3-031-12052-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-12053-4_57},
doi = {10.1007/978-3-031-12053-4_57},
abstract = {Recent advances in whole slide imaging (WSI) have transformed computer-aided pathological studies from small-scale (e.g., &lt;500 patients) to large-scale (e.g., &gt;10,000 patients). Moreover, a single whole slide image might yield Gigapixel resolution; thus, even basic preprocessing steps, such as foreground segmentation, tiling, and patch-wise feature extraction (e.g., via ImageNet pretrained models), can be computationally expensive. For example, it would take 2,400&nbsp;h to simply obtain patch-level low-dimensional features (e.g., 1D feature with 2048 dimension) from all foreground patches (e.g.,  images) in 10,000 WSI images. In this paper, we present a large-scale patch-wise pathological image feature dataset, covering 14,000 WSIs from TCGA and PAIP cohorts. The contribution of this study is five-fold: (1) We release a foreground patch-level feature dataset, saving 92.1% of storage space and 140 days of computational time; (2) The global spatial location of the patch-level features is provided to aggregate WSI-level results; (3) The feature dataset from two pretrained models (ImageNet and BiT) and two resolutions (1024 and 2048) are evaluated and released for flexible downstream analyses; (4) We containerize the foreground segmentation, tiling, and feature extraction steps as an operating system and hardware agnostic Docker toolkit, called PathContainer, to allow for convenient feature extraction; (5) The entire PathFeature dataset and the PathContainer software have been made publicly available. When performing a standard weakly supervised segmentation method on 940 WSIs, 85.3% of computational time was saved using the PathFeature dataset. The code and data have been made publicly available at .},
booktitle = {Medical Image Understanding and Analysis: 26th Annual Conference, MIUA 2022, Cambridge, UK, July 27–29, 2022, Proceedings},
pages = {778–786},
numpages = {9},
keywords = {Computational pathology, Feature extraction, Weakly supervised learning},
location = {<conf-loc content-type="InPerson">Cambridge, United Kingdom</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16876-5_8,
author = {Liu, Quan and Cui, Can and Deng, Ruining and Asad, Zuhayr and Yao, Tianyuan and Zhu, Zheyu and Huo, Yuankai},
title = {Leverage Supervised and&nbsp;Self-supervised Pretrain Models for&nbsp;Pathological Survival Analysis via&nbsp;a&nbsp;Simple and&nbsp;Low-cost Joint Representation Tuning},
year = {2022},
isbn = {978-3-031-16875-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16876-5_8},
doi = {10.1007/978-3-031-16876-5_8},
abstract = {The large-scale pretrained models from terabyte-level (TB) data are now broadly used in feature extraction, model initialization, and transfer learning in pathological image analyses. Most existing studies have focused on developing more powerful pretrained models, which are increasingly unscalable for academic institutes. Very few, if any, studies have investigated how to take advantage of existing, yet heterogeneous, pretrained models for downstream tasks. As an example, our experiments elucidated that self-supervised models (e.g., contrastive learning on the entire The Cancer Genome Atlas (TCGA) dataset) achieved a superior performance compared with supervised models (e.g., ImageNet pretraining) on a classification cohort. Surprisingly, it yielded an inferior performance when it was translated to a cancer prognosis task. Such a phenomenon inspired us to explore how to leverage the already trained supervised and self-supervised models for pathological survival analysis. In this paper, we present a simple and low-cost joint representation tuning (JRT) to aggregate task-agnostic vision representation (supervised ImageNet pretrained models) and pathological specific feature representation (self-supervised TCGA pretrained models) for downstream tasks. Our contribution is in three-fold: (1) we adapt and aggregate classification-based supervised and self-supervised representation to survival prediction via joint representation tuning, (2) comprehensive analyses on prevalent strategies of pretrained models are conducted, (3) the joint representation tuning provides a simple, yet computationally efficient, perspective to leverage large-scale pretrained models for both cancer diagnosis and prognosis. The proposed JRT method improved the c-index from 0.705 to 0.731 on the TCGA brain cancer survival dataset. The feature-direct JRT (f-JRT) method achieved 60\texttimes{} training speedup while maintaining 0.707 c-index score.},
booktitle = {Resource-Efficient Medical Image Analysis: First MICCAI Workshop, REMIA 2022, Singapore, September 22, 2022, Proceedings},
pages = {75–84},
numpages = {10},
keywords = {Pathology, Prognosis analysis, Representation tuning, Self-supervised learning},
location = {Singapore, Singapore}
}

@inproceedings{10.1109/SMC52423.2021.9658631,
author = {Ren, Yanni and Deng, Hangyu and Jiang, Hao and Zhu, Huilin and Hu, Jinglu},
title = {A Semi-Supervised Classification Method of Apicomplexan Parasites and Host Cell using Contrastive Learning Strategy},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC52423.2021.9658631},
doi = {10.1109/SMC52423.2021.9658631},
abstract = {A common shortfall of supervised learning for medical imaging is the greedy need for human annotations, which is often expensive and time-consuming to obtain. This paper proposes a semi-supervised classification method for three kinds of apicomplexan parasites and non-infected host cells microscopic images, which uses a small number of labeled data and a large number of unlabeled data for training. There are two challenges in microscopic image recognition. The first is that salient structures of the microscopic images are more fuzzy and intricate than natural images’ on a real-world scale. The second is that insignificant textures, like background staining, lightness, and contrast level, vary a lot in samples from different clinical scenarios. To address these challenges, we aim to learn a distinguishable and appearance-invariant representation by contrastive learning strategy. On one hand, macroscopic images, which share similar shape characteristics in morphology, are introduced to contrast for structure enhancement. On the other hand, different appearance transformations, including color distortion and flittering, are utilized to contrast for texture elimination. In the case where only 1% of microscopic images are labeled, the proposed method reaches an accuracy of 94.90% in a generalized testing set.},
booktitle = {2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
pages = {2973–2978},
numpages = {6},
location = {Melbourne, Australia}
}

@inproceedings{10.1007/978-3-030-61377-8_23,
author = {Medeiros, Elias P. and Ferreira, Daniel S. and Ramalho, Geraldo L. B.},
title = {Texture Analysis Based on Structural Co-occurrence Matrix Improves the Colorectal Tissue Characterization},
year = {2020},
isbn = {978-3-030-61376-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61377-8_23},
doi = {10.1007/978-3-030-61377-8_23},
abstract = {Colorectal cancer causes the deaths of thousands of people worldwide according to the World Health Organization. Automatic tissue recognition of histopathological images is essential for early disease diagnosis. Most research consists of employing texture descriptors to capture features that identify tumor samples. However, accurate multi-class classification is a challenge due to the complexity of colorectal tissue images. Recently, researchers have shown that the analysis of texture structural patterns degraded by image filtering provides valuable features for pre-diagnosis in several medical applications. Here we propose an approach to automatically classify eight types of colorectal tissues using Structural Co-occurrence Matrix. We carried on experiments on 5000 tissue patches from a public dataset to evaluate our algorithm, considering two scenarios: structural differences as a single descriptor, and combined with other characteristics. We found that our strategy improves the state-of-the-art, achieving, accuracy: 91.30%, precision: 91.41%, sensitivity: 91.31%, specificity: 98.76% e F1-score: 91.31%.},
booktitle = {Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I},
pages = {333–347},
numpages = {15},
keywords = {Image classification, Structural co-occurrence matrix, Colorectal cancer},
location = {Rio Grande, Brazil}
}

@inproceedings{10.1007/978-3-031-08757-8_23,
author = {Konopka, Aleksandra and Struniawski, Karol and Kozera, Ryszard and Trzci\'{n}ski, Pawe\l{} and Sas-Paszt, Lidia and Lisek, Anna and G\'{o}rnik, Krzysztof and Derkowska, Edyta and G\l{}uszek, S\l{}awomir and Sumorok, Beata and Fra̧c, Magdalena},
title = {Classification of&nbsp;Soil Bacteria Based on&nbsp;Machine Learning and&nbsp;Image Processing},
year = {2022},
isbn = {978-3-031-08756-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08757-8_23},
doi = {10.1007/978-3-031-08757-8_23},
abstract = {Soil bacteria play a fundamental role in plant growth. This paper focuses on developing and testing some techniques designed to identify automatically such microorganisms. More specifically, the recognition performed here deals with the specific five genera of soil bacteria. Their microscopic images are classified with machine learning methods using shape and image texture descriptors. Feature determination based on shape relies on interpolation and curvature estimation whereas feature recognition based on image texture resorts to the spatial relationships between chrominance and luminance of pixels using co-occurrence matrices. From the variety of modelling methods applied here the best reported result amounts to 97% of accuracy. This outcome is obtained upon incorporating the set of features from both groups and subsequently merging classification and feature selection methods: Extreme Learning Machine - Radial Basis Function with Sparse Multinomial Logistic Regression with Bayesian Regularization and also k-Nearest Neighbors classifier with Fast Correlation Based Filter. The optimal parameters involved in merged classifiers are obtained upon computational testing and simulation.},
booktitle = {Computational Science – ICCS 2022: 22nd International Conference, London, UK, June 21–23, 2022, Proceedings, Part III},
pages = {263–277},
numpages = {15},
keywords = {Computational optimization, Modelling and simulation, Spline interpolation, Shape and image texture extraction, Image analysis, Machine learning, Soil bacteria},
location = {London, United Kingdom}
}

@inproceedings{10.1007/978-3-030-61598-7_15,
author = {Liang, Hanwen and Plataniotis, Konstantinos N. and Li, Xingyu},
title = {Stain Style Transfer of Histopathology Images via Structure-Preserved Generative Learning},
year = {2020},
isbn = {978-3-030-61597-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61598-7_15},
doi = {10.1007/978-3-030-61598-7_15},
abstract = {Computational histopathology image diagnosis becomes increasingly popular and important. While pathologists do not struggle with color variations in slides, computational solutions usually suffer from this critical issue. In this regard, this study proposes two stain style transfer models, SSIM-GAN and DSCSI-GAN, based on the generative adversarial networks. By cooperating structural preservation metrics and feedback of an auxiliary diagnosis net in learning, medical-relevant information presented by image texture, structure, and chroma-contrast features is preserved in color-normalized images. Particularly, the smart treat of chromatic image content in our DSCSI-GAN model helps to achieve noticeable normalization improvement in image regions where stains mix due to histological substances co-localization. Extensive experimentation on public histopathology image sets indicates that our methods outperform prior arts in terms of generating more stain-consistent images, better preserving histological information in images, and obtaining significantly higher learning efficiency. Our python implementation is published on .},
booktitle = {Machine Learning for Medical Image Reconstruction: Third International Workshop, MLMIR 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 8, 2020, Proceedings},
pages = {153–162},
numpages = {10},
keywords = {Computational histopathology, Structural similarity, Color normalization, Generative model, Stain style transfer},
location = {Lima, Peru}
}

@inproceedings{10.1145/3528233.3530749,
author = {Huang, Weizhen and Merzbach, Sebastian and Callenberg, Clara and Stavenga, Doekele and Hullin, Matthias},
title = {Rendering Iridescent Rock Dove Neck Feathers},
year = {2022},
isbn = {9781450393379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528233.3530749},
doi = {10.1145/3528233.3530749},
abstract = {Bird feathers exhibit fascinating reflectance, which is governed by fiber-like structures. Unlike hair and fur, the feather geometric structures follow intricate hierarchical patterns that span many orders of magnitude in scale. At the smallest scales, fiber elements have strongly non-cylindrical cross-sections and are often complemented by regular nanostructures, causing rich structural color. Therefore, past attempts to render feathers using fiber- or texture-based appearance models missed characteristic aspects of the visual appearance. We introduce a new feather modeling and rendering framework, which abstracts the microscopic geometry and reflectance into a microfacet-like BSDF. The R, TRT and T lobes, also known from hair and fur, here account for specular reflection off the cortex, diffuse reflection off the medulla, and transmission due to barbule spacing, respectively. Our BSDF, which does not require precomputation or storage, can be efficiently importance-sampled and readily integrated into rendering pipelines that represent feather geometry down to the barb level. We verify our approach using a BSDF-capturing setup for small biological structures, as well as against calibrated photographs of rock dove neck feathers.},
booktitle = {ACM SIGGRAPH 2022 Conference Proceedings},
articleno = {43},
numpages = {8},
keywords = {rock dove, iridescence, Feather, BSDF},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH '22}
}

@inproceedings{10.1007/978-3-030-76352-7_45,
author = {Vaishali, Durgamahanthi and Priya, P. Vishnu and Govind, Nithyasri and Prabha, K. Venkat Ratna},
title = {Higher Order Statistical Analysis in Multiresolution Domain - Application to Breast Cancer Histopathology},
year = {2020},
isbn = {978-3-030-76351-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-76352-7_45},
doi = {10.1007/978-3-030-76352-7_45},
abstract = {Objective is to analyze textures in breast histopathology images for cancer diagnosis.Background: It is observed that breast cancer has second highest mortality rate in women. Detection of cancer in early stages can give more treatment options and thus reduce the mortality rate. In cancer diagnosis using histopathology images, histologists examine biopsy samples based on cell morphology, tissue distribution, randomness in their growth or placements. These methods are time taking and sometime leads to incorrect diagnosis. These methods are highly subjective/arbitrary. The new techniques use computers, archived data and standard algorithms to provide fast and accurate results.Material &amp; Methods: In this work we have proposed a multiresolution statistical model in wavelet domain. The primary idea is to study complex random field of histopathology images which contain long–range and nonlinear spatial interactions in wavelet domain. This model emphasizes the contribution of Gray level Run Length Matrix (GLRLM) and related higher order statistical features in wavelet subbands. The image samples are taken from ‘BreaKhis’ database. The standard database generated in collaboration with the P&amp;D Laboratory—Pathological Anatomy and Cytopathology, Parana, Brazil. This study has been designed for breast cancer histopathology images of ductal carcinoma. GLRLM feature dataset further classified by SVM classifier with linear kernel. The classification accuracies of signal resolution and multiresolution have been compared.Results: The results show that the GLRLM based features provides exceptional distinguishing features for multiresolution analysis of histopathology images. Apart from recent deep learning method this study proposes use of higher order statistics to gain stronger image features. These features carry inherent discriminative properties. This higher order statistical model will be suitable for cancer detection.Conclusion: This work proposes automated diagnosis. Tumor spatial heterogeneity is the main concern in analyzing, diagnosing and grading cancer. This model focuses on Long range spatial dependencies in heterogeneous spatial process and offers solutions for accurate classification in two class problems. The work describes an innovative way of using GLRLM based textural features to extract underlying information in breast cancer images.},
booktitle = {Service-Oriented Computing  – ICSOC 2020 Workshops: AIOps, CFTIC, STRAPS, AI-PA, AI-IOTS, and Satellite Events, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {495–508},
numpages = {14},
keywords = {Wavelet transforms, Multiresolution analysis, Texture analysis, SVM, Support vector machine, GLRLM, Grey Level Run Length Matrix, CAD, Computer assisted diagnostics},
location = {Dubai, United Arab Emirates}
}

@inproceedings{10.1145/3570773.3570843,
author = {Chen, Hao and Tang, Wending and Tao, Ran},
title = {An adaptive weight texture classification method based on Local Binary Pattern Variance},
year = {2022},
isbn = {9781450398442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570773.3570843},
doi = {10.1145/3570773.3570843},
abstract = {The extraction of local texture information using the traditional Local Binary Mode (LBP) is limited, and it ignores the representation of global texture information, which leads to an unsatisfactory outcome for the texture classification task. Local Binary Mode (LBP) has been widely used in texture classification. This paper utilizes LBPV to resolve this issue (Local Binary Pattern Variance) and proposes a novel adaptive weight joint multi-scale LBPV2 texture picture classification algorithm. The typical variance weight is replaced by the square of covariance as the cumulative weight of the histogram in this method, and the multi-scale texture information is retrieved using an adaptive weight and multi-scale scheme. Thus, the texture classification performance is further improved. Simulation experiments on the commonly used Outex reference texture database show that the proposed adaptive weight combined with multi-scale LBPV2 can significantly improve the performance of texture classification.In the fields of computer vision and pattern recognition, texture analysis is a fundamental visual problem with a wide range of applications, including object detection, remote sensing, content-based image retrieval, and medical picture analysis.For various research questions, numerous academics have put forth various LBP versions in recent years. The dominant LBP [2] model was proposed by Liao et al. in 2009, and it was empirically chosen as the best model out of all the models. Guo et al. proposed LBPV [3], which expresses local contrast information into the straight square of texture images using a local variance confidence and global matching scheme. In order to increase classification performance, the author also proposed a Completed Local Binary Pattern (CLBP) [4] in the same year. This pattern combines three complimentary groups—CLBPS, CLBPM, and CLBPC—using a combined probability distribution. To enhance the traditional local binary pattern's noise resistance and texture expression, Liu et al. proposed the extension of LBP [5] in 2012. Relevant scholars developed a pixel block sampling structure and local neighborhood intensity relationship model for texture classification in 2013 on the basis of conventional LBP [6], and produced notable results. More recently, in 2017, a multiscale LBP [7] was presented, transcending the constraints of conventional LBP representation and not only reflecting the microscopic texture structure but also effectively expressing the macroscopic texture structure of bigger areas.Although LBP and its variations have produced a number of remarkable texture classification results, there are still a lot of possible shortcomings. The expression of nearly entirely lost global information, for instance, results in inadequate texture classification results because most LBP versions are only capable of representing local texture information [1]. In order to significantly improve texture performance, this paper proposed a new method of adaptive weight joint multi-scale LBPV2 for classifying texture images. It did this by introducing adaptive weight classification and by replacing the variance weight in the LBPV calculation method with the variance square as the cumulative weight of the histogram.},
booktitle = {Proceedings of the 3rd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {400–403},
numpages = {4},
location = {<conf-loc>, <city>Amsterdam</city>, <country>Netherlands</country>, </conf-loc>},
series = {ISAIMS '22}
}

@inproceedings{10.1007/978-3-031-34048-2_58,
author = {Kofler, Florian and Shit, Suprosanna and Ezhov, Ivan and Fidon, Lucas and Horvath, Izabela and Al-Maskari, Rami and Li, Hongwei Bran and Bhatia, Harsharan and Loehr, Timo and Piraud, Marie and Erturk, Ali and Kirschke, Jan and Peeken, Jan C. and Vercauteren, Tom and Zimmer, Claus and Wiestler, Benedikt and Menze, Bjoern},
title = {blob loss: Instance Imbalance Aware Loss Functions for&nbsp;Semantic Segmentation},
year = {2023},
isbn = {978-3-031-34047-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-34048-2_58},
doi = {10.1007/978-3-031-34048-2_58},
abstract = {Deep convolutional neural networks (CNN) have proven to be remarkably effective in semantic segmentation tasks. Most popular loss functions were introduced targeting improved volumetric scores, such as the Dice coefficient (DSC). By design, DSC can tackle class imbalance, however, it does not recognize instance imbalance within a class. As a result, a large foreground instance can dominate minor instances and still produce a satisfactory DSC. Nevertheless, detecting tiny instances is crucial for many applications, such as disease monitoring. For example, it is imperative to locate and surveil small-scale lesions in the follow-up of multiple sclerosis patients. We propose a novel family of loss functions, blob loss, primarily aimed at maximizing instance-level detection metrics, such as F1 score and sensitivity. Blob loss is designed for semantic segmentation problems where detecting multiple instances matters. We extensively evaluate a DSC-based blob loss in five complex 3D semantic segmentation tasks featuring pronounced instance heterogeneity in terms of texture and morphology. Compared to soft Dice loss, we achieve 5% improvement for MS lesions, 3% improvement for liver tumor, and an average 2% improvement for microscopy segmentation tasks considering F1 score.},
booktitle = {Information Processing in Medical Imaging: 28th International Conference, IPMI 2023, San Carlos de Bariloche, Argentina, June 18–23, 2023, Proceedings},
pages = {755–767},
numpages = {13},
keywords = {lightsheet microscopy, multiple sclerosis, instance imbalance awareness, semantic segmentation loss function},
location = {San Carlos de Bariloche, Argentina}
}

@inproceedings{10.1007/978-3-031-57793-2_34,
author = {Marziali, Sara and Nunziati, Giacomo and Prete, Alessia Lucia and Niccolai, Neri and Brunetti, Sara and Bianchini, Monica},
title = {A Discrete Geometry Method for&nbsp;Atom Depth Computation in&nbsp;Complex Molecular Systems},
year = {2024},
isbn = {978-3-031-57792-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-57793-2_34},
doi = {10.1007/978-3-031-57793-2_34},
abstract = {The field of structural biology is rapidly advancing thanks to significant improvements in X-ray crystallography, nuclear magnetic resonance (NMR), cryo-electron microscopy, and bioinformatics. The identification of structural descriptors allows for the correlation of functional properties with characteristics such as accessible molecular surfaces, volumes, and binding sites. Atom depth has been recognized as an additional structural feature that links protein structures to their folding and functional properties. In the case of proteins, the atom depth is typically defined as the distance between the atom and the nearest surface point or nearby water molecule.In this paper, we propose a discrete geometry method to calculate the depth index with an alternative approach that takes into account the local molecular shape of the protein. To compute atom depth indices, we measure the volume of the intersection between the molecule and a sphere with an appropriate reference radius, centered on the atom for which we want to quantify the depth.We validate our method on proteins of diverse shapes and sizes and compare it with metrics based on the distance to the nearest water molecule from bulk solvent to demonstrate its effectiveness.},
booktitle = {Discrete Geometry and Mathematical Morphology: Third International Joint Conference, DGMM 2024, Florence, Italy, April 15–18, 2024, Proceedings},
pages = {443–455},
numpages = {13},
keywords = {Morphological Algorithms, Voxel Model, Structural biology, Atom depth, Molecular shape, Depth index, Discrete Geometry},
location = {<conf-loc content-type="InPerson">Florence, Italy</conf-loc>}
}

@inproceedings{10.1007/978-3-030-87586-2_1,
author = {Ravikumar, Sadhana and Wisse, Laura and Lim, Sydney and Irwin, David and Ittyerah, Ranjit and Xie, Long and Das, Sandhitsu R. and Lee, Edward and Tisdall, M. Dylan and Prabhakaran, Karthik and Detre, John and Mizsei, Gabor and Trojanowski, John Q. and Robinson, John and Schuck, Theresa and Grossman, Murray and Artacho-P\'{e}rula, Emilio and de Onzo\~{n}o Martin, Maria Mercedes I\~{n}iguez and del Mar Arroyo Jim\'{e}nez, Mar\'{\i}a and Mu\~{n}oz, Monica and Romero, Francisco Javier Molina and del Pilar Marcos Rabal, Maria and S\'{a}nchez, Sandra Cebada and Gonz\'{a}lez, Jos\'{e} Carlos Delgado and de la Rosa Prieto, Carlos and Parada, Marta C\'{o}rcoles and Wolk, David and Insausti, Ricardo and Yushkevich, Paul},
title = {Unfolding the Medial Temporal Lobe Cortex to Characterize Neurodegeneration Due to Alzheimer’s Disease Pathology Using Ex vivo Imaging},
year = {2021},
isbn = {978-3-030-87585-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87586-2_1},
doi = {10.1007/978-3-030-87586-2_1},
abstract = {Neurofibrillary tangle (NFT) pathology in the medial temporal lobe (MTL) is closely linked to neurodegeneration, and is the early pathological change associated with Alzheimer’s Disease (AD). In this work, we investigate the relationship between MTL morphometry features derived from high-resolution ex vivo imaging and histology-based measures of NFT pathology using a topological unfolding framework applied to a dataset of 18 human postmortem MTL specimens. The MTL has a complex 3D topography and exhibits a high degree of inter-subject variability in cortical folding patterns which poses a significant challenge for volumetric registration methods typically used during MRI template construction. By unfolding the MTL cortex, the proposed framework explicitly accounts for the sheet-like geometry of the MTL cortex and provides a two-dimensional reference coordinate space which can be used to implicitly register cortical folding patterns across specimens based on distance along the cortex despite large anatomical variability. Leveraging this framework in a subset of 15 specimens, we characterize the associations between NFTs and morphological features such as cortical thickness and surface curvature and identify regions in the MTL where patterns of atrophy are strongly correlated with NFT pathology.},
booktitle = {Machine Learning in Clinical Neuroimaging: 4th International Workshop, MLCN 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {3–12},
numpages = {10},
keywords = {Cortical unfolding, Ex vivo MRI, Medial temporal lobe},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87237-3_48,
author = {Wang, Zhi and Zhu, Xiaoya and Su, Lei and Meng, Gang and Zhang, Junsheng and Li, Ao and Wang, Minghui},
title = {Instance-Aware Feature Alignment for Cross-Domain Cell Nuclei Detection in Histopathology Images},
year = {2021},
isbn = {978-3-030-87236-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87237-3_48},
doi = {10.1007/978-3-030-87237-3_48},
abstract = {Robust nuclei detection is crucial prerequisite for histologic characteristics of nuclei that can assist various clinical tasks such as disease diagnosis and cancer grading. Despite of their success, most existing nuclei detection methods ignore the case where the testing (target) domain has different data distribution with the training (source) domain, which is known as the problem of domain shift. In fact, the domain shift problem is prevalent in histopathology images due to various reasons such as different staining procedures and organ specific nuclear morphology. Thus, the performance of a nuclei detection model in the source domain will be hurt if it is directly applied to the target domain. To address this problem, we propose a novel instance-aware domain adaption framework for nuclei detection in histopathology images, which includes both image-level alignment (IMA) and instance-level alignment (INA) components to minimize the domain shift. Especially, INA component extracts instance-level features by using nuclei locations as the guidance and effectively aligns the instance-level features via adversarial training. Furthermore, to facilitate instance-level feature alignment, a Temporal Ensembling based Nuclei Localization (TENL) module is introduced in INA component to automatically generate candidate nuclei locations in the target domain. We evaluate the proposed method on different benchmark settings and obtain remarkable improvements compared to existing methods on the challenging problem of cross-domain cell nuclei detection.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part VIII},
pages = {499–508},
numpages = {10},
keywords = {Domain adaptation, Digital pathology, Nuclei detection},
location = {Strasbourg, France}
}

@inproceedings{10.1145/3397391.3397406,
author = {Hortinela, Carlos C. and Fausto, Janette C. and Valiente, Flordeliza L. and Divina, Paul Daniel C. and Felices, John Philip T.},
title = {Development of Abnormal Red Blood Cells Classifier Using Image Processing Techniques with Support Vector Machine},
year = {2020},
isbn = {9781450377249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397391.3397406},
doi = {10.1145/3397391.3397406},
abstract = {In some studies conducted from the medical field, the abnormality in the morphological structure of red blood cells is used as an indicator of blood related diseases and conditions based on the classification of the abnormal red blood cells that have been found. The system provides an automated way of identification and classification of abnormal red blood cell based on morphology. This is done by capturing the microscopic image of a blood smear and applying image processing techniques. The captured image undergoes several processes such as image pre-processing, segmentation, and feature extraction. The system acquires important morphological properties of red blood cells through these processes. Properties such as diameter, shape geometric factor, central pallor and target flag are the main parameters used by the system. Support vector machine is applied as classifier to identify the type of the abnormal cell. The SVM will be able to do the classification by training it with dataset for each class of red blood cell included in the system. These datasets are images of the blood cells that have parameters within the range of a specific class. The system also provides a list of blood related diseases and condition associated with the abnormal red blood cells found to be present. Based on the result of the testing, the system was able to produce a high accuracy output of 96.67% for normal RBC, 100% for echinocytes, 97.5% for elliptocytes, 97.5% for dacrocytes, 98.33% for spherocytes, 97.5% for Target cells, 98.53% for stomatocytes and 86.67% for unknown class.},
booktitle = {Proceedings of the 2020 10th International Conference on Biomedical Engineering and Technology},
pages = {17–21},
numpages = {5},
keywords = {Support vector machine, Red blood cells, Morphological properties, Image processing, Blood smear},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {ICBET '20}
}

@inproceedings{10.1007/978-3-030-66415-2_20,
author = {Falkenstein, Brian and Kovashka, Adriana and Hwang, Seong Jae and Chennubhotla, S. Chakra},
title = {Classifying Nuclei Shape Heterogeneity in Breast Tumors with Skeletons},
year = {2020},
isbn = {978-3-030-66414-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66415-2_20},
doi = {10.1007/978-3-030-66415-2_20},
abstract = {In this study, we demonstrate the efficacy of scoring statistics derived from a medial axis transform, for differentiating tumor and non-tumor nuclei, in malignant breast tumor histopathology images. Characterizing nuclei shape is a crucial part of diagnosing breast tumors for human doctors, and these scoring metrics may be integrated into machine perception algorithms which aggregate nuclei information across a region to label whole breast lesions. In particular, we present a low-dimensional representation capturing characteristics of a skeleton extracted from nuclei. We show that this representation outperforms both prior morphological features, as well as CNN features, for classification of tumors. Nuclei and region scoring algorithms such as the one presented here can aid pathologists in the diagnosis of breast tumors.},
booktitle = {Computer Vision – ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part I},
pages = {310–323},
numpages = {14},
keywords = {Computer vision, Digital pathology, Breast cancer, Medial axis transform},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3451421.3451434,
author = {Li, Zihan and Li, Chen and Zhang, Jinghua and Xu, Hao and Yuan, Huaqian and Zhu, Xuemin and Lu, Bolin},
title = {A New Microorganism Dataset for Image Segmentation and Classification Evaluation},
year = {2021},
isbn = {9781450389686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451421.3451434},
doi = {10.1145/3451421.3451434},
abstract = {Environmental Microorganism Data Set Fifth Version (EMDS-5) is a microscopic image dataset including original Environmental Microorganism (EM) images and two sets of Ground Truth (GT) images. The GT image sets include a single-object GT image set and a multi-object GT image set. The EMDS-5 dataset has 21 types of EMs, each of which contains 20 original EM images, 20 single-object GT images and 20 multi-object GT images. EMDS-5 can realize to evaluate image preprocessing, image segmentation, feature extraction, image classification and image retrieval functions. We select the most representative algorithms and price indicators for testing and evaluation. The image preprocessing functions contain two parts: image denoising and image edge detection. Image segmentation includes single-object image segmentation and multi-object image segmentation. We extract nine features from the images in EMDS-5 and use the Support Vector Machine classifier for testing. In terms of image classification, we select the VGG16 feature to test different classifiers. We test two types of retrieval approaches: texture feature retrieval and deep learning feature retrieval.},
booktitle = {The Fourth International Symposium on Image Computing and Digital Medicine},
pages = {53–58},
numpages = {6},
keywords = {Microscopic Image, Image Segmentation, Image Retrieval, Image Feature Extraction, Image Dataset, Image Classification, Environmental Microorganism},
location = {Shenyang, China},
series = {ISICDM 2020}
}

@inproceedings{10.1007/978-3-031-44917-8_24,
author = {Kataria, Tushar and Knudsen, Beatrice and Elhabian, Shireen},
title = {To Pretrain or&nbsp;Not to&nbsp;Pretrain? A&nbsp;Case Study of&nbsp;Domain-Specific Pretraining for&nbsp;Semantic Segmentation in&nbsp;Histopathology},
year = {2023},
isbn = {978-3-031-47196-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44917-8_24},
doi = {10.1007/978-3-031-44917-8_24},
abstract = {Annotating medical imaging datasets is costly, so fine-tuning (or transfer learning) is the most effective method for digital pathology vision applications such as disease classification and semantic segmentation. However, due to texture bias in models trained on real-world images, transfer learning for histopathology applications might result in underperforming models, which necessitates the need for using unlabeled histopathology data and self-supervised methods to discover domain-specific characteristics. Here, we tested the premise that histopathology-specific pretrained models provide better initializations for pathology vision tasks, i.e., gland and cell segmentation. In this study, we compare the performance of gland and cell segmentation tasks with histopathology domain-specific and non-domain-specific (real-world images) pretrained weights. Moreover, we investigate the dataset size at which domain-specific pretraining produces significant gains in performance. In addition, we investigated whether domain-specific initialization improves the effectiveness of out-of-distribution testing on distinct datasets but the same task. The results indicate that performance gain using domain-specific pretrained weights depends on both the task and the size of the training dataset. In instances with limited dataset sizes, a significant improvement in gland segmentation performance was also observed, whereas models trained on cell segmentation datasets exhibit no improvement. .},
booktitle = {Medical Image Learning with Limited and Noisy Data: Second International Workshop, MILLanD 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 8, 2023, Proceedings},
pages = {246–256},
numpages = {11},
keywords = {Transfer Learning, Gland and Cell Segmentation, Domain Specific pretraining},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.1007/978-3-031-43996-4_39,
author = {Sommersperger, Michael and Dehghani, Shervin and Matten, Philipp and Mach, Kristina and Nasseri, M. Ali and Roodaki, Hessam and Eck, Ulrich and Navab, Nassir},
title = {Semantic Virtual Shadows (SVS) for&nbsp;Improved Perception in&nbsp;4D OCT Guided Surgery},
year = {2023},
isbn = {978-3-031-43995-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43996-4_39},
doi = {10.1007/978-3-031-43996-4_39},
abstract = {Swept-Source Optical Coherence Tomography (SS-OCT) integrated with surgical microscopes has enabled fast, high-resolution, and volumetric visualization of delicate tissue-instrument interactions. However, some visual features, which provide essential perceptual information in microscopic surgery, are not present in 4D OCT. Such a feature is the shadow of the surgical instruments cast onto the retina by the endo-illumination probe, which is among the most important cognitive cues for perceptual distance estimation. In this work, we propose Semantic Virtual Shadows (SVS), a novel concept to artificially generate instrument-specific shadows in OCT volumes, enabling naturally non-existent but important perceptual cues that are present in microscopic surgery. Semantic scene information is leveraged by considering only voxels associated with shadow-casting and shadow-receiving objects, identified using a learning-based approach and efficient volume processing, respectively. Real-time performance is achieved by a precomputed semantic shadow volume texture that assigns a shadowing factor to each voxel associated with a shadow-receiving object. The novelty of the method includes not only instrument-specific shadowing on the surface anatomy but also exclusively on deep-seated subsurface structures, providing advantages for various vitreoretinal procedures. Our user study indicates the benefits of the method for 4D OCT-guided surgery in several cognitive and performance-specific aspects.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part IX},
pages = {408–417},
numpages = {10},
keywords = {Optical Coherence Tomography, Real-Time Visualization, Visual Perception},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-030-59006-2_15,
author = {Roszkowiak, Lukasz and Zak, Jakub and Siemion, Krzysztof and Pijanowska, Dorota and Korzynska, Anna},
title = {Nuclei Detection with Local Threshold Processing in DAB&amp;H Stained Breast Cancer Biopsy Images},
year = {2020},
isbn = {978-3-030-59005-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59006-2_15},
doi = {10.1007/978-3-030-59006-2_15},
abstract = {Histopathological sections allow pathologists to evaluate a wide range of specimens, including breast cancer, obtained from biopsies and surgical procedures. The accuracy of the employed automated cell detection technique is critical in obtaining efficient diagnostic performance. In this paper we investigate 18 different adaptive threshold methods based on various approaches. We validate the methods on a set of histopathological images of breast cancer, where immunohistochemical staining of FOXP3 was performed with 3,3’-diaminobenzidine and hematoxylin. The thresholding is performed on monochromatic images derived from original images: separate channels of Red-Green-Blue and Hue-Saturation-Value, layers of results of color deconvolution, ‘brown’ channel, ‘blue-ratio’ layer. The main objective of the evaluation is to determine if the detected objects obtained by the tested methods of thresholding are consistent with the manually labeled ones. The performance is evaluated using precision, sensitivity and F1 score measures. It appears that satisfactory results were achieved only by 6 methods. It was found that bradley method is the best performing method for nuclei detection in this type of stained tissue samples. It has best sensitivity value for images after color deconvolution and Value layer (of Hue-Saturation-Value color space), 0.970 and 0.975 respectively. As a result, we recommend a most efficient local threshold technique in the case of nuclei detection in digitized immunohistochemically stained tissue sections. This initial detection of objects followed by texture, size and shape analysis will give a collection of cells’ nuclei to perform further accurate segmentation. The proposed detection method will be used in a framework focused on computer-aided diagnosis.},
booktitle = {Computer Vision and Graphics: International Conference, ICCVG 2020, Warsaw, Poland, September 14–16, 2020, Proceedings},
pages = {164–175},
numpages = {12},
keywords = {Breast cancer, Biomedical engineering, Digital pathology, Histopathology},
location = {Warsaw, Poland}
}

@inproceedings{10.1007/978-3-030-51002-2_15,
author = {Jirik, Miroslav and Moulisova, Vladimira and Schindler, Claudia and Cervenkova, Lenka and Palek, Richard and Rosendorf, Jachym and Arlt, Janine and Bolek, Lukas and Dejmek, Jiri and Dahmen, Uta and Jirikova, Kamila and Gruber, Ivan and Liska, Vaclav and Zelezny, Milos},
title = {MicrAnt: Towards Regression Task Oriented Annotation Tool for Microscopic Images},
year = {2020},
isbn = {978-3-030-51001-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-51002-2_15},
doi = {10.1007/978-3-030-51002-2_15},
abstract = {Annotating a dataset for training a Supervised Machine Learning algorithm is time and annotator’s attention intensive. Our goal was to create a tool that would enable us to create annotations of the dataset with minimal demands on expert’s time. Inspired by applications such as Tinder, we have created an annotation tool for describing microscopic images. A graphical user interface is used to select from a couple of images the one with the higher value of the examined parameter. Two experiments were performed. The first compares the speed of annotation of our application with the commonly used tool for processing microscopic images. In the second experiment, the texture description was compared with the annotations from MicrAnt application and commonly used application. The results showed that the processing time using our application is 3 times lower and the Spearman coefficient increases by 0.05 than using a commonly used application. In an experiment, we have shown that the annotations processed using our application increase the correlation of the studied parameter and texture descriptors compared with manual annotations


.},
booktitle = {Combinatorial Image Analysis: 20th International Workshop, IWCIA 2020, Novi Sad, Serbia, July 16–18, 2020, Proceedings},
pages = {209–218},
numpages = {10},
keywords = {Decellularization, Liver, Scaffold, Annotation, Microscopy},
location = {Novi Sad, Serbia}
}

@inproceedings{10.1007/978-981-99-8558-6_11,
author = {Zhong, Yunpeng and Li, Xiangru and Mei, Huanyu and Xiong, Shengchun},
title = {Probability-Based Nuclei Detection and&nbsp;Critical-Region Guided Instance Segmentation},
year = {2023},
isbn = {978-981-99-8557-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-8558-6_11},
doi = {10.1007/978-981-99-8558-6_11},
abstract = {Nuclear instance segmentation in histopathological images is a key procedure in pathological diagnosis. In this regard, a typical class of solutions are deep learning-like nuclear instance segmentation methods, especially those based on the detection of nuclear critical regions. The existing instance segmentation methods based on nuclear critical regions are still insufficient in detecting and segmenting adhesion nuclei. In this study, we proposed a Critical-Region Guided Instance Segmentation (CGIS) method to precisely segment adhesion nuclei. Specifically, CGIS embed the critical region of an instance into the original image to guide the model to segment only the target instance, and provide an accurate segmentation mask for each nucleus. To improve the accuracy of critical region detection in CGIS, we proposed a boundary-insensitive feature, Central Probability Field (CPF). The CPF is calculated based on global morphological characteristics of nuclei, and can reduce the negative effects from the unreliable nuclear boundary details on the nuclear critical regions detection. We evaluated the effectiveness of the proposed CGIS and CPF feature on the Lizard and PanNuke datasets. It is shown that the proposed CGIS method can accurately segment adhesion nuclei, and the CPF feature can efficiently detect the critical regions of nuclei.},
booktitle = {Pattern Recognition and Computer Vision: 6th Chinese Conference, PRCV 2023, Xiamen, China, October 13–15, 2023, Proceedings, Part XIII},
pages = {122–135},
numpages = {14},
keywords = {Critical region, Computational pathology, Nuclear instance segmentation},
location = {<conf-loc content-type="InPerson">Xiamen, China</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43987-2_71,
author = {Shi, Lulin and Zhang, Yan and Wong, Ivy H. M. and Lo, Claudia T. K. and Wong, Terence T. W.},
title = {MulHiST: Multiple Histological Staining for&nbsp;Thick Biological Samples via&nbsp;Unsupervised Image-to-Image Translation},
year = {2023},
isbn = {978-3-031-43986-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43987-2_71},
doi = {10.1007/978-3-031-43987-2_71},
abstract = {The conventional histopathology paradigm can provide the gold standard for clinical diagnosis, which, however, suffers from lengthy processing time and requires costly laboratory equipment. Recent advancements made in deep learning for computational histopathology have sparked lots of efforts in achieving a rapid chemical-free staining technique. Yet, existing approaches are limited to well-prepared thin sections, and invalid in handling more than one stain. In this paper, we present a multiple histological staining model for thick tissues (MulHiST), without any laborious sample preparation, sectioning, and staining process. We use the grey-scale light-sheet microscopy image of thick tissues as model input and transfer it into different histologically stained versions, including hematoxylin and eosin (H&amp;E), Masson’s trichrome (MT), and periodic acid-Schiff (PAS). This is the first work that enables the automatic and simultaneous generation of multiple histological staining for thick biological samples. Moreover, we empirically demonstrate that the AdaIN-based generator offers an advantage over other configurations to achieve higher-quality multi-style image generation. Extensive experiments also indicated that multi-domain data fusion is conducive to the model capturing shared pathological features. We believe that the proposed MulHiST can potentially be applied in clinical rapid pathology and will significantly improve the current histological workflow.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part VI},
pages = {735–744},
numpages = {10},
keywords = {Virtual staining, Multi-domain image translation, Thick tissues, Light-sheet microscopy},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-34960-7_15,
author = {Kee, Shaira L. and Sy, Michael Aaron G. and Border, Samuel P. and Lucarelli, Nicholas J. and Gupta, Akshita and Sarder, Pinaki and Masalunga, Marvin C. and Tan, Myles Joshua T.},
title = {Predicting Papillary Renal Cell Carcinoma Prognosis Using Integrative Analysis of Histopathological Images and Genomic Data},
year = {2023},
isbn = {978-3-031-34959-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-34960-7_15},
doi = {10.1007/978-3-031-34960-7_15},
abstract = {Renal cell carcinoma (RCC) is a common malignant tumor of the adult kidney, with the papillary subtype (pRCC) as the second most frequent. There is a need to improve evaluative criteria for pRCC due to overlapping diagnostic characteristics in RCC subtypes. To create a better prognostic model for pRCC, we proposed an integration of morphologic and genomic features. Matched images and genomic data from The Cancer Genome Atlas were used. Image features were extracted using CellProfiler, and prognostic image features were selected using least absolute shrinkage and selection operator and support vector machine algorithms. Eigengene modules were identified using weighted gene co-expression network analysis. Risk groups based on prognostic features were significantly distinct (p &lt; 0.05) according to Kaplan-Meier analysis and log-rank test results. We used two image features and nine eigengene modules to construct a model with the Random Survival Forest method, measuring 11-, 16-, and 20-month areas under the curve (AUC) of a time-dependent receiver operating curve. The integrative model (AUCs: 0.877, 0.769, and 0.811) outperformed models trained with eigengenes alone (AUCs: 0.75, 0.733, and 0.785) and morphological features alone (AUCs: 0.593, 0.523, 0.603). This suggests that an integrative prognostic model based on histopathological images and genomic features could significantly improve survival prediction for pRCC patients and assist in clinical decision-making.},
booktitle = {Bioinformatics and Biomedical Engineering: 10th International Work-Conference, IWBBIO 2023, Meloneras, Gran Canaria, Spain, July 12–14, 2023, Proceedings, Part II},
pages = {208–221},
numpages = {14},
keywords = {The Cancer Genome Atlas, histopathological images, genomic features, prognostic model, Renal cell carcinoma (RCC)},
location = {Meloneras, Gran Canaria, Spain}
}

@inproceedings{10.1007/978-3-642-17289-2_5,
author = {Song, SooMin and Son, Jeany and Kim, Myoung-Hee},
title = {Stitching of Microscopic Images for Quantifying Neuronal Growth and Spine Plasticity},
year = {2023},
isbn = {978-3-642-17288-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-17289-2_5},
doi = {10.1007/978-3-642-17289-2_5},
abstract = {In neurobiology, morphological change of neuronal structures such as dendrites and spines is important for understanding of brain functions or neuro-degenerative diseases. Especially, morphological changes of branching patterns of dendrites and volumetric spine structure is related to cognitive functions such as experienced-based learning, attention, and memory. To quantify their morphologies, we use confocal microscopy images which enables us to observe cellular structure with high resolution and three-dimensionally. However, the image resolution and field of view of microscopy is inversely proportional to the field of view (FOV) we cannot capture the whole structure of dendrite at on image. Therefore we combine partially obtained several images into a large image using image stitching techniques. To fine the overlapping region of adjacent images we use Fourier transform based phase correlation method. Then, we applied intensity blending algorithm to remove uneven intensity distribution and seam artifact at image boundaries which is coming from optical characteristics of microscopy. Finally, based on the integrated image we measure the morphology of dendrites from the center of cell to end of each branch. And geometrical characteristics of spine such as area, location, perimeter, and roundness, etc. are also quantified. Proposed method is fully automatic and provides accurate analysis of both local and global structural variations of neuron.},
booktitle = {Advances in Visual Computing: 6th International Symposium, ISVC 2010, Las Vegas, NV, USA, November 29-December 1, 2010. Proceedings, Part I},
pages = {45–53},
numpages = {9},
keywords = {Image Boundary, Lens Distortion, Spine Length, Neuronal Growth, Dendritic Spine},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1007/978-3-031-17976-1_5,
author = {Graziani, Mara and Marini, Niccol\`{o} and Deutschmann, Nicolas and Janakarajan, Nikita and M\"{u}ller, Henning and Mart\'{\i}nez, Mar\'{\i}a Rodr\'{\i}guez},
title = {Attention-Based Interpretable Regression of&nbsp;Gene Expression in&nbsp;Histology},
year = {2022},
isbn = {978-3-031-17975-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-17976-1_5},
doi = {10.1007/978-3-031-17976-1_5},
abstract = {Interpretability of deep learning is widely used to evaluate the reliability of medical imaging models and reduce the risks of inaccurate patient recommendations. For models exceeding human performance, e.g. predicting RNA structure from microscopy images, interpretable modelling can be further used to uncover highly non-trivial patterns which are otherwise imperceptible to the human eye. We show that interpretability can reveal connections between the microscopic appearance of cancer tissue and its gene expression profiling. While exhaustive profiling of all genes from the histology images is still challenging, we estimate the expression values of a well-known subset of genes that is indicative of cancer molecular subtype, survival, and treatment response in colorectal cancer. Our approach successfully identifies meaningful information from the image slides, highlighting hotspots of high gene expression. Our method can help characterise how gene expression shapes tissue morphology and this may be beneficial for patient stratification in the pathology unit. The code is available on GitHub.},
booktitle = {Interpretability of Machine Intelligence in Medical Image Computing: 5th International Workshop, IMIMIC 2022, Held in Conjunction with MICCAI 2022, Singapore, Singapore, September 22, 2022, Proceedings},
pages = {44–60},
numpages = {17},
keywords = {Interpretability, Histopathology, Transcriptomics, Attention},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16434-7_14,
author = {Lin, Yi and Wang, Zeyu and Cheng, Kwang-Ting and Chen, Hao},
title = {InsMix: Towards Realistic Generative Data Augmentation for&nbsp;Nuclei Instance Segmentation},
year = {2022},
isbn = {978-3-031-16433-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16434-7_14},
doi = {10.1007/978-3-031-16434-7_14},
abstract = {Nuclei Segmentation from histology images is a fundamental task in digital pathology analysis. However, deep-learning-based nuclei segmentation methods often suffer from limited annotations. This paper proposes a realistic data augmentation method for nuclei segmentation, named InsMix, that follows a Copy-Paste-Smooth principle and performs morphology-constrained generative instance augmentation. Specifically, we propose morphology constraints that enable the augmented images to acquire luxuriant information about nuclei while maintaining their morphology characteristics (e.g., geometry and location). To fully exploit the pixel redundancy of the background and improve the model’s robustness, we further propose a background perturbation method, which randomly shuffles the background patches without disordering the original nuclei distribution. To achieve contextual consistency between original and template instances, a smooth-GAN is designed with a foreground similarity encoder (FSE) and a triplet loss. We validated the proposed method on two datasets, i.e., Kumar and CPS datasets. Experimental results demonstrate the effectiveness of each component and the superior performance achieved by our method to the state-of-the-art methods.The source code is available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part II},
pages = {140–149},
numpages = {10},
keywords = {Data augmentation, Morphology constraints, Generative},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-030-87237-3_58,
author = {Claudio Quiros, Adalberto and Coudray, Nicolas and Yeaton, Anna and Sunhem, Wisuwat and Murray-Smith, Roderick and Tsirigos, Aristotelis and Yuan, Ke},
title = {Adversarial Learning of Cancer Tissue Representations},
year = {2021},
isbn = {978-3-030-87236-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87237-3_58},
doi = {10.1007/978-3-030-87237-3_58},
abstract = {Deep learning based analysis of histopathology images shows promise in advancing the understanding of tumor progression, tumor micro-environment, and their underpinning biological processes. So far, these approaches have focused on extracting information associated with annotations. In this work, we ask how much information can be learned from the tissue architecture itself.We present an adversarial learning model to extract feature representations of cancer tissue, without the need for manual annotations. We show that these representations are able to identify a variety of morphological characteristics across three cancer types: Breast, colon, and lung. This is supported by 1) the separation of morphologic characteristics in the latent space; 2) the ability to classify tissue type with logistic regression using latent representations, with an AUC of 0.97 and 85% accuracy, comparable to supervised deep models; 3) the ability to predict the presence of tumor in Whole Slide Images (WSIs) using multiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy.Our results show that our model captures distinct phenotypic characteristics of real tissue samples, paving the way for further understanding of tumor progression and tumor micro-environment, and ultimately refining histopathological classification for diagnosis and treatment (The code and pretrained models are available at: ).},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part VIII},
pages = {602–612},
numpages = {11},
keywords = {Histology, Generative adversarial networks},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87237-3_31,
author = {Cicalese, Pietro Antonio and Rizvi, Syed Asad and Wang, Victor and Patibandla, Sai and Yuan, Pengyu and Zare, Samira and Moos, Katharina and Batal, Ibrahim and Clahsen-van Groningen, Marian and Roufosse, Candice and Becker, Jan and Mohan, Chandra and Nguyen, Hien Van},
title = {MorphSet: Improving Renal Histopathology Case Assessment Through Learned Prognostic Vectors},
year = {2021},
isbn = {978-3-030-87236-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87237-3_31},
doi = {10.1007/978-3-030-87237-3_31},
abstract = {Computer Aided Diagnosis (CAD) systems for renal histopathology applications aim to understand and replicate nephropathologists’ assessments of individual morphological compartments (e.g. glomeruli) to render case-level histological diagnoses. Deep neural networks (DNNs) hold great promise in addressing the poor intra- and interobserver agreement between pathologists. This being said, the generalization ability of DNNs heavily depends on the quality and quantity of training labels. Current “consensus” labeling strategies require multiple pathologists to evaluate every compartment unit over thousands of crops, resulting in enormous annotative costs. Additionally, these techniques fail to address the underlying reproducibility issues we observe across various diagnostic feature assessment tasks. To address both of these limitations, we introduce MorphSet, an end-to-end architecture inspired by Set Transformers which maps the combined encoded representations of Monte Carlo (MC) sampled glomerular compartment crops to produce Whole Slide Image (WSI) predictions on a case basis without the need for expensive fine-grained morphological feature labels. To evaluate performance, we use a kidney transplant Antibody Mediated Rejection (AMR) dataset, and show that we are able to achieve 98.9% case level accuracy, outperforming the consensus label baseline. Finally, we generate a visualization of prediction confidence derived from our MC evaluation experiments, which provides physicians with valuable feedback.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part VIII},
pages = {319–328},
numpages = {10},
keywords = {Morphology, Antibody Mediated Rejection, Self attention},
location = {Strasbourg, France}
}

@inproceedings{10.1109/ICMA52036.2021.9512746,
author = {Du, Yimeng and Wang, Yuezong and Peng, Youfan},
title = {Design of Convenient Structured Light Projection System for Microscopic Measurement},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICMA52036.2021.9512746},
doi = {10.1109/ICMA52036.2021.9512746},
abstract = {This paper presents a convenient structured light projection system for microscopic measurement, which is used to design the structured-light pattern on the computer, you can freely edit the pattern structure, color, geometric size, motion mode, projection brightness, contrast and other attributes. An optional path system with zoom function is designed, which can output 10 micron wide structured-light fringe. The vision system is established by using camera and microscope, which is used to shoot the pattern of structured-light, and the 3D morphology is reconstructed by analyzing the pattern of structured-light. The experimental results show that the system can project small feature-size structured-light patterns and is very suitable for 3D morphology measurement of micro objects from tens of microns to hundreds of microns.},
booktitle = {2021 IEEE International Conference on Mechatronics and Automation (ICMA)},
pages = {253–258},
numpages = {6},
location = {Takamatsu, Japan}
}

@inproceedings{10.1007/978-3-030-64221-1_23,
author = {Xing, Cheng and Fan, Jianchao and Wang, Xinzhe and Xing, Jun},
title = {Edge Information Extraction of Overlapping Fiber Optical Microscope Imaging Based on Modified Watershed Algorithm},
year = {2020},
isbn = {978-3-030-64220-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64221-1_23},
doi = {10.1007/978-3-030-64221-1_23},
abstract = {In the microscopic fields of materials, biology, etc., the research based on the images under the optical microscope is an important part in experiments. Because of the specific characteristics along with various materials, the automatic identification, calculation and statistical techniques of particles in the microscope imaging are facing barriers. Though a number of methods have been invented for edge segmentation, the watershed algorithm tends to display good performance in image feature analysis. However, its result of processing the fiber edge images under the microscope does not meet the requirements based on the experiments. This paper aims to propose an improved watershed algorithm in order to better solve the problem of over-segmentation and insufficient segmentation in the field of overlapping optical fiber images. The morphological algorithm is essentially processed as a pretreatment in the beginning. Then after the reinforcement, the OTSU algorithm is demanded as a more accurate binarization process in order to better distinguish the images. On this basis, the distance between the feature pixels is supposed to be calculated aiming to reconstruct the pixel value corresponding to each point. Experimental results are compared with the version obtained by the foreground background marking watershed algorithm.},
booktitle = {Advances in Neural Networks – ISNN 2020: 17th International Symposium on Neural Networks, ISNN 2020, Cairo, Egypt, December 4–6, 2020, Proceedings},
pages = {261–269},
numpages = {9},
keywords = {Foreground background marking, Morphological, Pixel reconstruction, Overlapping fibers, Watershed algorithm},
location = {Cairo, Egypt}
}

@inproceedings{10.1007/978-3-030-59722-1_48,
author = {Shen, Yiqing and Ke, Jing},
title = {A Deformable CRF Model for Histopathology Whole-Slide Image Classification},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_48},
doi = {10.1007/978-3-030-59722-1_48},
abstract = {To detect abnormality from histopathology images in a patch-based convolutional neural network (CNN), spatial context is an important cue. However, whole-slide image (WSI) is characterized by high morphological heterogeneity in the shape and scale of tissues, hence a simple visual span to a larger context may not well capture the information associated with the central patch or disease of interest. In this paper, we propose a Deformable Conditional Random Field (DCRF) model to learn the offsets and weights of neighboring patches in a spatial-adaptive manner. Additionally, rather than using regularly tessellated or overlapped patches, we localize patches with more powerful feature representations by the adaptively adjusted offsets in a WSI. Both the employment of DCRF for better feature extraction from spatial sampling patches, as well as utilization of the auto-generated patches as training input, can achieve performance improvement in the target task. This model is feasible to the widespread annotation strategies in histopathology images, either with a contoured region of interest (ROI) or patch-wise multi-tissue labels. The proposed model is validated on the patient cohorts from The Cancer Genome Atlas (TCGA) dataset and the Camelyon16 dataset for performance evaluation. The experimental results demonstrate the advantage of the proposed model in the classification task, by the comparison against the baseline models.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {500–508},
numpages = {9},
keywords = {Whole-slide image, Spatial correlation, Auto-labelling, Deformable conditional random field},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59520-3_17,
author = {Deshpande, Srijay and Minhas, Fayyaz and Rajpoot, Nasir},
title = {Train Small, Generate Big: Synthesis of Colorectal Cancer Histology Images},
year = {2020},
isbn = {978-3-030-59519-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59520-3_17},
doi = {10.1007/978-3-030-59520-3_17},
abstract = {The construction of large tissue images is a challenging task in the field of generative modeling of histopathology images. Such synthetic images can be used for development and evaluation of various types of deep learning methods. However, memory and computational processing requirements limit the sizes of image constructed using neural generative models. To tackle this, we propose a conditional generative adversarial network framework that learns to generate and stitch small patches to construct large tissue image tiles while preserving global morphological characteristics. The key novelty of the proposed scheme is that it can be used to generate tiles larger than those used for training with high fidelity. Our evaluation of the Colorectal Adenocarcinoma Gland (CRAG) dataset shows that the proposed model can generate large tissue tiles that exhibit realistic morphological tissue features including glands appearance, nuclear structure, and stromal architecture. Our experimental results also show that the proposed model can be effectively used for evaluation of image segmentation models as well.},
booktitle = {Simulation and Synthesis in Medical Imaging: 5th International Workshop, SASHIMI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings},
pages = {164–173},
numpages = {10},
keywords = {Image synthesis, Generative adversarial networks, Computational pathology},
location = {Lima, Peru}
}

@inproceedings{10.1145/3397391.3397407,
author = {Divina, Paul Daniel C. and Felices, John Philip T. and Hortinela, Carlos C. and Fausto, Janette C. and Valiente, Flordeliza L. and Balbin, Jessie R.},
title = {Classification of Red Blood Cell Morphology Using Image Processing and Support Vector Machine},
year = {2020},
isbn = {9781450377249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397391.3397407},
doi = {10.1145/3397391.3397407},
abstract = {Blood serves as an indicator of health, a complete blood count (CBC) provides the clinician a view of the blood components. Diagnosis of the shape of RBC contributes information about relevant pathological diseases and condition. Red blood cells vary from the size of the cell, variations of shape, and presence of central pallor. With these observations several diseases and conditions showed correlations with the characteristic morphologic variations of red blood cells. The conventional use of peripheral blood smears remains laborious, time-consuming procedure and the lack of expertise of the microscopist is a factor to inaccurate results. The advancement of technology provided the medical field the benefits of an automated recognition and numerous studies are applying different methods in classifying RBCs. Every study differs, in what algorithm to use and what RBC is to be identified. Most of all studies were able to provide a satisfiable output and so continuous studies and development are being applied. This paper aims to develop a system that will correlate associated anemia conditions once the red blood cell was identified having an abnormality with its variations in shape or size. The proposed system was able to develop a reliable system to identify 7 different type of red blood cells normal, echinocytes, elliptocytes, dacrocytes, spherocytes, target cells, stomatocytes, and an unknown each cell achieved an accuracy of 98.33%,100%, 98.33%, 97.5%, 100%, 100%, 99.17%, and 95% respectively.},
booktitle = {Proceedings of the 2020 10th International Conference on Biomedical Engineering and Technology},
pages = {22–27},
numpages = {6},
keywords = {Support vector machine, Red blood cells, Red blood cell morphology, Image processing, Conventional microscopy},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {ICBET '20}
}

@inproceedings{10.1145/3377929.3398152,
author = {Howison, Toby and Iida, Fumiya},
title = {Automatically designing the behaviours of falling paper: the emergence of non-trivial behaviours via interaction with the physical world},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3398152},
doi = {10.1145/3377929.3398152},
abstract = {Biological systems exhibit extraordinary levels of diversity. Embodied behaviours are the emergent property of many loosely-coupled parallel processes, from the microscopic level, e.g. materials, to more abstracted levels, e.g. organs and limbs. We summarise our investigations into using a synthetic methodology, i.e. an understanding-by-building approach, for designing the complex interactions of falling paper shapes. By studying how simple systems such as a falling paper shape behave, we can analyse the specific characteristics of the interaction between morphology and the environment, and how this leads to programmable non-trivial behaviours. We present current results and discuss the implications on the future of design in robotics.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1385–1386},
numpages = {2},
keywords = {falling paper, automatic design},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1007/978-3-030-50420-5_6,
author = {Habrat, Magdalena and M\l{}ynarczuk, Mariusz},
title = {Granulation-Based Reverse Image Retrieval for Microscopic Rock Images},
year = {2020},
isbn = {978-3-030-50419-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-50420-5_6},
doi = {10.1007/978-3-030-50420-5_6},
abstract = {The paper presents a method of object detection on microscopic images of rocks, which makes it possible to identify images with similar structural features of the rock. These features are understood as the sizes and shapes of its components and the mutual relationships between them. The proposed detection methodology is an adaptive and unsupervised method that analyzes characteristic color clusters in the image. It achieves good detection results for rocks with clear clusters of colored objects. For the analyzed data set, the method finds in the rock image sets with high visual similarity, which translates into the geological classification of rocks at a level of above 78%. Considering the fact that the proposed method is based on segmentation that does not require any input parameters, this result should be considered satisfactory. In the authors’ opinion, this method can be used in issues of rock image search, sorting, or e.g. automatic selection of optimal segmentation techniques.},
booktitle = {Computational Science – ICCS 2020: 20th International Conference, Amsterdam, The Netherlands, June 3–5, 2020, Proceedings, Part III},
pages = {74–86},
numpages = {13},
keywords = {CBVIR, CBIR, Microscopic analysis of rock, Classification of rocks, Objects retrieval},
location = {Amsterdam, The Netherlands}
}

@inproceedings{10.1007/978-3-031-43996-4_22,
author = {Haouchine, Nazim and Dorent, Reuben and Juvekar, Parikshit and Torio, Erickson and Wells, William M. and Kapur, Tina and Golby, Alexandra J. and Frisken, Sarah},
title = {Learning Expected Appearances for&nbsp;Intraoperative Registration During Neurosurgery},
year = {2023},
isbn = {978-3-031-43995-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43996-4_22},
doi = {10.1007/978-3-031-43996-4_22},
abstract = {We present a novel method for intraoperative patient-to-image registration by learning Expected Appearances. Our method uses preoperative imaging to synthesize patient-specific expected views through a surgical microscope for a predicted range of transformations. Our method estimates the camera pose by minimizing the dissimilarity between the intraoperative 2D view through the optical microscope and the synthesized expected texture. In contrast to conventional methods, our approach transfers the processing tasks to the preoperative stage, reducing thereby the impact of low-resolution, distorted, and noisy intraoperative images, that often degrade the registration accuracy. We applied our method in the context of neuronavigation during brain surgery. We evaluated our approach on synthetic data and on retrospective data from 6 clinical cases. Our method outperformed state-of-the-art methods and achieved accuracies that met current clinical standards.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part IX},
pages = {227–237},
numpages = {11},
keywords = {Intraoperative Registration, Image-guided Neurosurgery, Augmented Reality, Neural Image Analogy, 3D Pose Estimation},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-030-59722-1_38,
author = {Sahasrabudhe, Mihir and Christodoulidis, Stergios and Salgado, Roberto and Michiels, Stefan and Loi, Sherene and Andr\'{e}, Fabrice and Paragios, Nikos and Vakalopoulou, Maria},
title = {Self-supervised Nuclei Segmentation in Histopathological Images Using Attention},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_38},
doi = {10.1007/978-3-030-59722-1_38},
abstract = {Segmentation and accurate localization of nuclei in histopathological images is a very challenging problem, with most existing approaches adopting a supervised strategy. These methods usually rely on manual annotations that require a lot of time and effort from medical experts. In this study, we present a self-supervised approach for segmentation of nuclei for whole slide histopathology images. Our method works on the assumption that the size and texture of nuclei can determine the magnification at which a patch is extracted. We show that the identification of the magnification level for tiles can generate a preliminary self-supervision signal to locate nuclei. We further show that by appropriately constraining our model it is possible to retrieve meaningful segmentation maps as an auxiliary output to the primary magnification identification task. Our experiments show that with standard post-processing, our method can outperform other unsupervised nuclei segmentation approaches and report similar performance with supervised ones on the publicly available MoNuSeg dataset. Our code and models are available online () to facilitate further research.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {393–402},
numpages = {10},
keywords = {Attention models, Self-supervision, Deep learning, Nuclei segmentation, Whole slide images, Pathology},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-031-44240-7_9,
author = {Aswath, Anusha and Alsahaf, Ahmad and Westenbrink, B. Daan and Giepmans, Ben N. G. and Azzopardi, George},
title = {COFI - Coarse-Semantic to&nbsp;Fine-Instance Unsupervised Mitochondria Segmentation in&nbsp;EM},
year = {2023},
isbn = {978-3-031-44239-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44240-7_9},
doi = {10.1007/978-3-031-44240-7_9},
abstract = {Instance segmentation is crucial for insightful analysis in the increasing use of large-scale electron microscopy (EM) to gain a better understanding of disease causes or progression. Instance segmentation is a more granular version of semantic segmentation, as it identifies and distinguishes individual object instances, whereas semantic segmentation only identifies object classes. In this study, we introduce a two-stage unsupervised approach called COFI, which stands for Coarse-Semantic to Fine-Instance segmentation, for the application of mitochondria segmentation in large-scale 2D EM images. In its first stage, it produces a rough region mask by clustering image patches and prompting a user to select the regions of interest. This is followed by a boundary delineation method based on the brain-inspired COSFIRE filter which is augmented by an inhibition component that makes it robust to image texture and noise. The effectiveness of the proposed COFI approach is evaluated on an EM dataset of the heart muscle of a mouse tissue, which consisted of four tiles of 16384\texttimes{}16384 pixels, containing a total of 2287 instances of mitochondria among other subcellular structures. It consistently achieved panoptic quality measures that are substantially superior to competing supervised methodologies. Besides its elevated effectiveness, the proposed COFI approach is conceptually simple and sufficiently versatile as the structure of interest is not intrinsic to the method.},
booktitle = {Computer Analysis of Images and Patterns: 20th International Conference, CAIP 2023, Limassol, Cyprus, September 25–28, 2023, Proceedings, Part II},
pages = {87–97},
numpages = {11},
keywords = {mitochondria, unsupervised, Instance segmentation},
location = {Limassol, Cyprus}
}

@inproceedings{10.1007/978-3-031-16434-7_30,
author = {Haq, Mohammad Minhazul and Huang, Junzhou},
title = {Self-supervised Pre-training for&nbsp;Nuclei Segmentation},
year = {2022},
isbn = {978-3-031-16433-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16434-7_30},
doi = {10.1007/978-3-031-16434-7_30},
abstract = {The accurate segmentation of nuclei is crucial for cancer diagnosis and further clinical treatments. For semantic segmentation of nuclei, Vision Transformers (VT) have the potentiality to outperform Convolutional Neural Network (CNN) based models due to their ability to model long-range dependencies (i.e., global context). Usually, VT and CNN models are pre-trained with large-scale natural image dataset (i.e., ImageNet) in fully-supervised manner. However, pre-training nuclei segmentation models with ImageNet is not much helpful because of morphological and textural differences between natural image domain and medical image domain. Also, ImageNet-like large-scale annotated histology dataset rarely exists in medical image domain. In this paper, we propose a novel region-level Self-Supervised Learning (SSL) approach and corresponding triplet loss for pre-training semantic nuclei segmentation model with unannotated histology images extracted from Whole Slide Images (WSI). Our proposed region-level SSL is based on the observation that, non-background (i.e., nuclei) patches of an input image are difficult to predict from surrounding neighbor patches, and vice versa. We empirically demonstrate the superiority of our proposed SSL incorporated VT model on two public nuclei segmentation datasets.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part II},
pages = {303–313},
numpages = {11},
keywords = {Nuclei segmentation, Self-supervised learning, Transformers},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1145/3456669.3456717,
author = {Smistad, Erik},
title = {FAST: A framework for high-performance medical image computing and visualization},
year = {2021},
isbn = {9781450390330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456669.3456717},
doi = {10.1145/3456669.3456717},
abstract = {Medical image processing and visualization is often computationally demanding. Ultrasound images are acquired in real-time and needs to be processed at a high framerate with low latency. Computed tomography (CT) and magnetic resonance imaging (MRI) create large three dimensional volumes with sizes up to 512 \texttimes{} 512 \texttimes{} 800 voxels. In digital pathology, whole slide microscopy images can have an extreme image size of up to 200, 000 \texttimes{} 100, 000 pixels, which does not even fit into the memory of most computers. Thus, there is a need for smart data storage, processing and visualization methods to handle medical image data. The development of FAST started in 2014, the goal was to create an open-source framework which made GPU and parallel processing of medical images easy and portable. While there existed popular image processing libraries such as the visualization toolkit (VTK), insight toolkit (ITK) and OpenCV, the GPU processing capabilities were still implemented ad-hoc and often implied copying data back and forth from the GPU and CPU. Thus it was decided to use the new OpenCL API to create a cross-platform framework designed bottom-up with GPU processing at the very core. One of the design goals was to remove the burden of moving data back and forth from different processors and memory spaces from the developer. Instead, the developer requests access to the data on a given processor, and FAST will copy and update data as needed. Now, seven years later FAST version 3.2 is released, it still uses OpenCL 1.2 and OpenGL 3.3 at the core of almost all of its operations. FAST can stream images in real-time from ultrasound scanners, webcameras, Intel’s RealSense depth camera, and read many different formats from disk including medical formats such as DICOM, Metaimage and huge microscopy images stored as tiled image pyramids. FAST uses a processing pipeline concept, meaning that you define a pipeline as multiple processing and visualization steps first, then initiate the processing by executing the pipeline. The advantages of this is that it’s easy to change data sources and processing steps. The same pipeline used to process an ultrasound image on disk, can be used to process a real-time stream of ultrasound images. Today FAST pipelines can be created with C++, Python 3 and even without any programming using simple text files. The pipeline approach also opens up possibilities for load balancing and tuning based on analyzing the pipeline as computational graphs, although this has not yet been implemented. In the last five years or so, deep neural networks have become the standard for almost all image processing tasks. Many high-performance frameworks for deep neural network inference already exist, but have very different APIs and use different formats for storing neural network models. FAST now provides a common API for neural networks with multiple backends such as NVIDIA’s TensorRT, Intel’s OpenVINO and Google’s TensorFlow. This removes the burden of the user to learn the API of every inference library, and makes neural network inference as simple as just loading a model stored on disk. This presentation will present the FAST framework and how OpenCL was used to make it. The trade-offs between portability/ease-of-use/code complexity and performance has been a constant challenge, often leading to sacrificing performance or having to write multiple versions of the same algorithm to handle different OpenCL implementations. The presentation will also discuss OpenCL features which have been important in developing this framework such as OpenGL interoperability and 2D/3D Images/Textures. FAST is open-source and we invite the community to contribute through GitHub at https://github.com/smistad/FAST},
booktitle = {Proceedings of the 9th International Workshop on OpenCL},
articleno = {14},
numpages = {2},
keywords = {High-performance, Image Processing, Medical Imaging, OpenCL, OpenGL, Visualization},
location = {<conf-loc>, <city>Munich</city>, <country>Germany</country>, </conf-loc>},
series = {IWOCL '21}
}

@inproceedings{10.1145/3463912.3482501,
author = {Chang, Wu-Ching},
title = {My Grandmother is an Egg},
year = {2021},
isbn = {9781450385282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463912.3482501},
doi = {10.1145/3463912.3482501},
abstract = {The animated short film is based on the director's grandmother's experience as a T'ung-yang-hsi. T'ung-yang-hsi is the traditional practice of pre-arranged marriage in East Asia, selling or giving a young girl to another family to be raised as a future daughter-in-law. The tradition has vanished for decades, but the patriarchal shadow still lingers.Focusing on female issues, the film aims to reflect upon women's oppression and struggle for freedom. Trying to be true to the historical context, the narration is based on interviews with the director's grandmother's children, and research in feminism in animated short films at Royal College of Art in the United Kingdom during pre-production stage.Egg is an important symbol in the film. As the metaphor of women in the film, eggs are the symbolization of the productive roles in the male-dominated society. After labor and oppression experienced by a T'ung-yang-hsi, the film reaches its climax with Hakka 'Old Mountain Song' combined with turbulent waves. Behind the rail track, there is the sea. Across the sea, therein lies freedom.Except for 2D animation with hand-painted texture, this film contains multiple experimental practices, such as animating with eggs, egg white, and egg yolk. Digital cut-out animation, and paint on old photos. This film also contains historical archives, the Household Registration Transcript during the time of Taiwan under Japanese rule.From microscopic to macroscopic, from personal witness to the general phenomenon in the society, audiences may glimpse the long past, imagine women's situation in our own times, and look forward to striving for gender equality in the future. The director hope audiences may gain fresh insight into the impact of social norms and power structures on our daily life.Egg is life per se. Eggs are fragile, but at the same time tough. My grandmother is an egg.},
booktitle = {SIGGRAPH Asia 2021 Computer Animation Festival},
articleno = {12},
numpages = {1},
location = {Tokyo, Japan},
series = {SA '21}
}

@inproceedings{10.1145/3645279.3645287,
author = {Liu, Guoyan and Zhao, Jincai and Zeng, Yanan and Wu, Haiyun and Li, Zhi and Wei, Yong},
title = {Polarization Image Processing Technology Based on Machine Vision Detection},
year = {2024},
isbn = {9798400709166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3645279.3645287},
doi = {10.1145/3645279.3645287},
abstract = {The detection and morphology characterization of these biological samples are the basis of life research. Optical microscopic imaging has great advantages in the characterization and detection of biological samples because of its characteristics of low sample requirements, good environmental adaptability, convenient, fast and non-destructive detection. However, due to the influence of optical diffraction limit, the resolution of optical microscopic imaging method is comparative low and thus limits the applications. Due to the polarization characteristics of the scattering field from scatterer are closely related to the microstructure of the scatterer, the measurement of the polarization characteristics of the scattering field can improve the detection and characterization ability of biological samples. Meanwhile, to achieve a higher resolution and to obtain a larger range of particle near-field scattering spectral distribution, the method in the paper takes the image acquisition part of the traditional machine vision system as the main body and a polarization modulation module is added to build an image acquisition device. The non-intuitive images obtained in the experiment are compared with the traditional direct imaging, and the results prove that the non-intuitive polarization parameter imaging can obtain the particle near-field scattering spectral distribution in a larger range than the traditional direct imaging. It is proved that non-intuitive light wave vectors are more sensitive to near-field scattering and have higher resolution.},
booktitle = {Proceedings of the 2023 International Conference on Big Data Mining and Information Processing},
pages = {40–44},
numpages = {5},
keywords = {Computing methodologies, Machine vision, Microimaging},
location = {<conf-loc>, <city>Xiamen City, China</city>, <country>China</country>, </conf-loc>},
series = {BDMIP '23}
}

@inproceedings{10.1145/3645259.3645262,
author = {Chen, Jiahao and Zhang, Jianxin and Lin, Yu},
title = {A Rolling Ball Segmentation and Pairing Algorithm Based on Cell Fluorescence Microscopic Stack},
year = {2024},
isbn = {9798400708473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3645259.3645262},
doi = {10.1145/3645259.3645262},
abstract = {In order to explore the effect of a certain cell membrane protein (NMIIB and RhoA) on regulating the tension of the cortical layer, we segmented and paired the cells in the fluorescence microscopic stack of membrane proteins of two groups cells in different states (natural or compressed), to compare the intensity changes of the membrane proteins of the same cell before and after compressed. We propose a method for cell segmentation and pairing based on cell fluorescence microscopy stacked images, using a slice by slice scanning method to compare the information of each cell in different slice to determine its intermediate layer; In single-slice segmentation, we adopted the rolling ball algorithm to strip the inner contour of the cell membrane based on the characteristics of its membrane protein image, effectively avoiding the impact of cell adhesion; Finally, pairing factors were calculated based on the shape characteristics of each cell in the two images for pairing, taking into account the impact of overall field of view migration.},
booktitle = {Proceedings of the 2024 6th International Conference on Image Processing and Machine Vision},
pages = {12–18},
numpages = {7},
keywords = {3D stack, Cell pairing, Cell segmentation, Rolling ball algorithm},
location = {<conf-loc>, <city>Macau</city>, <country>China</country>, </conf-loc>},
series = {IPMV '24}
}

@inproceedings{10.1007/978-3-031-58171-7_6,
author = {Guan, Xianchao and Wang, Yifeng and Lin, Yiyang and Zhang, Yongbing},
title = {Data Augmentation Based on&nbsp;DiscrimDiff for&nbsp;Histopathology Image Classification},
year = {2024},
isbn = {978-3-031-58170-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-58171-7_6},
doi = {10.1007/978-3-031-58171-7_6},
abstract = {Histopathological analysis is the present gold standard for cancer diagnosis. Accurate classification of histopathology images has great clinical significance and application value for assisting pathologists in diagnosis. However, the performance of histopathology image classification is greatly affected by data imbalance. To address this problem, we propose a novel data augmentation framework based on the diffusion model, DiscrimDiff, which expands the dataset by synthesizing images of rare classes. To compensate for the lack of discrimination ability of the diffusion model for synthesized images, we design a post-discrimination mechanism to provide image quality assurance for data augmentation. Our method significantly improves classification performance on multiple datasets. Furthermore, histomorphological features of different classes concerned by the diffusion model may provide guiding significance for pathologists in clinical diagnosis. Therefore, we visualize histomorphological features related to classification, which can be used to assist pathologist-in-training education and improve the understanding of histomorphology.},
booktitle = {Data Augmentation, Labelling, and Imperfections: Third MICCAI Workshop, DALI 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 12, 2023, Proceedings},
pages = {53–62},
numpages = {10},
keywords = {Computational pathology, Diffusion models, Data augmentation, Histomorphological features},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-51940-6_33,
author = {Mestizo-Guti\'{e}rrez, Sonia Lilia and Acosta-Mesa, H\'{e}ctor Gabriel and Garc\'{\i}a-Ortega, Francisco and Jim\'{e}nez-Cata\~{n}o, Mar\'{\i}a Esther},
title = {Analysis of Proteins in Microscopic Skin Images Using Machine Vision Techniques as a Tool for Detecting Alzheimer’s Disease},
year = {2024},
isbn = {978-3-031-51939-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-51940-6_33},
doi = {10.1007/978-3-031-51940-6_33},
abstract = {Alzheimer’s disease (AD) is a disease characterized by progressive loss of memory, orientation, judgement and language. Its progression is slow and there is no cure for this multifactorial disease, so it is of great importance the discovery of new methods of early diagnosis, as well as the development of more effective treatments. AD is the most common type of dementia. Currently, the incidence of dementia has increased and constitutes a priority health problem worldwide, so urgent measures focused on prevention and reduction of risk factors, as well as early diagnosis, are required. In this work, we proposed computer vision techniques as a tool for histological analysis to support experts in the search for a biochemical marker for early diagnosis of AD. We analized ten samples of skin tissue biopsies (5 controls and 5 AD patients) of microscopic images to find characteristic staining colors to differentiate the images of healthy subjects from Alzheimer’s disease patients. In the immunohistochemistry process, the antibodies that revealed tau protein oligomers antigens were AT22. For lamin A, we used anti-lamin A. Results allowed us to find differences between healthy subjects and subjects with Alzheimer’s disease.},
booktitle = {Advances in Computational Intelligence. MICAI 2023 International Workshops: WILE 2023, HIS 2023, and CIAPP 2023, Yucat\'{a}n, Mexico, November 13–18, 2023, Proceedings},
pages = {432–438},
numpages = {7},
keywords = {Alzheimer’s disease, Machine vision techniques},
location = {<conf-loc content-type="InPerson">Yucat\'{a}n, Mexico</conf-loc>}
}

@inproceedings{10.1007/978-981-99-8558-6_31,
author = {Zeng, Liang},
title = {Cell-CAEW: Cell Instance Segmentation Based on&nbsp;ConvAttention and&nbsp;Enhanced Watershed},
year = {2023},
isbn = {978-981-99-8557-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-8558-6_31},
doi = {10.1007/978-981-99-8558-6_31},
abstract = {Cell instance segmentation in microscopy images is a challenging task. The morphological differences between different types of cells are significant, it is difficult to distinguish the boundaries between adjacent or overlapping cells. To address these issues, we improved Cellpose’s framework and proposed Cell-CoaT. Cell-CoaT adopts CoaT as the encoder and designs a decoder that can integrate features from different scales, and predicts the center region and gradient fields of cells. In the post-processing stage, we utilized a Marker-Controlled Watershed Segmentation with center point labels predicted by the network to alleviate under-segmentation and over-segmentation. Cell-CAEW obtains an F1 score of 0.7724 on the tuning set. The code will be released soon.},
booktitle = {Pattern Recognition and Computer Vision: 6th Chinese Conference, PRCV 2023, Xiamen, China, October 13–15, 2023, Proceedings, Part XIII},
pages = {370–381},
numpages = {12},
keywords = {Enhanced Watershed, Attention, Cell Instance Segmentation},
location = {<conf-loc content-type="InPerson">Xiamen, China</conf-loc>}
}

@inproceedings{10.1145/3620679.3620682,
author = {Nagamura, Toru and Seno, Shigeto and Shigeta, Hironori and Mashita, Tomohiro and Matsuda, Hideo},
title = {Cell Tracking via Reinforcement Learning with Microscopic Image Simulator},
year = {2023},
isbn = {9798400707438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620679.3620682},
doi = {10.1145/3620679.3620682},
abstract = {Recent advances in optical microscopy and fluorescent protein technology have made it possible to record movies of cells over time while keeping them alive. Cell tracking is necessary to extract and analyze cell dynamics from these movies. Tracking-by-detection methods based on supervised deep learning are widely used for cell tracking. However, it is necessary to individually adjust the tracking algorithm for each cell movie that shows various characteristics and, in addition, to prepare a sufficient amount of data for training. To address these issues, we propose a method for training cell tracking models based on reinforcement learning with a simulator that imitates cell movies as an environment. The simulator can generate diverse and voluminous cell movies containing cell features from the correct trajectory of cell tracking. Through evaluation of the Cell Tracking Challenge dataset, the proposed method is confirmed to achieve a competitive performance that is better than the conventional reinforcement-learning tracker.},
booktitle = {Proceedings of the 2023 13th International Conference on Biomedical Engineering and Technology},
pages = {16–22},
numpages = {7},
keywords = {bioimage informatics, biological imaging, cell movement, deep reinforcement learning, generative adversarial network, object tracking},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {ICBET '23}
}

@inproceedings{10.1007/978-3-031-47634-1_24,
author = {Banerjee, Nirwan and Malakar, Samir and Gupta, Deepak Kumar and Horsch, Alexander and Prasad, Dilip K.},
title = {Guided U-Net Aided Efficient Image Data Storing with&nbsp;Shape Preservation},
year = {2023},
isbn = {978-3-031-47633-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-47634-1_24},
doi = {10.1007/978-3-031-47634-1_24},
abstract = {The proliferation of high-content microscopes (∼32 GB for a single image) and the increasing amount of image data generated daily have created a pressing need for compact storage solutions. Not only is the storage of such massive image data cumbersome, but it also requires a significant amount of storage and data bandwidth for transmission. To address this issue, we present a novel deep learning technique called Guided U-Net (GU-Net) that compresses images by training a U-Net architecture with a loss function that incorporates shape, budget, and skeleton losses. The trained model learns to selects key points in the image that need to be stored, rather than the entire image. Compact image representation is different from image compression because the former focuses on assigning importance to each pixel in an image and selecting the most important ones for storage whereas the latter encodes information of the entire image for more efficient storage. Experimental results on four datasets (CMATER, UiTMito, MNIST, and HeLA) show that GU-Net selects only a small percentage of pixels as key points (3%, 3%, 5%, and 22% on average, respectively), significantly reducing storage requirements while preserving essential image features. Thus, this approach offers a more efficient method of storing image data, with potential applications in a range of fields where large-scale imaging is a vital component of research and development.},
booktitle = {Pattern Recognition: 7th Asian Conference, ACPR 2023, Kitakyushu, Japan, November 5–8, 2023, Proceedings, Part I},
pages = {317–330},
numpages = {14},
keywords = {Storage Efficient, Skeleton Loss, Shape Loss, Budget Loss, Guided U-Net, Compact Image Representation},
location = {Kitakyushu, Japan}
}

@inproceedings{10.1007/978-3-031-45275-8_43,
author = {Moraes, Athos and Moreno, Marta and Ribeiro, Rog\'{e}rio and Ferreira, Pedro G.},
title = {Predicting Age from&nbsp;Human Lung Tissue Through Multi-modal Data Integration},
year = {2023},
isbn = {978-3-031-45274-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-45275-8_43},
doi = {10.1007/978-3-031-45275-8_43},
abstract = {The accurate prediction of biological age can bring important benefits in promoting therapeutic and behavioural strategies for healthy aging. We propose the development of age prediction models using multi-modal datasets, including transcriptomics, methylation and histological images from lung tissue samples of 793 human donors. From a technical point of view this is a challenging problem since not all donors are covered by the same data modalities and the datasets have a very high feature dimensionality with a relatively smaller number of samples. To fairly compare performance across different data types, we’ve created a test set including donors represented in each modality. Given the unique characteristics of the data distribution, we developed gradient boosting tree and convolutional neural network models for each dataset. The performance of the models can be affected by several covariates, including smoking history, and, most importantly, by a skewed distribution of age. Data-centric approaches, including feature engineering, feature selection, data stratification and resampling, proved fundamental in building models that were optimally adapted for each data modality, resulting in significant improvements in model performance for imbalanced regression. The models were then applied to the test set independently, and later combined into a multi-modal ensemble through a voting strategy, predicting age with a median absolute error of 4 years. Even if prediction accuracy remains a challenge, in this work we provide insights to address the difficulties of multi-modal data integration and imbalanced data prediction.},
booktitle = {Discovery Science: 26th International Conference, DS 2023, Porto, Portugal, October 9–11, 2023, Proceedings},
pages = {644–658},
numpages = {15},
keywords = {Regression, Multi-modal data, Bioinformatics, Computational Biology, Health applications},
location = {<conf-loc content-type="InPerson">Porto, Portugal</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43999-5_61,
author = {Byra, Michal and Poon, Charissa and Shimogori, Tomomi and Skibbe, Henrik},
title = {Implicit Neural Representations for&nbsp;Joint Decomposition and&nbsp;Registration of&nbsp;Gene Expression Images in&nbsp;the&nbsp;Marmoset Brain},
year = {2023},
isbn = {978-3-031-43998-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43999-5_61},
doi = {10.1007/978-3-031-43999-5_61},
abstract = {We propose a novel image registration method based on implicit neural representations that addresses the challenging problem of registering a pair of brain images with similar anatomical structures, but where one image contains additional features or artifacts that are not present in the other image. To demonstrate its effectiveness, we use 2D microscopy in situ hybridization gene expression images of the marmoset brain. Accurately quantifying gene expression requires image registration to a brain template, which is difficult due to the diversity of patterns causing variations in visible anatomical brain structures. Our approach uses implicit networks in combination with an image exclusion loss to jointly perform the registration and decompose the image into a support and residual image. The support image aligns well with the template, while the residual image captures individual image characteristics that diverge from the template. In experiments, our method provided excellent results and outperformed other registration techniques.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part X},
pages = {645–654},
numpages = {10},
keywords = {brain, deep learning, gene expression, implicit neural representations, registration},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43993-3_13,
author = {Huynh, Khoi Minh and Wu, Ye and Ahmad, Sahar and Yap, Pew-Thian},
title = {Microstructure Fingerprinting for&nbsp;Heterogeneously Oriented Tissue Microenvironments},
year = {2023},
isbn = {978-3-031-43992-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43993-3_13},
doi = {10.1007/978-3-031-43993-3_13},
abstract = {Most diffusion biophysical models capture basic properties of tissue microstructure, such as diffusivity and anisotropy. More realistic models that relate the diffusion-weighted signal to cell size and membrane permeability often require simplifying assumptions such as short gradient pulse and Gaussian phase distribution, leading to tissue features that are not necessarily quantitative. Here, we propose a method to quantify tissue microstructure without jeopardizing accuracy owing to unrealistic assumptions. Our method utilizes realistic signals simulated from the geometries of cellular microenvironments as fingerprints, which are then employed in a spherical mean estimation framework to disentangle the effects of orientation dispersion from microscopic tissue properties. We demonstrate the efficacy of microstructure fingerprinting in estimating intra-cellular, extra-cellular, and intra-soma volume fractions as well as axon radius, soma radius, and membrane permeability.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part VIII},
pages = {131–141},
numpages = {11},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43987-2_74,
author = {Azadi, Puria and Suderman, Jonathan and Nakhli, Ramin and Rich, Katherine and Asadi, Maryam and Kung, Sonia and Oo, Htoo and Keyes, Mira and Farahani, Hossein and MacAulay, Calum and Goldenberg, Larry and Black, Peter and Bashashati, Ali},
title = {ALL-IN: ALocal GLobal Graph-Based DIstillatioN Model for&nbsp;Representation Learning of&nbsp;Gigapixel Histopathology Images With Application In Cancer Risk Assessment},
year = {2023},
isbn = {978-3-031-43986-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43987-2_74},
doi = {10.1007/978-3-031-43987-2_74},
abstract = {The utility of machine learning models in histopathology image analysis for disease diagnosis has been extensively studied. However, efforts to stratify patient risk are relatively under-explored. While most current techniques utilize small fields of view (so-called local features) to link histopathology images to patient outcome, in this work we investigate the combination of global (i.e., contextual) and local features in a graph-based neural network for patient risk stratification. The proposed network not only combines both fine and coarse histological patterns but also utilizes their interactions for improved risk stratification. We compared the performance of our proposed model against the state-of-the-art (SOTA) techniques in histopathology risk stratification in two cancer datasets. Our results suggest that the proposed model is capable of stratifying patients into statistically significant risk groups (p&lt;0.01 across the two datasets) with clinical utility while competing models fail to achieve a statistical significance endpoint (p=0.148-0.494).},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part VI},
pages = {765–775},
numpages = {11},
keywords = {Histopathology, Risk Assessment, Graph Processing},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43987-2_65,
author = {Chen, Long and Li, Han and Zhou, S. Kevin},
title = {Label-Free Nuclei Segmentation Using Intra-Image Self Similarity},
year = {2023},
isbn = {978-3-031-43986-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43987-2_65},
doi = {10.1007/978-3-031-43987-2_65},
abstract = {In computational pathology, nuclei segmentation from histology images is a fundamental task. While deep learning based nuclei segmentation methods yield excellent results, they rely on a large amount of annotated images; however, annotating nuclei from histology images is tedious and time-consuming. To get rid of labeling burden completely, we propose a label-free approach for nuclei segmentation, motivated from one pronounced yet omitted property that characterizes histology images and nuclei: intra-image self similarity (IISS), that is, within an image, nuclei are similar in their shapes and appearances. First, we leverage traditional machine learning and image processing techniques to generate a pseudo segmentation map, whose connected components form candidate nuclei, both positive or negative. In particular, it is common that adjacent nuclei are merged into one candidate due to imperfect staining and imaging conditions, which violate the IISS property. Then, we filter the candidates based on a custom-designed index that roughly measures if a candidate contains multiple nuclei. The remaining candidates are used as pseudo labels, which we use to train a U-Net to discover the hierarchical features distinguish nuclei pixels from background. Finally, we apply the learned U-Net to produce final nuclei segmentation. We validate the proposed method on the public dataset MoNuSeg. Experimental results demonstrate the effectiveness of our design and, to the best of our knowledge, it achieves the state-of-the-art performances of label-free segmentation on the benchmark MoNuSeg dataset with a mean Dice score of 79.2%.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part VI},
pages = {673–682},
numpages = {10},
keywords = {Label-free, Nuclei segmentation, Pseudo Label},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43987-2_49,
author = {Jin, Ting and Xie, Xingran and Wan, Renjie and Li, Qingli and Wang, Yan},
title = {Gene-Induced Multimodal Pre-training for&nbsp;Image-Omic Classification},
year = {2023},
isbn = {978-3-031-43986-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43987-2_49},
doi = {10.1007/978-3-031-43987-2_49},
abstract = {Histology analysis of the tumor micro-environment integrated with genomic assays is the gold standard for most cancers in modern medicine. This paper proposes a Gene-induced Multimodal Pre-training (GiMP) framework, which jointly incorporates genomics and Whole Slide Images (WSIs) for classification tasks. Our work aims at dealing with the main challenges of multi-modality image-omic classification w.r.t. (1) the patient-level feature extraction difficulties from gigapixel WSIs and tens of thousands of genes, and (2) effective fusion considering high-order relevance modeling. Concretely, we first propose a group multi-head self-attention gene encoder to capture global structured features in gene expression cohorts. We design a masked patch modeling paradigm (MPM) to capture the latent pathological characteristics of different tissues. The mask strategy is randomly masking a fixed-length contiguous subsequence of patch embeddings of a WSI. Finally, we combine the classification tokens of paired modalities and propose a triplet learning module to learn high-order relevance and discriminative patient-level information. After pre-training, a simple fine-tuning can be adopted to obtain the classification results. Experimental results on the TCGA dataset show the superiority of our network architectures and our pre-training framework, achieving 99.47% in accuracy for image-omic classification. The code is publicly available at},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part VI},
pages = {508–517},
numpages = {10},
keywords = {Multimodal learning, Whole slide image classification},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-44210-0_4,
author = {Fang, Yating and Zhong, Baojiang},
title = {A Multi-scale Method for Cell Segmentation in Fluorescence Microscopy Images},
year = {2023},
isbn = {978-3-031-44209-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44210-0_4},
doi = {10.1007/978-3-031-44210-0_4},
abstract = {Accurate segmentation of cells in fluorescent microscopy images plays a key role in high-throughput applications such as the quantification of protein expression and the study of cell function. Existing cell segmentation methods have drawbacks in terms of inaccurate location of segmentation boundary, misidentification, and inaccurate segmentation of overlapping cells. To address these issues, a novel multi-scale method for cell segmentation in fluorescence microscopy images (MMCS) is proposed in this paper. Our motivation to adopt multi-scale image analysis in the cell segmentation task originates from the basic observation that cells on fluorescence microscope images are often composed of different structures at different scales. In our proposed MMCS, three scales are exploited. At the high scale, noise effects are sufficiently suppressed, and the cell contour is fully smoothed. Then, scale fusion is further performed, that is, the cell contours obtained by segmentation at high, medium, and low scales are averaged, to improve the location accuracy of contour segmentation. To solve the problems of misidentification and cell overlapping, an improved Bradley technique with constraints based on shape and intensity features and region-based fitting of overlapping ellipses technique are also developed and embedded in our multi-scale approach for extracting cell contours at each single scale. The experimental results obtained on a large number of fluorescence microscope images from two data sets show that the proposed MMCS can outperform state-of-the-art methods by a large margin.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2023: 32nd International Conference on Artificial Neural Networks, Heraklion, Crete, Greece, September 26–29, 2023, Proceedings, Part II},
pages = {38–50},
numpages = {13},
keywords = {Fluorescence microscopy images, Multi-scale, Ellipse fitting, Cell segmentation},
location = {Heraklion, Greece}
}

@inproceedings{10.1145/3594806.3594865,
author = {Yadav, Ankur and Daescu, Ovidiu and Leavey, Patrick and Rudzinski, Erin},
title = {Machine Learning for Rhabdomyosarcoma Whole Slide Images Sub-type Classification},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3594865},
doi = {10.1145/3594806.3594865},
abstract = {The most frequent malignant soft tissue tumor in children is Rhabdomyosarcoma (RMS). RMS has several subtypes that differentiate treatment and patient outcomes. Because of variations in the appearance of histopathology images, manual subtype classification requires a high level of expertise and is time-consuming. While several machine-learning techniques have been developed to classify the most common tumor types in histology images, more must be understood about the automatic classification of tumor subtypes. Moreover, existing techniques for classifying whole slide image (WSI) histopathological subtypes rely on small, randomly selected image tiles or on representative tiles chosen by specialists from the considerably larger WSIs. These methods do not draw knowledge from the whole tissue region captured by a WSI, possibly missing significant tumor signatures. They also fail to account for the spatial distribution of patterns that could play a role in reliable subtype classification. This paper proposes a novel methodology that combines different methods to extract features from a whole slide image and generates a whole slide feature map (WSFM). This map and additional clinical features are then used to train various machine-learning models. We obtain 91.84% WSI tumor subtype classification accuracy on a diverse dataset. A direct advantage of our methodology is that it does not require any WSI-level annotation by pathology experts. Training and testing can be performed much faster computationally using simple machine learning algorithms instead of complex deep learning architectures.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {192–196},
numpages = {5},
keywords = {Whole Slide Feature Map, Subtype Classification, Rhabdomyosarcoma, Machine Learning},
location = {<conf-loc>, <city>Corfu</city>, <country>Greece</country>, </conf-loc>},
series = {PETRA '23}
}

@inproceedings{10.1007/978-981-99-4749-2_9,
author = {Yuan, Lin and Lai, Jinling and Shen, Zhen and Yu, Wendong and Wei, Hongwei and Zhao, Ling and Xu, Zhijie and Wang, Xingang and Geng, Yushui},
title = {An Improved Method for CFNet Identifying Glioma Cells},
year = {2023},
isbn = {978-981-99-4748-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-4749-2_9},
doi = {10.1007/978-981-99-4749-2_9},
abstract = {Glioma cells are a type of tumor cells that originate in the supportive tissue of the brain and spinal cord called glial cells. These cells can proliferate rapidly, leading to the formation of a mass or tumor in the brain or spinal cord. Glioma cells can be classified into different types based on their morphology under a microscope and the genetic alterations they undergo. Some types of glioma cells are more aggressive and have a worse prognosis than others. Treatment for glioma cells typically involves a combination of surgery, radiation therapy, and chemotherapy. In recent years, deep learning techniques have found extensive applications in the field of medical imaging, particularly in neuroimaging. Deep learning has been used to aid physicians in automatically detecting and classifying images of glioma cells to improve diagnostic accuracy and treatment effectiveness. This approach typically requires a significant amount of data to train neural networks to identify specific features and types of glioma cells. On this basis, we proposed Cascade Fusion Network (CFNet) to try to improve the accuracy of identification of glioma cells.},
booktitle = {Advanced Intelligent Computing Technology and Applications: 19th International Conference, ICIC 2023, Zhengzhou, China, August 10–13, 2023, Proceedings, Part III},
pages = {97–105},
numpages = {9},
keywords = {Cascade Fusion Network, Deep learning, Glioma},
location = {Zhengzhou, China}
}

@inproceedings{10.1007/978-3-031-36402-0_34,
author = {Darapaneni, Narayana and Paduri, Anwesh Reddy and Gulani, Jayesh and Aithu, Sanath and Santhosh, M. M. and Varghese, Shaji},
title = {Nuclei Segmentation Approach for Computer Aided Diagnosis},
year = {2023},
isbn = {978-3-031-36401-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-36402-0_34},
doi = {10.1007/978-3-031-36402-0_34},
abstract = {Computer aided diagnosis based on computational pathology combines the concepts of pathology with computer science to develop automated mechanisms for interpretation of histological images. Nuclei segmentation is a type of computation pathology that enables the identification and separation of Nuclei cells in a histopathology image. These segmentation routines play a critical role in quantification of cellular structures and provide insights to identify abnormalities related to pathological conditions such as cancer. In order to determine the progress and severity of a pathological condition it is necessary to use tools that measure the properties of tissue structure in different stages of a disease. Biomedical image analysis for Nuclei segmentation using deep learning methodologies is an ongoing area of research wherein to improve histopathology examination it is desired to develop a model that can generalize and accurately segment different types of nuclei. However, creating a suitable model for this process is difficult due to variability in patient data, overlapping nuclei cell boundaries, image quality, background noise, and provision of well-annotated data. In this study, we describe our findings on the performance of a model developed using the standard U-Net architecture and compare it against a model that is developed based on pre-trained weights and models using ImageNet as the backbone. The aim is to develop a semantic segmentation model that can capture high-level information in terms of features from a data set that consists of Whole-slide images (WSI). The outcome of the research is to be able to select a semantic segmentation model that is efficient and fast to classify Nuclei cells for the given biomedical tissue images. The model uses a stochastic gradient descent optimizer and the evaluation criteria for this network i.e., the ground truth against prediction is based on Mean Intersection over Union.},
booktitle = {Multi-Disciplinary Trends in Artificial Intelligence: 16th International Conference, MIWAI 2023, Hyderabad, India, July 21–22, 2023, Proceedings},
pages = {368–379},
numpages = {12},
keywords = {Jaccard Index, histopathology, semantic segmentation, U-Net, deep learning, Nuclei segmentation},
location = {Hyberabad, India}
}

@inproceedings{10.1007/978-3-031-35302-4_2,
author = {Syed, Tabish A. and Wang, Yanan and Dileep, Drisya and Sirajuddin, Minhajuddin and Siddiqi, Kaleem},
title = {Ultrastructure Analysis of&nbsp;Cardiomyocytes and&nbsp;Their Nuclei},
year = {2023},
isbn = {978-3-031-35301-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-35302-4_2},
doi = {10.1007/978-3-031-35302-4_2},
abstract = {Cardiomyocytes are elongated and densely packed in the mammalian heart and connected end on end to achieve a functional syncytium. Qualitative accounts describe their nuclei as being elongated in their long axis direction, which might help to better distribute mechanical load and reduce mechanical stress during contraction. Alterations of nuclear orientation and shape have also been known to be associated with certain cardiomyopathies. Yet, to date, the alignment of cardiomyocytes and their nuclei at the cellular (micron) scale has not been assessed in a quantitative fashion. To examine this we developed 3D computer vision methods to segment myocytes and their nuclei in cleared and membrane stained thick 3D tissue sections from a wild type mouse heart, imaged using confocal microscopy. We extended a geometric flow based superpixel algorithm to 3D and then adaptively merged the resulting supervoxels to recover individual myocytes. In parallel we also applied recent popular deep learning based cell segmentation methods to the same data. Our experiments revealed a close alignment of myocyte orientation with nucleus orientation, with a median difference of approximately 10∘, and also showed that most cardiomyocytes contain only one or two nuclei. These findings pave the way for future investigations of the effect of specific cardiac diseases on nuclear shape, elongation and number.},
booktitle = {Functional Imaging and Modeling of the Heart: 12th International Conference, FIMH 2023, Lyon, France, June 19–22, 2023, Proceedings},
pages = {14–24},
numpages = {11},
keywords = {orientation analysis, cell segmentation, confocal microscopy, staining, tissue clearing, nuclei, cardiomyocytes},
location = {Lyon, France}
}

@inproceedings{10.1007/978-3-031-34048-2_60,
author = {Zhang, Jingwei and Kapse, Saarthak and Ma, Ke and Prasanna, Prateek and Vakalopoulou, Maria and Saltz, Joel and Samaras, Dimitris},
title = {Precise Location Matching Improves Dense Contrastive Learning in&nbsp;Digital Pathology},
year = {2023},
isbn = {978-3-031-34047-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-34048-2_60},
doi = {10.1007/978-3-031-34048-2_60},
abstract = {Dense prediction tasks such as segmentation and detection of pathological entities hold crucial clinical value in computational pathology workflows. However, obtaining dense annotations on large cohorts is usually tedious and expensive. Contrastive learning (CL) is thus often employed to leverage large volumes of unlabeled data to pre-train the backbone network. To boost CL for dense prediction, some studies have proposed variations of dense matching objectives in pre-training. However, our analysis shows that employing existing dense matching strategies on histopathology images enforces invariance among incorrect pairs of dense features and, thus, is imprecise. To address this, we propose a precise location-based matching mechanism that utilizes the overlapping information between geometric transformations to precisely match regions in two augmentations. Extensive experiments on two pretraining datasets (TCGA-BRCA, NCT-CRC-HE) and three downstream datasets (GlaS, CRAG, BCSS) highlight the superiority of our method in semantic and instance segmentation tasks. Our method outperforms previous dense matching methods by up to 7.2% in average precision for detection and 5.6% in average precision for instance segmentation tasks. Additionally, by using our matching mechanism in the three popular contrastive learning frameworks, MoCo-v2, VICRegL, and ConCL, the average precision in detection is improved by 0.7% to 5.2%, and the average precision in segmentation is improved by 0.7% to 4.0%, demonstrating generalizability. Our code is available at .},
booktitle = {Information Processing in Medical Imaging: 28th International Conference, IPMI 2023, San Carlos de Bariloche, Argentina, June 18–23, 2023, Proceedings},
pages = {783–794},
numpages = {12},
keywords = {Computational Pathology, Detection, Segmentation, Self-supervised learning, Dense contrastive learning},
location = {San Carlos de Bariloche, Argentina}
}

@inproceedings{10.1007/978-3-031-26387-3_26,
author = {Rymarczyk, Dawid and Pardyl, Adam and Kraus, Jaros\l{}aw and Kaczy\'{n}ska, Aneta and Skomorowski, Marek and Zieli\'{n}ski, Bartosz},
title = {ProtoMIL: Multiple Instance Learning with&nbsp;Prototypical Parts for&nbsp;Whole-Slide Image Classification},
year = {2023},
isbn = {978-3-031-26386-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-26387-3_26},
doi = {10.1007/978-3-031-26387-3_26},
abstract = {The rapid development of histopathology scanners allowed the digital transformation of pathology. Current devices fastly and accurately digitize histology slides on many magnifications, resulting in whole slide images (WSI). However, direct application of supervised deep learning methods to WSI highest magnification is impossible due to hardware limitations. That is why WSI classification is usually analyzed using standard Multiple Instance Learning (MIL) approaches, that do not explain their predictions, which is crucial for medical applications. In this work, we fill this gap by introducing ProtoMIL, a novel self-explainable MIL method inspired by the case-based reasoning process that operates on visual prototypes. Thanks to incorporating prototypical features into objects description, ProtoMIL unprecedentedly joins the model accuracy and fine-grained interpretability, as confirmed by the experiments conducted on five recognized whole-slide image datasets.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2022, Grenoble, France, September 19–23, 2022, Proceedings, Part I},
pages = {421–436},
numpages = {16},
keywords = {Interpretable deep learning, Digital pathology, Multiple instance learning},
location = {Grenoble, France}
}

@inproceedings{10.1007/978-3-540-75759-7_97,
author = {Darkner, Sune and Larsen, Rasmus and Paulsen, Rasmus R.},
title = {Analysis of Deformation of the Human Ear and Canal Caused by Mandibular Movement},
year = {2023},
isbn = {978-3-540-75758-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-75759-7_97},
doi = {10.1007/978-3-540-75759-7_97},
abstract = {Many hearing aid users experience physical discomfort when wearing their device. The main contributor to this problem is believed to be deformation of the ear and ear canal caused by movement of the mandible. Physical discomfort results from added pressure on soft tissue areas in the ear. Identifying features that can predict potential deformation is therefore important for identifying problematic cases in advance. A study on the physical deformation of the human ear and canal due to movement of the mandible is presented. The study is based on laser scannings of 30 pairs of ear impressions from 9 female and 21 male subjects. Two impressions have been taken from each subject, one with open mouth, and one with the mouth closed. All impressions are registered using non-rigid surface registration and a shape model is built. From each pair of impressions a deformation field is generated and propagated to the shape model, enabling the building of a deformation model in the reference frame of the shape model. A relationship between the two models is established, showing that the shape variation can explain approximately 50% of the variation in the deformation model. An hypothesis test for significance of the deformations for each deformation field reveals that all subjects have significant deformation at Tragus and in the canal. Furthermore, a relation between the magnitude of the deformation and the gender of the subject is demonstrated. The results are successfully validated by comparing the outcome to the anatomy by using a single set of high resolution histological sectionings of the region of interest.},
booktitle = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2007: 10th International Conference, Brisbane, Australia, October 29 - November 2, 2007, Proceedings, Part II},
pages = {801–808},
numpages = {8},
keywords = {Statistical Shape Model, Rigid Registration, Physical Discomfort, Deformation Model, Shape Model},
location = {Brisbane, QLD, Australia}
}

@inproceedings{10.1007/978-3-031-25082-8_38,
author = {Chattopadhyay, Nilanjan and Gehlot, Shiv and Singhal, Nitin},
title = {FUSION: Fully Unsupervised Test-Time Stain Adaptation via&nbsp;Fused Normalization Statistics},
year = {2023},
isbn = {978-3-031-25081-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-25082-8_38},
doi = {10.1007/978-3-031-25082-8_38},
abstract = {Staining reveals the micro-structure of the aspirate while creating histopathology slides. Stain variation, defined as a chromatic difference between the source and the target, is caused by varying characteristics during staining, resulting in a distribution shift and poor performance on the target. The goal of stain normalization is to match the target’s chromatic distribution to that of the source. However, stain normalisation causes the underlying morphology to distort, resulting in an incorrect diagnosis. We propose FUSION, a new method for promoting stain-adaption by adjusting the model to the target in an unsupervised test-time scenario, eliminating the necessity for significant labelling at the target end. FUSION works by altering the target’s batch-normalization statistics and fusing them with source statistics using a weighting factor. The algorithm reduces to one of two extremes based on the weighting factor. Despite the lack of training or supervision, FUSION surpasses existing equivalent algorithms for classification and dense predictions (segmentation), as demonstrated by comprehensive experiments on two public datasets.},
booktitle = {Computer Vision – ECCV 2022 Workshops: Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VII},
pages = {566–576},
numpages = {11},
keywords = {Stain variation, Unsupervised, Stain adaptation},
location = {<conf-loc content-type="InPerson">Tel Aviv, Israel</conf-loc>}
}

@inproceedings{10.1145/3569966.3571169,
author = {Yuan, Bo and Cao, Peng},
title = {Research on Image Information Restoration Algorithm of Printing Micro Dots Based on GAN},
year = {2022},
isbn = {9781450397780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569966.3571169},
doi = {10.1145/3569966.3571169},
abstract = {During printing and shooting, the degradation of printing micro dots significantly affects the decoding and reading of hidden anti-counterfeiting information. However, existing image restoration methods cannot effectively restore image information. Moreover, there are relatively few datasets related to halftone dot images, and most datasets differ from the real data. Therefore, we propose an end-to-end restoration model based on the single-image super-resolution information. Specifically, we constructed a PMD dataset for real printing of anti-counterfeiting scenes. Based on this dataset, we used the high-resolution image information as the target. The positional inclination of the degraded images is corrected using the blank and interline characteristics of the printing micro dots images. The restoration is completed with the help of feature extraction and upsample of ESRGAN. In addition, we propose evaluation measures suitable for error detection, correction, and decoding requirements for microscopic image information. The experimental results show that, within the noise tolerance range, the image information restored by our method has a maximum average bit error rate is 0.97% and a Euclidean distance is 0.00804 pixels, whereas traditional filtering measures cannot effectively restore image information. The experimental results verified the effectiveness and robustness of the proposed method.},
booktitle = {Proceedings of the 5th International Conference on Computer Science and Software Engineering},
pages = {615–622},
numpages = {8},
keywords = {printing anti-counterfeiting, image information restoration, generative adversarial networks, deep learning, bitmap image},
location = {<conf-loc>, <city>Guilin</city>, <country>China</country>, </conf-loc>},
series = {CSSE '22}
}

@inproceedings{10.1007/978-3-031-17979-2_12,
author = {Batchkala, George and Chakraborti, Tapabrata and McCole, Mark and Gleeson, Fergus and Rittscher, Jens},
title = {Active Data Enrichment by&nbsp;Learning What to&nbsp;Annotate in&nbsp;Digital Pathology},
year = {2022},
isbn = {978-3-031-17978-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-17979-2_12},
doi = {10.1007/978-3-031-17979-2_12},
abstract = {Our work aims to link pathology with radiology with the goal to improve the early detection of lung cancer. Rather than utilising a set of predefined radiomics features, we propose to learn a new set of features from histology. Generating a comprehensive lung histology report is the first vital step toward this goal. Deep learning has revolutionised the computational assessment of digital pathology images. Today, we have mature algorithms for assessing morphological features at the cellular and tissue levels. In addition, there are promising efforts that link morphological features with biologically relevant information. While promising, these efforts mostly focus on narrow, well-defined questions. Developing a comprehensive report that is required in our setting requires an annotation strategy that captures all clinically relevant patterns specified in the WHO guidelines. Here, we propose and compare approaches aimed to balance the dataset and mitigate the biases in learning by automatically prioritising regions with clinical patterns underrepresented in the dataset. Our study demonstrates the opportunities active data enrichment can provide and results in a new lung-cancer dataset annotated to a degree that is not readily available in the public domain.},
booktitle = {Cancer Prevention Through Early Detection: First International Workshop, CaPTion 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings},
pages = {118–127},
numpages = {10},
keywords = {Computational pathology, Histology annotation process, Unbalanced data, Image retrieval, Active and continual learning},
location = {<conf-loc content-type="InPerson">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16876-5_9,
author = {Qin, Wenkang and Jiang, Shan and Luo, Lin},
title = {Pathological Image Contrastive Self-supervised Learning},
year = {2022},
isbn = {978-3-031-16875-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16876-5_9},
doi = {10.1007/978-3-031-16876-5_9},
abstract = {Self-supervised learning methods have been receiving wide attentions in recent years, where contrastive learning starts to show encouraging performance in many tasks in the field of computer vision. Contrastive learning methods build pre-training weight parameters by crafting positive/negative samples and optimizing their distance in the feature space. It is easy to construct positive/negative samples on natural images, but the methods cannot directly apply to histopathological images because of the unique characteristics of the images such as staining invariance and vertical flip invariance. This paper proposes a general method for constructing clinical-equivalent positive sample pairs on histopathological images for applying contrastive learning on histopathological images. Results on the PatchCamelyon benchmark show that our method can improve model accuracy up to 6% while reducing the training costs, as well as reducing reliance on labeled data.},
booktitle = {Resource-Efficient Medical Image Analysis: First MICCAI Workshop, REMIA 2022, Singapore, September 22, 2022, Proceedings},
pages = {85–94},
numpages = {10},
keywords = {Representation learning, Self-supervised learning, Histopathological images},
location = {Singapore, Singapore}
}

@inproceedings{10.1007/978-3-031-17266-3_12,
author = {Li, Yilong and Wang, Yaqi and Dong, Le and Ye, Juan and Wang, Linyan and Ge, Ruiquan and Zhou, Huiyu and Zhang, Qianni},
title = {Light Annotation Fine Segmentation: Histology Image Segmentation Based on&nbsp;VGG Fusion with&nbsp;Global Normalisation CAM},
year = {2022},
isbn = {978-3-031-17265-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-17266-3_12},
doi = {10.1007/978-3-031-17266-3_12},
abstract = {Deep learning has been widely used to segment tumour regions in stained histopathology images. However, precise annotations are expensive and labour-consuming. To reduce the manual annotation workload, we propose a light annotation-based fine-level segmentation approach for histology images based on a VGG-based Fusion network with Global Normalisation CAM. The experts are only required to provide a rough segmentation annotation on the images, and then accurate fine-level segmentation boundaries can be produced using this method. To validate the proposed approach, three datasets with rough and fine quality segmentation annotation are built. The fine quality labels are used only as ground truth in evaluation. The VFGN-CAM method includes three main components: an annotation enhancement to boost boundary accuracy and model generalisability; a VGG Fusion module that integrates multi-scale tumour features; and a Global Normalisation CAM module that combines local and global gradient information of tumour regions. Our VGG fusion and Global Normalisation CAM outperform the existing methods with a Dice of 84.188%. The final improvement for our proposed methods against the original rough labels is around 22.8%.},
booktitle = {Computational Mathematics Modeling in Cancer Analysis: First International Workshop, CMMCA 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings},
pages = {121–130},
numpages = {10},
keywords = {Segmentation, Tumor, Annotation improvement},
location = {<conf-loc content-type="InPerson">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16440-8_55,
author = {Li, Yi and Yu, Yiduo and Zou, Yiwen and Xiang, Tianqi and Li, Xiaomeng},
title = {Online Easy Example Mining for&nbsp;Weakly-Supervised Gland Segmentation from&nbsp;Histology Images},
year = {2022},
isbn = {978-3-031-16439-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16440-8_55},
doi = {10.1007/978-3-031-16440-8_55},
abstract = {Developing an AI-assisted gland segmentation method from histology images is critical for automatic cancer diagnosis and prognosis; however, the high cost of pixel-level annotations hinders its applications to broader diseases. Existing weakly-supervised semantic segmentation methods in computer vision achieve degenerative results for gland segmentation, since the characteristics and problems of glandular datasets are different from general object datasets. We observe that, unlike natural images, the key problem with histology images is the confusion of classes owning to morphological homogeneity and low color contrast among different tissues. To this end, we propose a novel method Online Easy Example Mining (OEEM) that encourages the network to focus on credible supervision signals rather than noisy signals, therefore mitigating the influence of inevitable false predictions in pseudo-masks. According to the characteristics of glandular datasets, we design a strong framework for gland segmentation. Our results exceed many fully-supervised methods and weakly-supervised methods for gland segmentation over 4.6% and 6.04% at mIoU, respectively. Code is available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part IV},
pages = {578–587},
numpages = {10},
keywords = {Online Easy Example Mining, Histology image, Gland segmentation, Wealy-supervised semantic segmentation},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16440-8_2,
author = {Liu, Xiaoyu and Hu, Bo and Huang, Wei and Zhang, Yueyi and Xiong, Zhiwei},
title = {Efficient Biomedical Instance Segmentation via&nbsp;Knowledge Distillation},
year = {2022},
isbn = {978-3-031-16439-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16440-8_2},
doi = {10.1007/978-3-031-16440-8_2},
abstract = {Biomedical instance segmentation is vulnerable to complicated instance morphology, resulting in over-merge and over-segmentation. Recent advanced methods apply convolutional neural networks to predict pixel embeddings to overcome this problem. However, these methods suffer from heavy computational burdens and massive storage. In this paper, we present the first knowledge distillation method tailored for biomedical instance segmentation to transfer the knowledge from a cumbersome teacher network to a lightweight student one. Different from existing distillation methods on other tasks, we consider three kinds of essential knowledge of the instance segmentation task, i.e., instance-level features, instance relationships in the feature space and pixel-level instance boundaries. Specifically, we devise two distillation schemes: (i) instance graph distillation that transfers the knowledge of instance-level features and instance relationships by the instance graphs built from embeddings of the teacher-student pair, respectively, and (ii) pixel affinity distillation that converts pixel embeddings into pixel affinities and explicitly transfers the structured knowledge of instance boundaries encoded in affinities. Experimental results on a 3D electron microscopy dataset (CREMI) and a 2D plant phenotype dataset (CVPPP) demonstrate that the student models trained through our distillation method use fewer than 1% parameters and less than 10% inference time while achieving promising performance compared with corresponding teacher models. Code is available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part IV},
pages = {14–24},
numpages = {11},
keywords = {Biomedical instance segmentation, Pixel embeddings, Instance graph distillation, Pixel affinity distillation},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16434-7_5,
author = {Xie, Xingran and Wang, Yan and Li, Qingli},
title = {S3R: Self-supervised Spectral Regression for&nbsp;Hyperspectral Histopathology Image Classification},
year = {2022},
isbn = {978-3-031-16433-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16434-7_5},
doi = {10.1007/978-3-031-16434-7_5},
abstract = {Benefited from the rich and detailed spectral information in hyperspectral images (HSI), HSI offers great potential for a wide variety of medical applications such as computational pathology. But, the lack of adequate annotated data and the high spatiospectral dimensions of HSIs usually make classification networks prone to overfit. Thus, learning a general representation which can be transferred to the downstream tasks is imperative. To our knowledge, no appropriate self-supervised pre-training method has been designed for histopathology HSIs. In this paper, we introduce an efficient and effective Self-supervised Spectral Regression (S3R) method, which exploits the low rank characteristic in the spectral domain of HSI. More concretely, we propose to learn a set of linear coefficients that can be used to represent one band by the remaining bands via masking out these bands. Then, the band is restored by using the learned coefficients to reweight the remaining bands. Two pre-text tasks are designed: (1) S3R-CR, which regresses the linear coefficients, so that the pre-trained model understands the inherent structures of HSIs and the pathological characteristics of different morphologies; (2) S3R-BR, which regresses the missing band, making the model to learn the holistic semantics of HSIs. Compared to prior arts i.e., contrastive learning methods, which focuses on natural images, S3R converges at least 3 times faster, and achieves significant improvements up to 14% in accuracy when transferring to HSI classification tasks.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part II},
pages = {46–55},
numpages = {10},
keywords = {Self-supervised learning, Hyperspectral histopathology image classification, Low-rank},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16474-3_12,
author = {Biloborodova, Tetiana and Lomakin, Semen and Skarga-Bandurova, Inna and Krytska, Yana},
title = {Region of Interest Identification in the Cervical Digital Histology Images},
year = {2022},
isbn = {978-3-031-16473-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16474-3_12},
doi = {10.1007/978-3-031-16474-3_12},
abstract = {The region of interest (RoI) identification has a significant potential for yielding information about relevant histological features and is imperative to improve the effectiveness of digital pathology in clinical practice. The typical RoI is the stratified squamous epithelium (SSE) that appears on relatively small image areas. Hence, taking the entire image for classification adds noise caused by irrelevant background, making classification networks biased towards the background fragments. This paper proposes a novel approach for epithelium RoI identification based on automatic bounding boxes (bb) construction and SSE extraction and compares it with state-of-the-art histology RoI localization and detection techniques. Further classification of the extracted epithelial fragments based on DenseNet made it possible to effectively identify the SSE RoI in cervical histology images (CHI). The design brings significant improvement to the identification of diagnostically significant regions. For this research, we created two CHI datasets, the CHI-I containing 171 color images of the cervical histology microscopy and CHI-II containing 1049 extracted fragments of microscopy, which are the most considerable publicly available SSE datasets.},
booktitle = {Progress in Artificial Intelligence: 21st EPIA Conference on Artificial Intelligence, EPIA 2022, Lisbon, Portugal, August 31–September 2, 2022, Proceedings},
pages = {133–145},
numpages = {13},
keywords = {Bounding box, Cervical histology image, Region of Interest (RoI)},
location = {Lisbon, Portugal}
}

@inproceedings{10.1007/978-3-031-12053-4_55,
author = {Ali, Hesham and Elattar, Mustafa and Selim, Sahar},
title = {A Multi-scale Self-supervision Method for Improving Cell Nuclei Segmentation in Pathological Tissues},
year = {2022},
isbn = {978-3-031-12052-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-12053-4_55},
doi = {10.1007/978-3-031-12053-4_55},
abstract = {Nuclei detection and segmentation in histopathological images is a prerequisite step for quantitative analysis including morphological shape and size to help in identifying cancer prognosis. Digital pathology field aims to improve the quality of cancer diagnosis and has helped pathologists to reduce their efforts and time. Different deep learning architectures are widely used recently in Digital pathology field, yielding promising results in different problems. However, Deep convolutional neural networks (CNNs) need a large subset of labelled data that are not easily available all the time in the field of digital pathology. On the other hand, self-supervision methods are frequently used in different problems with the aim to overcome the lack of labelled data. In this study, we examine the impact of using self-supervision approaches on the segmentation problem. Also, we introduce a new multi-scale self-supervision method based on the zooming factor of the tissue. We compare the proposed method to the basic segmentation method and other popular self-supervision approaches that are used in other applications. The proposed Multi-scale self-supervision approach is applied on two publicly available pathology datasets. The results showed that the proposed approach outperforms Baseline U-Net by 0.2% and 0.02% for nuclei segmentation–mean Aggregated Jaccard Index (AJI), in TNBC and MoNuSeg, respectively.},
booktitle = {Medical Image Understanding and Analysis: 26th Annual Conference, MIUA 2022, Cambridge, UK, July 27–29, 2022, Proceedings},
pages = {751–763},
numpages = {13},
keywords = {Nuclei segmentation, Histopathological images, Self-supervision, Transfer learning, Deep learning},
location = {<conf-loc content-type="InPerson">Cambridge, United Kingdom</conf-loc>}
}

@inproceedings{10.1145/3532213.3532314,
author = {Li, Tianyu and Li, Guangxu and Li, Fangting and Zhang, Chen},
title = {A SuperPoint-based Method for Automatic Stitching of Corneal Nerve Microscopy Images},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532314},
doi = {10.1145/3532213.3532314},
abstract = {This paper presents an end-to-end image stitching method for a large number of in vivo corneal confocal microscopy images. We firstly learn a feature point detector for self-labeling from a randomly generated synthetic shape dataset. And then the self-labeling detector is utilized to the confocal microscopic images to match the nerve structure. Finally the microscopic image data with pseudo-ground truth feature points is utilized to the training model again to obtain the stable and repeatable feature points. Images registration is carried out using the Brute-Force matching and the adapted affine transform. In the experiments, the effectiveness of the method is verified by comparing with the classical feature detection algorithm and existing stitching methods. A large scale of corneal confocal microscopy is obtained with accurate joint of corneal nerves.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {664–669},
numpages = {6},
keywords = {laser scanning in vivo confocal microscopy, images stitching, corneal nerve, computer aided diagnosis},
location = {Tianjin, China},
series = {ICCAI '22}
}

@inproceedings{10.1007/978-3-031-08751-6_29,
author = {Levental, Maksim and Chard, Ryan and Chard, Kyle and Foster, Ian and Wildenberg, Gregg},
title = {Ultrafast Focus Detection for&nbsp;Automated Microscopy},
year = {2022},
isbn = {978-3-031-08750-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08751-6_29},
doi = {10.1007/978-3-031-08751-6_29},
abstract = {Technological advancements in modern scientific instruments, such as  scanning electron microscopes (SEMs), have significantly increased data acquisition rates and image resolutions enabling new questions to be explored; however, the resulting data volumes and velocities, combined with automated experiments, are quickly overwhelming scientists as there remain crucial steps that require human intervention, for example reviewing image focus. We present a fast out-of-focus detection algorithm for electron microscopy images collected serially and demonstrate that it can be used to provide near-real-time quality control for neuroscience workflows. Our technique, Multi-scale Histologic Feature Detection, adapts classical computer vision techniques and is based on detecting various fine-grained histologic features. We exploit the inherent parallelism in the technique to employ GPU primitives in order to accelerate characterization. We show that our method can detect out-of-focus conditions within just 20&nbsp;ms. To make these capabilities generally available, we deploy our feature detector as an on-demand service and show that it can be used to determine the degree of focus in approximately 230&nbsp;ms, enabling near-real-time use.},
booktitle = {Computational Science – ICCS 2022: 22nd International Conference, London, UK, June 21–23, 2022, Proceedings, Part I},
pages = {403–416},
numpages = {14},
location = {London, United Kingdom}
}

@inproceedings{10.1007/978-3-031-09037-0_12,
author = {Qiang, Yu and He, Shixu and Ding, Renpeng and Ma, Kailong and Hou, Yong and Zhou, Yan and Rohr, Karl},
title = {A Framework for&nbsp;Registration of&nbsp;Multi-modal Spatial Transcriptomics Data},
year = {2022},
isbn = {978-3-031-09036-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-09037-0_12},
doi = {10.1007/978-3-031-09037-0_12},
abstract = {Observing the spatial characteristics of gene expression by image-based spatial transcriptomics technology allows studying gene activity across different cells and intracellular structures. We present a framework for the registration and analysis of transcriptome images and immunostaining images. The method is based on particle filters and jointly exploits intensity information and image features. We applied our approach to synthetic data as well as real transcriptome images and immunostaining microscopy images of the mouse brain. It turns out that our approach accurately registers the multi-modal images and yields better results than a state-of-the-art method.},
booktitle = {Pattern Recognition and Artificial Intelligence: Third International Conference, ICPRAI 2022, Paris, France, June 1–3, 2022, Proceedings, Part I},
pages = {134–145},
numpages = {12},
keywords = {Registration, Multi-modal images, Spatial transcriptomics},
location = {Paris, France}
}

@inproceedings{10.1145/3512388.3512416,
author = {Gu, Wen and Wang, Shenghui and Zhao, Shuaihua and Wan, Lili and Zhu, Zhenfeng},
title = {HistoSegResT: A Weakly Supervised Learning Method for Histopathology Image Segmentation},
year = {2022},
isbn = {9781450395465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512388.3512416},
doi = {10.1145/3512388.3512416},
abstract = {The morphology of glands, such as size and contour, has been used routinely by pathologists to diagnose the malignant degree of several adenocarcinomas in the process of pathological diagnosis. To automatically segment the gland regions, fully supervised segmentation algorithms require labor-intensive and time- consuming labeling at the pixel level. In this paper, we propose a weakly supervised learning method HistoSegResT (HSRT), which only uses image-level labels (i.e., malignant and benign) to complete histopathology image segmentation. In HSRT, the structure of CNN is used to extract the underlying features of the image, and the self-attention mechanism of the transformer is used to encode the long-range dependencies in histopathology images. In addition, a reconstruction loss is designed to discover the most integrated region of the object. A series of experiments show that the proposed HSRT method outperformed existing state-of-the-art methods with the same level of supervision on the GlaS dataset and can effectively relieve under-activation and over-activation of generated CAMs.},
booktitle = {Proceedings of the 2022 5th International Conference on Image and Graphics Processing},
pages = {189–195},
numpages = {7},
keywords = {Weakly Supervised Learning, Transformer, Self-Attention, Histopathology Image Segmentation, Glands, Convolutional Neural Network},
location = {Beijing, China},
series = {ICIGP '22}
}

@inproceedings{10.1007/978-3-319-10470-6_60,
author = {Zhang, Xiaofan and Yang, Lin and Liu, Wei and Su, Hai and Zhang, Shaoting},
title = {Mining Histopathological Images via Composite Hashing and Online Learning},
year = {2022},
isbn = {978-3-319-10469-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-10470-6_60},
doi = {10.1007/978-3-319-10470-6_60},
abstract = {With a continuous growing amount of annotated histopathological images, large-scale and data-driven methods potentially provide the promise of bridging the semantic gap between these images and their diagnoses. The purpose of this paper is to increase the scale at which automated systems can entail scalable analysis of histopathological images in massive databases. Specifically, we propose a principled framework to unify hashing-based image retrieval and supervised learning. Concretely, composite hashing is designed to simultaneously fuse and compress multiple high-dimensional image features into tens of binary hash bits, enabling scalable image retrieval with a very low computational cost. Upon a local data subset that retains the retrieved images, supervised learning methods are applied on-the-fly to model image structures for accurate classification. Our framework is validated thoroughly on 1120 lung microscopic tissue images by differentiating adenocarcinoma and squamous carcinoma. The average accuracy is 87.5% with only 17ms running time, which compares favorably with other commonly used methods.},
booktitle = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2014},
pages = {479–486},
numpages = {8},
keywords = {Squamous Carcinoma, Online Learn, Image Retrieval, Texture Feature, Support Vector Machine}
}

@inproceedings{10.1007/978-3-030-88010-1_32,
author = {Guo, Yuanhao and Huang, Jiaxing and Zhou, Yanfeng and Luo, Yaoru and Li, Wenjing and Yang, Ge},
title = {Segmentation of Intracellular Structures in Fluorescence Microscopy Images by Fusing Low-Level Features},
year = {2021},
isbn = {978-3-030-88009-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88010-1_32},
doi = {10.1007/978-3-030-88010-1_32},
abstract = {Intracellular organelles such as the endoplasmic reticulum, mitochondria, and the cell nucleus exhibit complex structures and diverse morphologies. These intracellular structures (ICSs) play important roles in serving essential physiological functions of cells. Their accurate segmentation is crucial to many important life sciences and clinical applications. When using deep learning-based segmentation models to extract ICSs in fluorescence microscopy images (FLMIs), we find that U-Net provides superior performance, while other well-designed models such as DeepLabv3+ and SegNet perform poorly. By investigating the relative importance of the features learned by U-Net, we find that low-level features play a dominant role. Therefore, we develop a simple strategy that modifies general-purpose segmentation models by fusing low-level features via a decoder architecture to improve their performance in segmenting ICSs from FLMIs. For a given segmentation model, we first use a group of convolutions at the original image scale as the input layer to obtain low-level features. We then use a decoder to fuse the multi-scale features, which directly passes information of low-level features to the prediction layer. Experimental results on two custom datasets, ER and MITO, and a public dataset NUCLEUS, show that the strategy substantially improves the performance of all the general-purpose models tested in segmentation of ICSs from FLMIs. Data and code of this study are available at .},
booktitle = {Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV 2021, Beijing, China, October 29 – November 1, 2021, Proceedings, Part III},
pages = {386–397},
numpages = {12},
location = {Beijing, China}
}

@inproceedings{10.1145/3474085.3475489,
author = {Wang, Liuan and Sun, Li and Zhang, Mingjie and Zhang, Huigang and Ping, Wang and Zhou, Rong and Sun, Jun},
title = {Exploring Pathologist Knowledge for Automatic Assessment of Breast Cancer Metastases in Whole-slide Image},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475489},
doi = {10.1145/3474085.3475489},
abstract = {Automatic assessment of breast cancer metastases plays an important role to help pathologist reduce the time-consuming work in histopathological whole-slide image diagnosis. From the utilization of knowledge point of view, the low-magnification level and high-magnification level are carefully checked by the pathologists for tumor pattern and cell tumor characteristic. In this paper, we propose a novel automatic patient-level tumor segmentation and classification method, which makes full use of the diagnosis knowledge clues from pathologists. For tumor segmentation, a multi-level view DeepLabV3+ (MLV-DeepLabV3+) is designed to explore the distinguishing features of cell characteristics between tumor and normal tissue. Furthermore, the expert segmentation models are selected and integrated by Pareto-front optimization to imitate the expert consultation to get perfect diagnosis. For wholeslide classification, multi-level magnifications are adaptive checked to focus on the effective features in different magnification. The experimental results demonstrate that our pathologist knowledge-based automatic assessment of whileslide image is effective and robust on the public benchmark dataset.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {255–263},
numpages = {9},
keywords = {tumor segmentation and classification, pathologist knowledge, breast cancer metastases},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1007/978-3-030-88210-5_8,
author = {Deng, Ruining and Liu, Quan and Bao, Shunxing and Jha, Aadarsh and Chang, Catie and Millis, Bryan A. and Tyska, Matthew J. and Huo, Yuankai},
title = {CaCL: Class-Aware Codebook Learning for Weakly Supervised Segmentation on Diffuse Image Patterns},
year = {2021},
isbn = {978-3-030-88209-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88210-5_8},
doi = {10.1007/978-3-030-88210-5_8},
abstract = {Weakly supervised learning has been rapidly advanced in biomedical image analysis to achieve pixel-wise labels (segmentation) from image-wise annotations (classification), as biomedical images naturally contain image-wise labels in many scenarios. The current weakly supervised learning algorithms from the computer vision community are largely designed for focal objects (e.g., dogs and cats). However, such algorithms are not optimized for diffuse patterns in biomedical imaging (e.g., stains and fluorescence in microscopy imaging). In this paper, we propose a novel class-aware codebook learning (CaCL) algorithm to perform weakly supervised learning for diffuse image patterns. Specifically, the CaCL algorithm is deployed to segment protein expressed brush border regions from histological images of human duodenum. Our contribution is three-fold: (1) we approach the weakly supervised segmentation from a novel codebook learning perspective; (2) the CaCL algorithm segments diffuse image patterns rather than focal objects; and (3) the proposed algorithm is implemented in a multi-task framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) via joint image reconstruction, classification, feature embedding, and segmentation. The experimental results show that our method achieved superior performance compared with baseline weakly supervised algorithms. The code is available at .},
booktitle = {Deep Generative Models, and Data Augmentation, Labelling, and Imperfections: First Workshop, DGM4MICCAI 2021, and First Workshop, DALI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, October 1, 2021, Proceedings},
pages = {93–102},
numpages = {10},
keywords = {AutoEncoder, Segmentation, Weakly supervised learning},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87722-4_6,
author = {Mahapatra, Dwarikanath and Kuanar, Shiba and Bozorgtabar, Behzad and Ge, Zongyuan},
title = {Self-supervised Learning of Inter-label Geometric Relationships for Gleason Grade Segmentation},
year = {2021},
isbn = {978-3-030-87721-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87722-4_6},
doi = {10.1007/978-3-030-87722-4_6},
abstract = {Segmentation of Prostate Cancer (PCa) tissues from Gleason graded histopathology images is vital for accurate diagnosis. Although deep learning (DL) based segmentation methods achieve state-of-the-art accuracy, they rely on large datasets with manual annotations. We propose a method to synthesize PCa histopathology images by learning the geometrical relationship between different disease labels using self-supervised learning. Manual segmentation maps from the training set are used to train a Shape Restoration Network (ShaRe-Net) that predicts missing mask segments in a self-supervised manner. Using DenseUNet as the backbone generator architecture we incorporate latent variable sampling to inject diversity in the image generation process and thus improve robustness. Experimental results demonstrate the superiority of our method over competing image synthesis methods for segmentation tasks. Ablation studies show the benefits of integrating geometry and diversity in generating high-quality images. Our self-supervised approach with limited class-labeled data achieves better performance than fully supervised learning.},
booktitle = {Domain Adaptation and Representation Transfer, and Affordable Healthcare and AI for Resource Diverse Global Health: Third MICCAI Workshop, DART 2021, and First MICCAI Workshop, FAIR 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27 and October 1, 2021, Proceedings},
pages = {57–67},
numpages = {11},
keywords = {GANs, Geometric modeling, Self-supervised learning},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-89128-2_19,
author = {Putzu, Lorenzo and Untesco, Maxim and Fumera, Giorgio},
title = {Automatic Myelofibrosis Grading from Silver-Stained Images},
year = {2021},
isbn = {978-3-030-89127-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89128-2_19},
doi = {10.1007/978-3-030-89128-2_19},
abstract = {Histopathology studies the tissues to provide evidence of a disease, type and grade. Usually, the interpretation of these tissue specimens is performed under a microscope by human experts, but since the advent of digital pathology, the slides are digitised, shared and viewed remotely, facilitating diagnosis, prognosis and treatment planning. Furthermore, digital slides can be analysed automatically with computer vision methods to provide diagnostic support, reduce subjectivity and improve efficiency. This field has attracted many researchers in recent years who mainly focused on the analysis of cells morphology on Hematoxylin &amp; Eosin stained samples. In this work, instead, we focused on the analysis of reticulin fibres from silver stained images. This task has been addressed rarely in the literature, mainly due to the total absence of public data sets, but it is beneficial to assess the presence of fibrotic degeneration. One of them is myelofibrosis, characterised by an excess of fibrous tissue. Here we propose an automated method to grade myelofibrosis from image patches. We evaluated different Convolutional Neural Networks for this purpose, and the obtained results demonstrate that myelofibrosis can be identified and graded automatically.},
booktitle = {Computer Analysis of Images and Patterns: 19th International Conference, CAIP 2021, Virtual Event, September 28–30, 2021, Proceedings, Part I},
pages = {195–205},
numpages = {11},
keywords = {Convolutional neural networks, Image classification, Myelofibrosis grading, Silver staining, Digital pathology}
}

@inproceedings{10.1007/978-3-030-87237-3_55,
author = {Ke, Jing and Shen, Yiqing and Liang, Xiaoyao and Shen, Dinggang},
title = {Contrastive Learning Based Stain Normalization Across Multiple Tumor in Histopathology},
year = {2021},
isbn = {978-3-030-87236-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87237-3_55},
doi = {10.1007/978-3-030-87237-3_55},
abstract = {Generative adversarial network (GAN) has been a prevalence in color normalization techniques to assist deep learning analysis in H&amp;E stained histopathology images. The widespread adoption of GAN has effectively released pathologists from the heavy manual workload in the conventional template image selection. However, the transformation might cause significant information loss, or generate undesirable results such as mode collapse in all likelihood, which may affect the performance in the succeeding diagnostic task. To address the issue, we propose a contrastive learning method with a color-variation constraint, which maximally retains the recognizable phenotypic features at the training of a color-normalization GAN. In a self-supervised manner, the discriminative tissue patches across multiple types of tumors are clustered, taken as the salient input to feed the GAN. Empirically, the model is evaluated by public datasets of large cohorts on different cancer diseases from TCGA and Camelyon16. We show better phenotypical recognizability along with an improved performance in the histology image classification.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part VIII},
pages = {571–580},
numpages = {10},
keywords = {Generative adversarial network, Contrastive learning, Self-supervised learning, Stain normalization, Histopathology},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87237-3_46,
author = {Reich, Christoph and Prangemeier, Tim and Wildner, Christian and Koeppl, Heinz},
title = {Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy},
year = {2021},
isbn = {978-3-030-87236-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87237-3_46},
doi = {10.1007/978-3-030-87237-3_46},
abstract = {Time-lapse fluorescent microscopy (TLFM) combined with predictive mathematical modelling is a powerful tool to study the inherently dynamic processes of life on the single-cell level. Such experiments are costly, complex and labour intensive. A complimentary approach and a step towards in silico experimentation, is to synthesise the imagery itself. Here, we propose Multi-StyleGAN as a descriptive approach to simulate time-lapse fluorescence microscopy imagery of living cells, based on a past experiment. This novel generative adversarial network synthesises a multi-domain sequence of consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live yeast cells in microstructured environments and train on a dataset recorded in our laboratory. The simulation captures underlying biophysical factors and time dependencies, such as cell morphology, growth, physical interactions, as well as the intensity of a fluorescent reporter protein. An immediate application is to generate additional training and validation data for feature extraction algorithms or to aid and expedite development of advanced experimental techniques such as online monitoring or control of cells.Code and dataset is available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part VIII},
pages = {476–486},
numpages = {11},
keywords = {Synthetic biology, Systems biology, Time-lapse fluorescence microscopy, Deep learning, Generative adversarial networks},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87231-1_59,
author = {Xiao, Rudan and Debreuve, Eric and Ambrosetti, Damien and Descombes, Xavier},
title = {Renal Cell Carcinoma Classification from&nbsp;Vascular Morphology},
year = {2021},
isbn = {978-3-030-87230-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87231-1_59},
doi = {10.1007/978-3-030-87231-1_59},
abstract = {Renal Cell Carcinoma (RCC) is one of the most common malignancies, and pathological diagnosis is the gold standard for RCC diagnostic method. Recognizing the type of RCC tumor and the possibility of cell migration highly depends on the geometric and topological properties of the vascular network. Motivated by the diagnosis pipeline, we explore whether the vascular network visible in RCC histopathological images is sufficient to characterize the RCC subtype. To achieve this, we firstly build a new vascular network-based RCC histopathological image dataset of 7 patients, namely VRCC200, with 200 well-labeled vascular network annotations. Based on these vascular networks of RCC histopathological images, we propose new hand-crafted features, namely skeleton features and lattice features. These features well represent the geometric and topological properties of the vascular networks of RCC histopathological images. Then we build strong benchmark results with various algorithms (both traditional and deep learning models) on the VRCC200 dataset. The result of skeleton and lattice features can outperform popular deep learning models. Finally, we further prove the robustness and advantage of proposed features on an additional database VRCC60 of 20 patients, with 60 vascular annotated images. All of the results of our experiments prove that the vascular network structure of RCC is one of the most important biomarkers for RCC diagnosis.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part VI},
pages = {611–621},
numpages = {11},
keywords = {RCC classification, Lattice features, Skeleton features, Vascular network, RCC histopathological image dataset},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87156-7_8,
author = {Venkataramanan, Aishwarya and Laviale, Martin and Figus, C\'{e}cile and Usseglio-Polatera, Philippe and Pradalier, C\'{e}dric},
title = {Tackling Inter-class Similarity and Intra-class Variance for Microscopic Image-Based Classification},
year = {2021},
isbn = {978-3-030-87155-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87156-7_8},
doi = {10.1007/978-3-030-87156-7_8},
abstract = {Automatic classification of aquatic microorganisms is based on the morphological features extracted from individual images. The current works on their classification do not consider the inter-class similarity and intra-class variance that causes misclassification. We are particularly interested in the case where variance within a class occurs due to discrete visual changes in microscopic images. In this paper, we propose to account for it by partitioning the classes with high variance based on the visual features. Our algorithm automatically decides the optimal number of sub-classes to be created and consider each of them as a separate class for training. This way, the network learns finer-grained visual features. Our experiments on two databases of freshwater benthic diatoms and marine plankton show that our method can outperform the state-of-the-art approaches for classification of these aquatic microorganisms.},
booktitle = {Computer Vision Systems: 13th International Conference, ICVS 2021, Virtual Event, September 22-24, 2021, Proceedings},
pages = {93–103},
numpages = {11},
keywords = {Inter-class similarity, Intra-class variance, Automatic clustering, Micro-organisms classification}
}

@inproceedings{10.1145/3477206.3477456,
author = {Denizot, Audrey and Cal\`{\i}, Corrado and Berry, Hugues and De Schutter, Erik},
title = {Stochastic Spatially-Extended Simulations Predict the Effect of ER Distribution on Astrocytic Microdomain Ca2+ Activity},
year = {2021},
isbn = {9781450387101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477206.3477456},
doi = {10.1145/3477206.3477456},
abstract = {Astrocytes are cells of the central nervous system that can regulate neuronal activity. Most astrocyte-neuron communication occurs at so-called tripartite synapses, where calcium signals are triggered in astrocytes by neuronal activity, resulting in the release of neuroactive molecules by the astrocyte. Most astrocytic Ca2+ signals occur in very thin astrocytic branchlets, containing low copy number of molecules, so that reactions are highly stochastic. As those sub-cellular compartments cannot be resolved by diffraction-limited microscopy techniques, stochastic reaction-diffusion computational approaches can give crucial insights on astrocyte activity. Here, we use our stochastic voxel-based model of IP3R-mediated Ca2+ signals to investigate the effect of the distance between the synapse and the closest astrocytic endoplasmic reticulum (ER) on neuronal activity-induced Ca2+ signals. Simulations are performed in three dimensional meshes characterized by various ER-synapse distances. Our results suggest that Ca2+ peak amplitude, duration and frequency decrease rapidly as ER-synapse distance increases. We propose that this effect mostly results from the increased cytosolic volume of branchlets that are characterized by larger ER-synapse distances. In particular, varying ER-synapse distance with constant cytosolic volume does not affect local Ca2+ activity. This study illustrates the insights that can be provided by three-dimensional stochastic reaction-diffusion simulations on the biophysical constraints that shape the spatio-temporal characteristics of astrocyte activity at the nanoscale.},
booktitle = {Proceedings of the Eight Annual ACM International Conference on Nanoscale Computing and Communication},
articleno = {20},
numpages = {5},
keywords = {tripartite synapses, reaction-diffusion simulations, computational neuroscience, calcium microdomain, astrocytes},
location = {Virtual Event, Italy},
series = {NANOCOM '21}
}

@inproceedings{10.1109/CEC45853.2021.9504811,
author = {Attaoui, Mohammed Oualid and Azzag, Hanene and Keskes, Nabil and Lebbah, Mustapha},
title = {A New Subspace Multi-Objective Approach for the Clustering and Selection of Regions of Interests in Histopathological Images},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC45853.2021.9504811},
doi = {10.1109/CEC45853.2021.9504811},
abstract = {Histopathology images represent a source of assistance for pathologists when diagnosing Cancer. However, in histopathology or in cancer image analysis, pathologists mostly diagnose the pathology as positive if a small part of it is considered cancer tissue. These small parts are called regions of interest (ROI) or patches. Finding the relevant patches is crucial as it can save computation time and memory. Subspace clustering discovers clusters embedded in multiple, overlapping subspaces of high dimensional data. It is an extension of feature selection, which tries to identify relevant subsets of features that are relevant to the clustering process. However, subspace clustering algorithms provide a partition of the data based on one cluster validity measure, assuming a homogeneous similarity measure over the entire data set makes the algorithms not robust to variations in the data characteristics. Therefore, it is beneficial to optimize multiple validity indices simultaneously to capture different aspects of the datasets. The goal of Multi-Objective clustering methods (MOC) is to derive significant clusters by applying two or more objective functions. This paper proposes a new clustering algorithm for patch selection based on subspace and multi-objective clustering to find the data’s best partitioning and the images’ most relevant patches.},
booktitle = {2021 IEEE Congress on Evolutionary Computation (CEC)},
pages = {556–563},
numpages = {8},
location = {Krak\'{o}w, Poland}
}

@inproceedings{10.1145/3449639.3459331,
author = {Fitzgerald, Seth G. and Delaney, Gary W. and Howard, David and Maire, Frederic},
title = {Evolving soft robotic jamming grippers},
year = {2021},
isbn = {9781450383509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449639.3459331},
doi = {10.1145/3449639.3459331},
abstract = {Jamming Grippers are a novel class of soft robotic actuators that can robustly grasp and manipulate objects of arbitrary shape. They are formed by placing a granular material within a flexible skin connected to a vacuum pump and function by pressing the un-jammed gripper against a target object and evacuating the air to transition the material to a jammed (solid) state, gripping the target object. However, due to the complex interactions between grain morphology and target object shape, much uncertainty still remains regarding optimal constituent grain shapes for specific gripping applications. We address this challenge by utilising a modern Evolutionary Algorithm, NSGA-III, combined with a Discrete Element Method soft robot model to perform a multi-objective optimisation of grain morphology for use in jamming grippers for a range of target object sizes. Our approach optimises the microscopic properties of the system to elicit bespoke functional granular material performance driven by the complex relationship between the individual particle morphologies and the related emergent behaviour of the bulk state. Results establish the important contribution of grain morphology to gripper performance and the critical role of local surface curvature and the length scale governed by the relative sizes of the grains and target object.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {102–110},
numpages = {9},
keywords = {soft robotics, multi-objective optimization, granular materials, evolutionary robotics, discrete element method, computational modelling, NSGA-III},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.1145/3388440.3414917,
author = {Abdilleh, Kawther and Aguilar, Boris and Thomson, J. Ross},
title = {Multi-omics data integration in the Cloud: Analysis of Statistically Significant Associations Between Clinical and Molecular Features in Breast Cancer},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3414917},
doi = {10.1145/3388440.3414917},
abstract = {Breast Cancers are among the most common forms of cancers impacting women with over 1 million diagnoses every year worldwide. They are complex cancers characterized by distinct clinical outcomes, morphological and molecular features. As high-throughput technologies generating data at the mRNA and protein levels become cheaper and more accessible, researchers are now able to study these entities in concert with clinical features to gain a more holistic picture of Breast Cancer and other complex diseases.In this poster, we aimed at identifying the concordance or discordance of mRNA and protein expressions that are significantly associated with Breast Cancer histological subtypes and other relevant clinical features. We employed a novel cloud-based approach to analyze these statistical associations using available genomic, proteomic, and clinical cancer data on the Google Cloud through the ISB-CGC, one of the National Cancer Institute's (NCI) Cloud Resources.Our results indicate that, considering all available clinical features, a considerable number of molecules (genes and proteins) are significantly associated with the Breast Cancer histological subtypes of infiltrating ductal carcinoma and infiltrating lobular carcinoma, two common forms associated with invasive Breast Cancer. Moreover, statistically significant associations were overrepresented for molecules involved in PI3K/AKT signaling, negative regulation of the PI3K/AKT network and extra-nuclear estrogen signaling. Taken together, these results demonstrate how powerful cloud-based analytics can be in identifying novel molecular relationships relevant for Breast Cancer. text here.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {85},
numpages = {1},
keywords = {TCGA, Multi-omics data integration, Cloud Computing, Cancer Genomics, BigQuery},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1007/978-3-030-60633-6_20,
author = {Li, Chen and Zhang, Jiawei and Kulwa, Frank and Qi, Shouliang and Qi, Ziyu},
title = {A SARS-CoV-2 Microscopic Image Dataset with Ground Truth Images and Visual Features},
year = {2020},
isbn = {978-3-030-60632-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60633-6_20},
doi = {10.1007/978-3-030-60633-6_20},
abstract = {SARS-CoV-2 has characteristics of wide contagion and quick propagation velocity. To analyse the visual information of it, we build a SARS-CoV-2 Microscopic Image Dataset (SC2-MID) with 48 electron microscopic images and also prepare their ground truth images. Furthermore, we extract multiple classical features and novel deep learning features to describe the visual information of SARS-CoV-2. Finally, it is proved that the visual features of the SARS-CoV-2 images which are observed under the electron microscopic can be extracted and analysed.},
booktitle = {Pattern Recognition and Computer Vision: Third Chinese Conference, PRCV 2020, Nanjing, China, October 16–18, 2020, Proceedings, Part I},
pages = {244–255},
numpages = {12},
keywords = {SARS-CoV-2, Image dataset, Visual features, Ground truth image, Classical feature extraction, Deep learning feature extraction},
location = {<conf-loc>Nanjing, China</conf-loc>}
}

@inproceedings{10.1007/978-3-030-59861-7_61,
author = {Wang, Pei and Chung, Albert C. S.},
title = {Learning Hierarchical Semantic Correspondence and Gland Instance Segmentation},
year = {2020},
isbn = {978-3-030-59860-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59861-7_61},
doi = {10.1007/978-3-030-59861-7_61},
abstract = {The morphology statistics of colon glands is a key feature for pathologists to diagnose colorectal cancer. Current gland instance segmentation methods show good overall performances, but accurate segmentation of extremely deformed glands in highly malignant cases or some rare benign cases remains to be challenging . In this paper, we propose a hybrid model that learns hierarchical semantic feature matching from histological pairs in an attentive process, where both spatial details and morphological appearances can be well preserved and balanced, especially for the glands with severe deformation. A consistency loss function is also introduced to enforce simultaneous satisfaction of semantic correspondence and gland instance segmentation on the pixel-level. The novel proposed model is validated on two publicly available colon gland datasets GlaS and CRAG. The model successfully boosts the segmentation performances on greatly mutated or deformed cases, and outperforms the state-of-the-art approaches.},
booktitle = {Machine Learning in Medical Imaging: 11th International Workshop, MLMI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings},
pages = {603–613},
numpages = {11},
keywords = {Gland instance segmentation, Attentitive process, Consistency loss, Hierarchical semantic correspondence},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59722-1_53,
author = {Parvatikar, Akash and Choudhary, Om and Ramanathan, Arvind and Navolotskaia, Olga and Carter, Gloria and Tosun, Akif Burak and Fine, Jeffrey L. and Chennubhotla, S. Chakra},
title = {Modeling Histological Patterns for Differential Diagnosis of Atypical Breast Lesions},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_53},
doi = {10.1007/978-3-030-59722-1_53},
abstract = {Our goal in this paper is to build parametric models for a dictionary of histological patterns that aid in the differential diagnosis of atypical breast lesions and evaluate the inferential power of these hand-crafted features. Diagnosis of high-risk atypical breast lesions is challenging and remains a critical component of breast cancer screening, presenting even for experienced pathologists a more difficult classification problem than the binary detection task of cancer vs not-cancer. Following guidelines in the WHO classification of the tumors of the breast (an essential reference for pathologists, clinicians and researchers) and in consultation with our team of breast sub-specialists (N = 3), we assembled a visual dictionary of sixteen histological patterns (e.g., cribriform, picket-fence), a subset that pathologists frequently use in making complex diagnostic decisions of atypical breast lesions. We invoke parametric models for each pattern using a mix of unary, binary and ternary features that account for morphological and architectural tissue properties. We use 1441 ductal regions of interest (ROIs) extracted automatically from 93 whole slide images (WSIs) with a computational pathology pipeline. We collected diagnostic labels for all of the ROIs: normal and columnar cell changes (CCC) as low-risk benign lesions (=&nbsp;1124), and flat epithelium atypia (FEA) and atypical ductal hyperplasia (ADH) as high-risk benign lesions (=&nbsp;317). We generate likelihood maps for each dictionary pattern across a given ROI and integrate this information to determine a diagnostic label of high- or low-risk. Our method has comparable classification accuracies to the pool of breast pathology sub-specialists. Our study enables a deeper understanding of the discordance among pathologists in diagnosing atypical breast lesions.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {550–560},
numpages = {11},
keywords = {Computational pathology, Parametric models, Visual feature dictionary, Atypical breast lesions},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59722-1_46,
author = {Abbet, Christian and Zlobec, Inti and Bozorgtabar, Behzad and Thiran, Jean-Philippe},
title = {Divide-and-Rule: Self-Supervised Learning for Survival Analysis in Colorectal Cancer},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_46},
doi = {10.1007/978-3-030-59722-1_46},
abstract = {With the long-term rapid increase in incidences of colorectal cancer (CRC), there is an urgent clinical need to improve risk stratification. The conventional pathology report is usually limited to only a few histopathological features. However, most of the tumor microenvironments used to describe patterns of aggressive tumor behavior are ignored. In this work, we aim to learn histopathological patterns within cancerous tissue regions that can be used to improve prognostic stratification for colorectal cancer. To do so, we propose a self-supervised learning method that jointly learns a representation of tissue regions as well as a metric of the clustering to obtain their underlying patterns. These histopathological patterns are then used to represent the interaction between complex tissues and predict clinical outcomes directly. We furthermore show that the proposed approach can benefit from linear predictors to avoid overfitting in patient outcomes predictions. To this end, we introduce a new well-characterized clinicopathological dataset, including a retrospective collective of 374 patients, with their survival time and treatment information. Histomorphological clusters obtained by our method are evaluated by training survival models. The experimental results demonstrate statistically significant patient stratification, and our approach outperformed the state-of-the-art deep clustering methods.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {480–489},
numpages = {10},
keywords = {Colorectal cancer, Survival analysis, Histology, Self-supervised learning},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59722-1_34,
author = {Cicalese, Pietro Antonio and Mobiny, Aryan and Yuan, Pengyu and Becker, Jan and Mohan, Chandra and Nguyen, Hien Van},
title = {StyPath: Style-Transfer Data Augmentation for Robust Histology Image Classification},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_34},
doi = {10.1007/978-3-030-59722-1_34},
abstract = {The classification of Antibody Mediated Rejection (AMR) in kidney transplant remains challenging even for experienced nephropathologists; this is partly because histological tissue stain analysis is often characterized by low inter-observer agreement and poor reproducibility. One of the implicated causes for inter-observer disagreement is the variability of tissue stain quality between (and within) pathology labs, coupled with the gradual fading of archival sections. Variations in stain colors and intensities can make tissue evaluation difficult for pathologists, ultimately affecting their ability to describe relevant morphological features. Being able to accurately predict the AMR status based on kidney histology images is crucial for improving patient treatment and care. We propose a novel pipeline to build robust deep neural networks for AMR classification based on StyPath, a histological data augmentation technique that leverages a light weight style-transfer algorithm as a means to reduce sample-specific bias. Each image was generated in 1.84±0.03&nbsp;s using a single GTX TITAN V gpu and pytorch, making it faster than other popular histological data augmentation techniques. We evaluated our model using a Monte Carlo (MC) estimate of Bayesian performance and generate an epistemic measure of uncertainty to compare both the baseline and StyPath augmented models. We also generated Grad-CAM representations of the results which were assessed by an experienced nephropathologist; we used this qualitative analysis to elucidate on the assumptions being made by each model. Our results imply that our style-transfer augmentation technique improves histological classification performance (reducing error from 14.8% to 11.5%) and generalization ability.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {351–361},
numpages = {11},
keywords = {Inter-observer agreement, Data augmentation, CNN classifier, Style-transfer, Pathology},
location = {Lima, Peru}
}

@inproceedings{10.1145/3411109.3411143,
author = {Mitchell, Thomas J. and Jones, Alex J. and O'Connor, Michael B. and Wonnacott, Mark D. and Glowacki, David R. and Hyde, Joseph},
title = {Towards molecular musical instruments: interactive sonifications of 17-alanine, graphene and carbon nanotubes},
year = {2020},
isbn = {9781450375634},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411109.3411143},
doi = {10.1145/3411109.3411143},
abstract = {Scientists increasingly rely on computational models of atoms and molecules to observe, understand and make predictions about the microscopic world. Atoms and molecules are in constant motion, with vibrations and structural fluctuations occurring at very short time-scales and corresponding length-scales. But can these microscopic oscillations be converted into sound? And, what would they sound like? In this paper we present our initial steps towards a generalised approach for sonifying data produced by a real-time molecular dynamics simulation. The approach uses scanned synthesis to translate real-time geometric simulation data into audio. The process is embedded within a stand alone application as well as a variety of audio plugin formats to enable the process to be used as an audio synthesis method for music making. We review the relevant background literature before providing an overview of our system. Simulations of three molecules are then considered: 17-alanine, graphene and a carbon nanotube. Four examples are then provided demonstrating how the technique maps molecular features and parameters onto the auditory character of the resulting sound. A case study is then provided in which the sonification/synthesis method is used within a musical composition.},
booktitle = {Proceedings of the 15th International Audio Mostly Conference},
pages = {214–221},
numpages = {8},
keywords = {virtual reality, spatial audio, sound art, sonification, sonic interaction design, musicology, game audio, augmented reality},
location = {Graz, Austria},
series = {AM '20}
}

@inproceedings{10.1007/978-3-030-59137-3_21,
author = {Sriram, Aditya and Kalra, Shivam and Babaie, Morteza and Kieffer, Brady and Drobi, W. Al and Rahnamayan, Shahryar and Kashani, Hany and Tizhoosh, Hamid R.},
title = {Forming Local Intersections of Projections for Classifying and Searching Histopathology Images},
year = {2020},
isbn = {978-3-030-59136-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59137-3_21},
doi = {10.1007/978-3-030-59137-3_21},
abstract = {In this paper, we propose a novel image descriptor called “Forming Local Intersections of Projections” (FLIP) and its multi-resolutional version (mFLIP) for representing histopathology images. The descriptor is based on the Radon transform wherein we apply parallel projections in small local neighborhoods of gray-level images. Using equidistant projection directions in each window, we extract unique and invariant characteristics of the neighborhood by taking the intersection of adjacent projections. Thereafter, we construct a histogram for each image, which we call the FLIP histogram. Various resolutions provide different FLIP histograms which are then concatenated to form the mFLIP descriptor. Our experiments included training common networks from scratch and fine-tuning pre-trained networks to benchmark our proposed descriptor. Experiments are conducted on the publicly available dataset KIMIA Path24 and KIMIA Path960. For both of these datasets, FLIP and mFLIP descriptors show promising results in all experiments. Using KIMIA Path24 data, FLIP outperformed non-fine-tuned Inception-v3 and fine-tuned VGG16 and mFLIP outperformed fine-tuned Inception-v3 in feature extracting.},
booktitle = {Artificial Intelligence in Medicine: 18th International Conference on Artificial Intelligence in Medicine, AIME 2020, Minneapolis, MN, USA, August 25–28, 2020, Proceedings},
pages = {227–237},
numpages = {11},
keywords = {Radon projections histopathology, Image search, Feature extraction, Image descriptors},
location = {<conf-loc>Minneapolis, MN, USA</conf-loc>}
}

@inproceedings{10.1007/978-3-030-66415-2_22,
author = {Wang, Pei and Chung, Albert C. S.},
title = {DoubleU-Net: Colorectal Cancer Diagnosis and Gland Instance Segmentation with Text-Guided Feature Control},
year = {2020},
isbn = {978-3-030-66414-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66415-2_22},
doi = {10.1007/978-3-030-66415-2_22},
abstract = {With the rapid therapeutic advancement in personalized medicine, the role of pathologists for colorectal cancer has greatly expanded from morphologists to clinical consultants. In addition to cancer diagnosis, pathologists are responsible for multiple assessments based on glandular morphology statistics, like selecting appropriate tissue sections for mutation analysis [6]. Therefore, we propose DoubleU-Net that determines the initial gland segmentation and diagnoses the histologic grades simultaneously, and then incorporates the diagnosis text data to produce more accurate final segmentation. Our DoubleU-Net shows three advantages: (1) Besides the initial segmentation, it offers histologic grade diagnosis and enhanced segmentation for full-scale assistance. (2) The textual features extracted from diagnosis data provide high-level guidance related to gland morphology, and boost the performance of challenging cases with seriously deformed glands. (3) It can be extended to segmentation tasks with text data like key clinical phrases or pathology descriptions. The model is evaluated on two public colon gland datasets and achieves state-of-the-art performance.},
booktitle = {Computer Vision – ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part I},
pages = {338–354},
numpages = {17},
keywords = {Morphological feature guidance, Multi-domain learning, Gland segmentation, Cancer diagnosis},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3377929.3389951,
author = {Delaney, Gary W. and Howard, Gerard},
title = {Multi-objective exploration of a granular matter design space},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3389951},
doi = {10.1145/3377929.3389951},
abstract = {Granular materials, such as sands, grains, soils and powders are central to many industrial processes from mining and food production to pharmaceuticals and construction. These materials display many interesting properties, including their ability to flow like a liquid at low densities and jam in to a solid state at high densities. However, controlling the microscopic properties of granular media to elicit bespoke functional granular systems remains challenging due to the complex relationship between the individual particle morphologies and the related emergent behaviour of the bulk state. Here, we investigate the use of evolution to explore the functional landscapes of granular systems. We employ a superellipsoid representation of the particle shape, and examine a range of bi-disperse systems of prolate particles. Results show the ability to determine optimal particle morphologies and trade-offs for density and two different structural order measures. This represents an important further step towards the creation of bespoke jammed systems with a range of practical applications across broad swathes of industry.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {263–264},
numpages = {2},
keywords = {robotics, packing, non-spherical particle shape, granular materials, evolutionary algorithms},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1109/I2MTC43012.2020.9129636,
author = {Keley, Meysam and Borghi, Fabr\'{\i}cio F. and Allil, Alexandre S. and Allil, Regina and Dutra, F\'{a}bio and Carvalho, Cesar and Werneck, Marcelo},
title = {Optical H&lt;inf&gt;2&lt;/inf&gt;S sensor base on Cu&lt;inf&gt;2-x&lt;/inf&gt;O-Functionalized FBG},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/I2MTC43012.2020.9129636},
doi = {10.1109/I2MTC43012.2020.9129636},
abstract = {In present work, a glass optical fiber having Bragg grating was functionalized through partial substitution of the cladding with a copper oxide thin film. After cladding removal using a hydrogen fluoride solution, Pulsed Laser Deposition (PLD) is explored to coat the FBG with a nanosized thin film of Cu&lt;inf&gt;2-x&lt;/inf&gt;O. As the spectral position of the Bragg wavelength (λB) depends on the effective refractive index n&lt;inf&gt;eff&lt;/inf&gt;, the modulation of the optical characteristics of the sensitive Cu&lt;inf&gt;2-x&lt;/inf&gt;O thin film leads to a shift in λB. The fabricated sensor was then subjected to sensing tests and revealed a dislocation during H&lt;inf&gt;2&lt;/inf&gt;S exposure. Unlike resistive–type Semiconductor Metal Oxide (SMOX) gas sensors, the present sensing probe is not electrically charged nor demands a heating system as it functions perfectly at room temperature (25°C). Atomic Force Microscopy (AFM) and Scanning Electron Microscopy (SEM) of PLD copper oxide thin film present a uniform underlayer with nanosized disc-shaped islands. The X-ray Diffraction (XRD) spectrum of the film resembles cuprite (Cu&lt;inf&gt;2&lt;/inf&gt;O). The FTIR spectra of the sensitive thin film produced before and after exposure to H&lt;inf&gt;2&lt;/inf&gt;S support the hypothesis that the hydrogen sulfide interacts with absorbed Oxygen species $O_{(ads)}^{ - alpha }$ on the surface of the film, which is the proposed sensing mechanism. The sensor demonstrated a remarkable selectivity to H&lt;inf&gt;2&lt;/inf&gt;S and did not produce any signal during exposure to methane (CH&lt;inf&gt;4&lt;/inf&gt;) as which is usually accompanied by H&lt;inf&gt;2&lt;/inf&gt;S in the ambient. During the fabrication of almost all SMOX-based gas sensors, an annealing step at temperatures higher than 400°C is fundamental. However, in the present fabrication method, no post-heating step is necessary},
booktitle = {2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)},
pages = {1–5},
numpages = {5},
location = {Dubrovnik, Croatia}
}

@inproceedings{10.1007/978-3-030-45385-5_32,
author = {Saweros, Emil and Song, Yeong-Tae},
title = {Behavioral Risk Factors Based Cancer Prediction Model Utilizing Public and Personal Health Records},
year = {2020},
isbn = {978-3-030-45384-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45385-5_32},
doi = {10.1007/978-3-030-45385-5_32},
abstract = {Cancer has become one of the major public health problems worldwide while the etiology of cancer remains largely unknown. Even with advanced cancer research, prediction of it has not been as practical as those of other chronic diseases such as diabetes. However, behavioral risk factors (BRF) may be used to monitor and predict cancer diseases. Such factors can be used to demonstrate and educate the general public on potential cancer pathways and possible outcomes. For that purpose, we have analyzed public health data and personal health records and discovered some histological features related to behavioral risk factors in the cancer diseases. Such characteristics are then used to assess each individual’s BRFs for classification and prognosis of the diseases. It plays a key role in many health-related realms, including increasing patient awareness on BRFs, developing related medical procedures, the way of handling patients’ records and the treatment of chronic diseases. In this paper, we presented predictive analysis based on some BRFs that might contribute to increase the chances of developing cancer disease.},
booktitle = {Bioinformatics and Biomedical Engineering: 8th International Work-Conference, IWBBIO 2020, Granada, Spain, May 6–8, 2020, Proceedings},
pages = {362–377},
numpages = {16},
keywords = {Cross-sectional analysis, Electronic health record (EHR), Personal health record (PHR), Health care, Behavioral risk factors, Data science},
location = {Granada, Spain}
}

@inproceedings{10.5555/3600270.3600504,
author = {Ullah, Ihsan and Carri\'{o}n-Ojeda, Dustin and Escalera, Sergio and Guyon, Isabelle and Huisman, Mike and Mohr, Felix and van Rijn, Jan N. and Sun, Haozhe and Vanschoren, Joaquin and Vu, Phan Anh},
title = {Meta-album: multi-domain meta-dataset for few-shot image classification},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce Meta-Album, an image classification meta-dataset designed to facilitate few-shot learning, transfer learning, meta-learning, among other tasks. It includes 40 open datasets, each having at least 20 classes with 40 examples per class, with verified licences. They stem from diverse domains, such as ecology (fauna and flora), manufacturing (textures, vehicles), human actions, and optical character recognition, featuring various image scales (microscopic, human scales, remote sensing). All datasets are preprocessed, annotated, and formatted uniformly, and come in 3 versions (Micro ⊂ Mini ⊂ Extended) to match users' computational resources. We showcase the utility of the first 30 datasets on few-shot learning problems. The other 10 will be released shortly after. Meta-Album is already more diverse and larger (in number of datasets) than similar efforts, and we are committed to keep enlarging it via a series of competitions. As competitions terminate, their test data are released, thus creating a rolling benchmark, available through OpenML.org. Our website https://meta-album.github.io/ contains the source code of challenge winning methods, baseline methods, data loaders, and instructions for contributing either new datasets or algorithms to our expandable meta-dataset.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {234},
numpages = {16},
location = {<conf-loc>, <city>New Orleans</city>, <state>LA</state>, <country>USA</country>, </conf-loc>},
series = {NIPS '22}
}

@inproceedings{10.1007/978-3-030-66415-2_27,
author = {Ke, Rihuan and Bugeau, Aur\'{e}lie and Papadakis, Nicolas and Schuetz, Peter and Sch\"{o}nlieb, Carola-Bibiane},
title = {Learning to Segment Microscopy Images with Lazy Labels},
year = {2020},
isbn = {978-3-030-66414-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66415-2_27},
doi = {10.1007/978-3-030-66415-2_27},
abstract = {The need for labour intensive pixel-wise annotation is a major limitation of many fully supervised learning methods for segmenting bioimages that can contain numerous object instances with thin separations. In this paper, we introduce a deep convolutional neural network for microscopy image segmentation. Annotation issues are circumvented by letting the network being trainable on coarse labels combined with only a very small number of images with pixel-wise annotations. We call this new labelling strategy ‘lazy’ labels. Image segmentation is stratified into three connected tasks: rough inner region detection, object separation and pixel-wise segmentation. These tasks are learned in an end-to-end multi-task learning framework. The method is demonstrated on two microscopy datasets, where we show that the model gives accurate segmentation results even if exact boundary labels are missing for a majority of annotated data. It brings more flexibility and efficiency for training deep neural networks that are data hungry and is applicable to biomedical images with poor contrast at the object boundaries or with diverse textures and repeated patterns.},
booktitle = {Computer Vision – ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part I},
pages = {411–428},
numpages = {18},
keywords = {Image segmentation, Convolutional neural networks, Multi-task learning, Microscopy images},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1007/978-3-031-16434-7_35,
author = {Boyd, Joseph and Villa, Ir\`{e}ne and Mathieu, Marie-Christine and Deutsch, Eric and Paragios, Nikos and Vakalopoulou, Maria and Christodoulidis, Stergios},
title = {Region-Guided CycleGANs for&nbsp;Stain Transfer in&nbsp;Whole Slide Images},
year = {2022},
isbn = {978-3-031-16433-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16434-7_35},
doi = {10.1007/978-3-031-16434-7_35},
abstract = {In whole slide imaging, commonly used staining techniques based on hematoxylin and eosin (H &amp;E) and immunohistochemistry (IHC) stains accentuate different aspects of the tissue landscape. In the case of detecting metastases, IHC provides a distinct readout that is readily interpretable by pathologists. IHC, however, is a more expensive approach and not available at all medical centers. Virtually generating IHC images from H &amp;E using deep neural networks thus becomes an attractive alternative. Deep generative models such as CycleGANs learn a semantically-consistent mapping between two image domains, while emulating the textural properties of each domain. They are therefore a suitable choice for stain transfer applications. However, they remain fully unsupervised, and possess no mechanism for enforcing biological consistency in stain transfer. In this paper, we propose an extension to CycleGANs in the form of a region of interest discriminator. This allows the CycleGAN to learn from unpaired datasets where, in addition, there is a partial annotation of objects for which one wishes to enforce consistency. We present a use case on whole slide images, where an IHC stain provides an experimentally generated signal for metastatic cells. We demonstrate the superiority of our approach over prior art in stain transfer on histopathology tiles over two datasets. Our code and model are available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part II},
pages = {356–365},
numpages = {10},
keywords = {Stain transfer, CycleGANs, Region-based discriminator},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1109/ITSC48978.2021.9564637,
author = {Troullinos, Dimitrios and Chalkiadakis, Georgios and Manolis, Diamantis and Papamichail, Ioannis and Papageorgiou, Markos},
title = {Lane- Free Microscopic Simulation for Connected and Automated Vehicles},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ITSC48978.2021.9564637},
doi = {10.1109/ITSC48978.2021.9564637},
abstract = {This paper presents the ongoing development of the microscopic TrafficFluid-Sim simulator, aimed primarily for Connected and Automated Vehicles (CAVs) under a novel lane-free traffic paradigm. In particular, TrafficFluid-Sim builds on the SUMO simulation infrastructure to model traffic environments featuring two novel vehicle characteristics: (i) Vehicles can be located at any arbitrary lateral position within the road boundaries; and (ii) Vehicles may exert, based on their automated driving and connectivity capabilities, “vehicle nudging” to other surrounding vehicles. As such, TrafficFluid-Sim enables simulation of novel CAV movement strategies for various types of road infrastructure and is an indispensable tool for the design, testing and evaluation of the characteristics of a future CAV traffic flow as an efficient artificial fluid, as envisaged by the ongoing TrafficFluid ERC project.},
booktitle = {2021 IEEE International Intelligent Transportation Systems Conference (ITSC)},
pages = {3292–3299},
numpages = {8},
location = {Indianapolis, IN, USA}
}

@inproceedings{10.1007/978-3-031-57793-2_32,
author = {Liu, Xiaohu and Blusseau, Samy and Velasco-Forero, Santiago},
title = {Counting Melanocytes with&nbsp;Trainable h-Maxima and&nbsp;Connected Component Layers},
year = {2024},
isbn = {978-3-031-57792-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-57793-2_32},
doi = {10.1007/978-3-031-57793-2_32},
abstract = {Bright objects on a dark background, such as cells in microscopy images, can sometimes be modeled as maxima of sufficient dynamic, called h-maxima. Such a model could be sufficient to count these objects in images, provided we know the dynamic threshold that tells apart actual objects from irrelevant maxima. In this paper we introduce a neural architecture that includes a morphological pipeline counting the number of h-maxima in an image, preceded by a classical CNN which predicts the dynamic h yielding the right number of objects. This is made possible by geodesic reconstruction layers, already introduced in previous work, and a new module counting connected components. This architecture is trained end-to-end to count melanocytes in microscopy images. Its performance is close to the state of the art CNN on this dataset, with much fewer parameters (1/100) and an increased interpretability.},
booktitle = {Discrete Geometry and Mathematical Morphology: Third International Joint Conference, DGMM 2024, Florence, Italy, April 15–18, 2024, Proceedings},
pages = {417–430},
numpages = {14},
location = {<conf-loc content-type="InPerson">Florence, Italy</conf-loc>}
}

@inproceedings{10.5555/3635637.3662920,
author = {Ghasemi, Farnoud and Kucharski, Rafal},
title = {Modelling the Rise and Fall of Two-sided Markets},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Two-sided markets disrupted our economies, reshaping markets as diverse as tourism (airbnb), mobility (Uber) and food deliveries (UberEats). New market leaders arose leveraging on platform-based business model, questioning well-established paradigms. The underlying processes behind their growth are non-trivial, inherently microscopic, and leverage on complex human interactions. Platforms need to reach critical mass of both supply and demand to trigger the so-called cross-sided network effects.To this end, platforms adopt a variety of strategies to first create the market, then expand it and finally successfully compete with others. Such a complex social system with many non-linear interactions and learning processes calls for a dedicated modelling approach. State-of-the-art methods well estimate the macroscopic equilibrium conditions, but struggle to reproduce the complex growth patterns and individual human behaviour behind. To bridge this gap, we propose the microscopic S-shaped learning model where agents build their perception on the new service with time, affected by both endogenous (service quality) and exogenous (marketing and word-of-mouth) factors cumulated from experiences. We illustrate it with the case of two-sided mobility platform (Uber), where the platform applies a series of marketing actions leading to rise and then fall on the market where 200 drivers serve 2000 travellers on the complex urban network of Amsterdam.Our model is the first to reproduce not only behaviourally sound, but also empirically observed growth trajectories, it remains sensitive to a variety of marketing strategies, allows reproducing the competition between platforms and is designed to be integrated with machine learning algorithms to identify the optimal market entry strategy.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {679–687},
numpages = {9},
keywords = {adaptive multiagent systems, agent-based simulation, complex social systems, learning processes, two-sided platforms},
location = {<conf-loc>, <city>Auckland</city>, <country>New Zealand</country>, </conf-loc>},
series = {AAMAS '24}
}

@inproceedings{10.1007/978-3-031-58171-7_3,
author = {Li, Shijie and Ren, Mengwei and Ach, Thomas and Gerig, Guido},
title = {Microscopy Image Segmentation via&nbsp;Point and&nbsp;Shape Regularized Data Synthesis},
year = {2024},
isbn = {978-3-031-58170-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-58171-7_3},
doi = {10.1007/978-3-031-58171-7_3},
abstract = {Current deep learning-based approaches for the segmentation of microscopy images heavily rely on large amount of training data with dense annotation, which is highly costly and laborious in practice. Compared to full annotation where the complete contour of objects is depicted, point annotations, specifically object centroids, are much easier to acquire and still provide crucial information about the objects for subsequent segmentation. In this paper, we assume access to point annotations only during training and develop a unified pipeline for microscopy image segmentation using synthetically generated training data. Our framework includes three stages: (1) it takes point annotations and samples a pseudo dense segmentation mask constrained with shape priors; (2) with an image generative model trained in an unpaired manner, it translates the mask to a realistic microscopy image regularized by object level consistency; (3) the pseudo masks along with the synthetic images then constitute a pairwise dataset for training an ad-hoc segmentation model. On the public MoNuSeg dataset, our synthesis pipeline produces more diverse and realistic images than baseline models while maintaining high coherence between input masks and generated images. When using the identical segmentation backbones, the models trained on our synthetic dataset significantly outperform those trained with pseudo-labels or baseline-generated images. Moreover, our framework achieves comparable results to models trained on authentic microscopy images with dense labels, demonstrating its potential as a reliable and highly efficient alternative to labor-intensive manual pixel-wise annotations in microscopy image segmentation. The code can be accessed through .},
booktitle = {Data Augmentation, Labelling, and Imperfections: Third MICCAI Workshop, DALI 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 12, 2023, Proceedings},
pages = {23–32},
numpages = {10},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1109/ASP-DAC58780.2024.10473793,
author = {Liu, Zhaoxiang and Chen, Kejun and Sullivan, Dean and Arias, Orlando and Dutta, Raj and Jin, Yier and Guo, Xiaolong},
title = {Microscope: Causality Inference Crossing the Hardware and Software Boundary from Hardware Perspective},
year = {2024},
isbn = {9798350393545},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASP-DAC58780.2024.10473793},
doi = {10.1109/ASP-DAC58780.2024.10473793},
abstract = {The increasing complexity of System-on-Chip (SoC) designs and the rise of third-party vendors in the semiconductor industry have led to unprecedented security concerns. Traditional formal methods struggle to address software-exploited hardware bugs, and existing solutions for hardware-software co-verification often fall short. This paper presents Microscope, a novel framework for inferring software instruction patterns that can trigger hardware vulnerabilities in SoC designs. Microscope enhances the Structural Causal Model (SCM) with hardware features, creating a scalable Hardware Structural Causal Model (HW-SCM). A domain-specific language (DSL) in SMT-LIB represents the HW-SCM and predefined security properties, with incremental SMT solving deducing possible instructions. Microscope identifies causality to determine whether a hardware threat could result from any software events, providing a valuable resource for patching hardware bugs and generating test input. Extensive experimentation demonstrates Microscope's capability to infer the causality of a wide range of vulnerabilities and bugs located in SoC-level benchmarks.},
booktitle = {Proceedings of the 29th Asia and South Pacific Design Automation Conference},
pages = {933–938},
numpages = {6},
keywords = {causality inference, hardware security, hardware and software co-verification},
location = {<conf-loc>, <city>Incheon</city>, <country>Republic of Korea</country>, </conf-loc>},
series = {ASPDAC '24}
}

@inproceedings{10.1007/978-3-031-55088-1_8,
author = {Godson, Lucy and Alemi, Navid and Nsengimana, J\'{e}r\'{e}mie and Cook, Graham P. and Clarke, Emily L. and Treanor, Darren and Bishop, D. Timothy and Newton-Bishop, Julia and Magee, Derek},
title = {Multi-level Graph Representations of&nbsp;Melanoma Whole Slide Images for&nbsp;Identifying Immune Subgroups},
year = {2024},
isbn = {978-3-031-55087-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-55088-1_8},
doi = {10.1007/978-3-031-55088-1_8},
abstract = {Stratifying melanoma patients into immune subgroups is important for understanding patient outcomes and treatment options. Current weakly supervised classification methods often involve dividing digitised whole slide images into patches, which leads to the loss of important contextual diagnostic information. Here, we propose using graph attention neural networks, which utilise graph representations of whole slide images, to introduce context to classifications. In addition, we present a novel hierarchical graph approach, which leverages histopathological features from multiple resolutions to improve on state-of-the-art (SOTA) multiple instance learning (MIL) methods. We achieve a mean test area under the curve metric of 0.80 for classifying low and high immune melanoma subtypes, using multi-level and 20x patch graph representations of whole slide images, compared to 0.77 when using SOTA MIL methods. Our experimental results comprehensively show how our whole slide image graph representation is a valuable improvement on the MIL paradigm and could help to determine early-stage prognostic markers and stratify melanoma patients for effective treatments. Code is available at .},
booktitle = {Graphs in Biomedical Image Analysis, and Overlapped Cell on Tissue Dataset for Histopathology: 5th MICCAI Workshop, GRAIL 2023 and 1st MICCAI Challenge, OCELOT 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, September 23, and October 4, 2023, Proceedings},
pages = {85–96},
numpages = {12},
keywords = {Computational pathology, graph neural networks, melanoma},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-55088-1_15,
author = {Ha, Suk Min and Ko, Young Sin and Park, Youngjin},
title = {Generating BlobCell Label from Weak Annotations for Precise Cell Segmentation},
year = {2024},
isbn = {978-3-031-55087-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-55088-1_15},
doi = {10.1007/978-3-031-55088-1_15},
abstract = {Cell segmentation in histopathological image analysis is critical for identifying cancer cells and predicting disease severity. However, manual cell labeling is time-consuming. Many experiments speed up the generation of cell data by annotating central cell points and classes, generating cell segmentation labels with a fixed radius. However, the accuracy of this method depends on the specified given radius, which is problematic due to the variety of cell sizes and shapes, including elongated ovals and linear shapes. The use of a fixed radius is considered in-accurate and unreasonable. To address this, we propose BlobCell labeling, which uses blob extraction for accurate cell labeling based on central coordinates, resulting in a +9.02% improvement in the dice score. Furthermore, to improve cell detection from cell segmentation results such as the proposed challenge baseline [1], we designed a new network architecture that utilizes BloCell information within the Injection model structure, we achieved a significant performance improvement of +12.11% in mF1 score on the test set.},
booktitle = {Graphs in Biomedical Image Analysis, and Overlapped Cell on Tissue Dataset for Histopathology: 5th MICCAI Workshop, GRAIL 2023 and 1st MICCAI Challenge, OCELOT 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, September 23, and October 4, 2023, Proceedings},
pages = {161–170},
numpages = {10},
keywords = {Cell Detection, Weak annotation, Cell-Tissue Interaction, Blob detection algorithm, Deep learning segmentation, Weak annotation, Pathology},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-55088-1_14,
author = {Li, Zhaoyang and Li, Wangkai and Mai, Huayu and Zhang, Tianzhu and Xiong, Zhiwei},
title = {Enhancing Cell Detection in&nbsp;Histopathology Images: A ViT-Based U-Net Approach},
year = {2024},
isbn = {978-3-031-55087-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-55088-1_14},
doi = {10.1007/978-3-031-55088-1_14},
abstract = {Cell detection in histology images is a pivotal and fundamental task within the field of computational pathology. Recent advancements have led to the introduction of the OCELOT dataset, which offers annotated images featuring overlapping cell and tissue structures derived from diverse organs. The significance of OCELOT dataset lies in its provision of valuable insights into the intricate relationship between the surrounding tissue structures and individual cells. Based on the OCELOT dataset, We propose a ViT-based U-Net (Cell-Tissue-ViT) in a unified deep model via an encoder-decoder structure for robust cell detection, simultaneously leveraging tissue and cell information. Specifically, we adopt the pretrained ViT encoder of the large-scale pre-trained Segment Anything Model(SAM) as our backbone to enhance the feature extraction capability of the model and adopt LoRA to fine-tune the backbone, intending to enhance its suitability for our specific task. Our approach achieves highly promising results in cell detection on the OCELOT dataset, with an F1-detection score of 0.7558, as indicated by the preliminary results on the validation set. What’s more, we achieved 1st place on the official test set. The code is available in .},
booktitle = {Graphs in Biomedical Image Analysis, and Overlapped Cell on Tissue Dataset for Histopathology: 5th MICCAI Workshop, GRAIL 2023 and 1st MICCAI Challenge, OCELOT 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, September 23, and October 4, 2023, Proceedings},
pages = {150–160},
numpages = {11},
keywords = {Cell Detection, Computational Pathology, Vision Transformer, Low-Rank Adaptation},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-54605-1_23,
author = {Bauer, Markus and Augenstein, Christoph},
title = {Self-supervised Learning in&nbsp;Histopathology: New Perspectives for&nbsp;Prostate Cancer Grading},
year = {2024},
isbn = {978-3-031-54604-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-54605-1_23},
doi = {10.1007/978-3-031-54605-1_23},
abstract = {The prostate carcinoma (PCa) is the second most common cause of cancer-deaths among men. To estimate the appropriate therapy pathway after diagnosis, the Gleason score (GS) has been established as an international measure. While the GS has been proven to be a good tool for tumour assessment, it naturally suffers from subjectivity. Especially for cancers of lower to medium severity, this leads to inter- and intra observer variability and a remarkable amount of over- and under therapy. The PCa thus is in the focus of various research works, that aim to improve the grading procedure. With recently emerging AI technologies, solutions have been proposed to automate the GS-based PCa-grading while keeping predictions consistent. Current solutions, however, fail to handle data variability arising from preparation differences among hospitals and typically require a large amount of annotated data, which is often not available. Thus, in this paper, we propose self-supervised learning (SSL) as a new perspective for AI-based PCa grading. Using several thousand PCa cases, we demonstrate that SSL may be a feasible alternative for analysing histopathological samples and pretraining grading models. Our SSL-pretrained models extract features related to the Gleason grades (GGs), and achieve competitive accuracy for PCa downstream classification.},
booktitle = {Pattern Recognition: 45th DAGM German Conference, DAGM GCPR 2023, Heidelberg, Germany, September 19–22, 2023, Proceedings},
pages = {348–360},
numpages = {13},
keywords = {Prostate Cancer, Self-Supervised Learning, Artificial Intelligence},
location = {<conf-loc content-type="InPerson">Heidelberg, Germany</conf-loc>}
}

@inproceedings{10.1145/3637732.3637778,
author = {Zeng, Lingxi and Zhang, Yinglin and Higashita, Risa and Liu, Jiang},
title = {Corneal Endothelial Cell Segmentation with Multiple Long-range Dependencies},
year = {2024},
isbn = {9798400708343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637732.3637778},
doi = {10.1145/3637732.3637778},
abstract = {Corneal endothelial cell segmentation is an important task in ophthalmology, but it is challenging due to variations in image characteristics across different datasets. Existing deep learning methods have limitations in capturing long-range dependencies that are critical for accurate segmentation. To address this issue, we propose a novel multiple long-range dependencies network (MLD-Net) that effectively incorporates different types of long-range dependency information to achieve robust segmentation across datasets. The network employs dilated convolutions and attention gates to capture spatial and layer-level dependencies, respectively. The entire network is densely connected, facilitating the sharing of long-range dependency information across multiple scales. We demonstrate the effectiveness of MLD-Net on four different corneal endothelium microscope image datasets: SREP, BiolmLab, Rodrep, and TM-EM3000. Our experimental results show that MLD-Net outperforms existing state-of-the-art methods, achieving robustness and high accuracy in corneal endothelial cell segmentation.},
booktitle = {Proceedings of the 2023 10th International Conference on Biomedical and Bioinformatics Engineering},
pages = {1},
numpages = {1},
keywords = {Corneal Endothelial Cell., Deep Learning, Long-range Dependency, Segmentation},
location = {<conf-loc>, <city>Kyoto</city>, <country>Japan</country>, </conf-loc>},
series = {ICBBE '23}
}

@inproceedings{10.1007/978-3-031-53767-7_12,
author = {Bahadir, Cagla Deniz and Liechty, Benjamin and Pisapia, David J. and Sabuncu, Mert R.},
title = {Characterizing the&nbsp;Features of&nbsp;Mitotic Figures Using a&nbsp;Conditional Diffusion Probabilistic Model},
year = {2024},
isbn = {978-3-031-53766-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-53767-7_12},
doi = {10.1007/978-3-031-53767-7_12},
abstract = {Mitotic figure detection in histology images is a hard-to-define, yet clinically significant task, where labels are generated with pathologist interpretations and where there is no “gold-standard” independent ground-truth. However, it is well-established that these interpretation based labels are often unreliable, in part, due to differences in expertise levels and human subjectivity. In this paper, our goal is to shed light on the inherent uncertainty of mitosis labels and characterize the mitotic figure classification task in a human interpretable manner. We train a probabilistic diffusion model to synthesize patches of cell nuclei for a given mitosis label condition. Using this model, we can then generate a sequence of synthetic images that correspond to the same nucleus transitioning into the mitotic state. This allows us to identify different image features associated with mitosis, such as cytoplasm granularity, nuclear density, nuclear irregularity and high contrast between the nucleus and the cell body. Our approach offers a new tool for pathologists to interpret and communicate the features driving the decision to recognize a mitotic figure.},
booktitle = {Deep Generative Models: Third MICCAI Workshop, DGM4MICCAI 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 8, 2023, Proceedings},
pages = {121–131},
numpages = {11},
keywords = {Mitotic Figure Detection, Conditional Diffusion Models},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-53969-5_4,
author = {Zaleshina, Margarita and Zaleshin, Alexander},
title = {Flocking Method for Identifying of Neural Circuits in Optogenetic Datasets},
year = {2024},
isbn = {978-3-031-53968-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-53969-5_4},
doi = {10.1007/978-3-031-53969-5_4},
abstract = {This work introduces a new approach to spatial analysis of brain activity in optogenetics datasets based on application of flocking method for an identification of stable neuronal activity locations. Our method uses a multiple local directivity and interaction in neuronal activity paths. It can be seen as a flocking behaviour that promotes sustainable structuration because they use collective information to move. We processed sets of mouse brain images obtained by light-sheet fluorescence microscopy method. Location variations of neural activity patterns were calculated on the basis of flocking algorithm. An important advantage of using this method is the identification of locations where a pronounced directionality of neuronal activity trajectories can be observed in a sequence of several adjacent slices, as well as the identification of areas of through intersection of activities. The trace activity of neural circuits can affect parameters of subsequent activation of neurons occurring in the same locations. We analyzed neuronal activity based on its distributions from slice to slice obtained with a time delay. We used GDAL Tools and LF Tools in QGIS for geometric and topological analysis of multi-page TIFF files with optogenetics datasets. As a result, we were able to identify localizations of sites with small movements of group neuronal activity passing in the same locations (with retaining localization) from slice to slice over time.},
booktitle = {Machine Learning, Optimization, and Data Science: 9th International Conference, LOD 2023, Grasmere, UK, September 22–26, 2023, Revised Selected Papers, Part I},
pages = {39–52},
numpages = {14},
keywords = {Brain Imaging, Pattern Recognition, Optogenetics, Mouse Brain},
location = {<conf-loc content-type="InPerson">Grasmere, United Kingdom</conf-loc>}
}

@inproceedings{10.1007/978-3-031-47425-5_12,
author = {Liu, Quan and Yao, Jiawen and Yao, Lisha and Chen, Xin and Zhou, Jingren and Lu, Le and Zhang, Ling and Liu, Zaiyi and Huo, Yuankai},
title = {M2Fusion: Bayesian-Based Multimodal Multi-level Fusion on&nbsp;Colorectal Cancer Microsatellite Instability Prediction},
year = {2024},
isbn = {978-3-031-47424-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-47425-5_12},
doi = {10.1007/978-3-031-47425-5_12},
abstract = {Colorectal cancer (CRC) micro-satellite instability (MSI) prediction on histopathology images is a challenging weakly supervised learning task that involves multi-instance learning on gigapixel images. To date, radiology images have proven to have CRC MSI information and efficient patient imaging techniques. Different data modalities integration offers the opportunity to increase the accuracy and robustness of MSI prediction. Despite the progress in representation learning from the whole slide images (WSI) and exploring the potential of making use of radiology data, CRC MSI prediction remains a challenge to fuse the information from multiple data modalities (e.g., pathology WSI and radiology CT image). In this paper, we propose M2Fusion: a Bayesian-based multimodal multi-level fusion pipeline for CRC MSI. The proposed fusion model M2Fusion is capable of discovering more novel patterns within and across modalities that are beneficial for predicting MSI than using a single modality alone, as well as other fusion methods. The contribution of the paper is three-fold: (1) M2Fusion is the first pipeline of multi-level fusion on pathology WSI and 3D radiology CT image for MSI prediction; (2) CT images are the first time integrated into multimodal fusion for CRC MSI prediction; (3) feature-level fusion strategy is evaluated on both Transformer-based and CNN-based method. Our approach is validated on cross-validation of 352 cases and outperforms either feature-level (0.8177 vs. 0.7908) or decision-level fusion strategy (0.8177 vs. 0.7289) on AUC score.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023 Workshops: MTSAIL 2023, LEAF 2023, AI4Treat 2023, MMMI 2023, REMIA 2023, Held in Conjunction with MICCAI 2023,  Vancouver, BC, Canada, October 8–12, 2023, Proceedings},
pages = {125–134},
numpages = {10},
keywords = {Colorectal cancer, Bayesian, Transformer, Pathology},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-52448-6_21,
author = {Agrawal, Shaleka and Ashby, Joseph and Bai, Jeiyun and Feng, Fan and Cai, Xue J. and Yanni, Joseph and Jones, Caroline B. and Logantha, Sunil J. R. J. and Vohra, Akbar and Hutcheon, Robert C. and Corno, Antonio F. and Dobrzynski, Halina and Stephenson, Robert S. and Boyett, Mark and Hart, George and Jarvis, Jonathan and Smaill, Bruce and Zhao, Jichao},
title = {Inherent Atrial Fibrillation Vulnerability in the Appendages Exacerbated in Heart Failure},
year = {2024},
isbn = {978-3-031-52447-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-52448-6_21},
doi = {10.1007/978-3-031-52448-6_21},
abstract = {Atrial fibrillation (AF) frequently accompanies heart failure (HF), however, the causal mechanism underlying their atrial electrophysiological substrates remains unclear. In the present study, we evaluated the effects of abnormal anatomical characteristics on the electrophysiology of rabbit atria with HF. Micro-CT images from adult New Zealand white rabbit hearts (n = 4 HF and n = 4 control) were acquired. Novel imaging methods were used to reconstruct atrial myofiber architecture at a high resolution of 21&nbsp;µm3/voxel for quantitative analysis of the structural remodelling. Effects of this structural remodelling on the vulnerability to atrial re-entrant waves was analysed using computer simulation. Reconstructed data showed increased chamber lumen and an uneven reduction in wall thickness across the appendages in HF. Anatomically, myofibers in epicardial walls of the appendages were identified to be circumferential, perpendicular to the pectinate muscles (PMs). The relative ratio of average PM thickness to the atrial wall was larger in HF vs. control (right atrial appendages: 3.5 versus 2.7 and left atrial appendages: 4.4 versus 3.7, p &lt; 0.001). Furthermore, the uncoupled myofiber orientation between the PMs and atrial wall was verified using confocal microscopy at a spatial resolution of 0.2 µm3. Computer simulations suggested (1) uncoupled myofiber orientation of the PMs and the atrial wall may increase the vulnerability to AF; and (2) decreased atrial thickness and dilated chambers may amplify the unstable substrates leading to re-entry formation in HF. Our ex-vivo to in-silico results demonstrate that uncoupled myofiber orientation in the atria is an important component of the structural remodelling, facilitating the development and maintenance of AF in HF.},
booktitle = {Statistical Atlases and Computational Models of the Heart. Regular and CMRxRecon Challenge Papers: 14th International Workshop, STACOM 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 12, 2023, Revised Selected Papers},
pages = {220–229},
numpages = {10},
keywords = {Atrial fibrillation, heart failure, atrial appendage, computer modelling, myofiber architecture},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1145/3634875.3634882,
author = {Qiao, Yu and Li, Guangxu and Li, Fangting and Zhang, Chen},
title = {NerveStitcher2.0: Evolution of Stitching Algorithm for Corneal Confocal Microscope Images with Optical Flow},
year = {2024},
isbn = {9798400716584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634875.3634882},
doi = {10.1145/3634875.3634882},
abstract = {The NerveStitcher demonstrated that a set of in vivo confocal microscopy (IVCM) images can be merged under a framework of graph convolutional neural network. However, the high similarity of the nerval structure on IVCM image results in mis-stitching when the internal images in sequence happen to drop. Particularly on large gap between a pair of adjacent images, NerveStitcher sometimes cannot detected. In this paper, we advance the concept of global optical flow of IVCM image pair and intergrate it to the existing framework. The large improvements in algorithm robustness are caused by that we make a trade of global displacement of the images sequence according to optical flow, and the local corresponding of key points. We firstly analyze the global shifts in pixel intensity and position between consecutive images, which allows us to determine the motion of each pixel. The displacement vector in the optical flow is then used to calculate the displacement distance of the image. After obtaining the displacement distance of each image using the optical flow, we then use NerveStitcher to stitch the same image sequence. By using a feature point matching algorithm, we can calculate the displacement distance of matching feature points in the two images. We then subtract the displacement distances obtained by the two methods and locate and modify the incorrect stitching results based on the difference value range. Experimental results showed that the improved algorithm, named NerveStitcher2.0, decreases the estimation error by more than 25%. The implemented code is available at https://github.com/better-77/nervestitcher2.0.},
booktitle = {Proceedings of the 2023 8th International Conference on Biomedical Imaging, Signal Processing},
pages = {46–54},
numpages = {9},
keywords = {Feature matching, Feature point displacement, Image stitching, Neural network, Optical flow information},
location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
series = {ICBSP '23}
}

@inproceedings{10.1145/3595916.3626392,
author = {Wang, Haixin and Yang, Jian and Katayama, Ryohei and Matsusaki, Michiya and Miyao, Tomoyuki and Zhou, Jinjia},
title = {NuclSeg: nuclei segmentation using semi-supervised stain deconvolution},
year = {2024},
isbn = {9798400702051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3595916.3626392},
doi = {10.1145/3595916.3626392},
abstract = {Recently, deep learning-inferred stain deconvolution/separation-based nuclei segmentation works demonstrated significant results by translating low-cost and prevalent immunohistochemical (IHC) slides to more expensive-yet-informative multiplex immunofluorescence (mpIF) images. However, when the input stain style is changed to Hematoxylin and Eosin stain (H&amp;E), which is one of the principal tissue stains used in histology, the stain deconvolution/separation based works can not achieve satisfactory performance because the features of the input stain image are greatly changed. To solve this problem, we integrate stain transfer (H&amp;E-&gt;IHC) before stain deconvolution (IHC-&gt;mpIF) to revise the image style. Moreover, a new semi-supervised learning strategy collaborating supervised and unsupervised learning processes are employed to diversify training data content for stain deconvolution.Firstly, in the supervised learning process, generative adversarial network (GAN) based image-to-image mapping (stain deconvolution) and inverse mapping models are trained on the dataset of co-registered IHC staining and mpIF staining of the same slides to respectively convert IHC to mpIF (I2m), and mpIF to IHC (m2I). After that, in the unsupervised learning process, high-quality IHC images from m2I model are selected according to the Discriminator score on another unpaired dataset, and then, IHC images are paired with input mpIF as the unsupervised training data for further improving the I2m model. This semi-supervised scheme balances two supervised and unsupervised errors while optimizing to limit the effect of imperfect pseudo inputs but still enhance stain deconvolution. Furthermore, image enhancement is applied after the stain deconvolution model to obtain high-quality segmentation masks. We thoroughly evaluate our method on publicly available benchmark datasets. The results show the proposed model obtains significant improvement compared to the SOTAs.},
booktitle = {Proceedings of the 5th ACM International Conference on Multimedia in Asia},
articleno = {22},
numpages = {6},
keywords = {deep learning, fine-tuning, nuclei segmentation, semi-supervised learning},
location = {<conf-loc>, <city>Tainan</city>, <country>Taiwan</country>, </conf-loc>},
series = {MMAsia '23}
}

@inproceedings{10.1145/3589132.3625586,
author = {Tang, Yinzhou and Wang, Huandong and Li, Yong},
title = {Enhancing Spatial Spread Prediction of Infectious Diseases through Integrating Multi-scale Human Mobility Dynamics},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589132.3625586},
doi = {10.1145/3589132.3625586},
abstract = {With the increasing prevalence of infectious diseases like COVID-19, there is a growing interest in modeling and predicting their transmission. Leveraging the wealth of mobile trajectory data collected through advanced localization and mobile communication techniques, numerous approaches have been proposed to predict the spatial spread of infectious diseases based on human mobility dynamics characterized by microscopic user contact graphs or macroscopic population flow graphs. However, existing pure macroscopic and microscopic models have limitations in terms of modeling capabilities or in protecting user privacy. Thus, in this study, we present a Multi-scale Spatial Disease prediction Network (MSDNet) for predicting the spatial spread of infectious diseases. The model predicts the spread of infectious diseases using a macromicro collaborative approach by combining the temporal and spatial characteristics of the macroscopic information in the population flow graph and the microscopic information in the user contact graph. To understand the coupling between human mobility and infectious disease transmission, we propose a loss term that combines infectious disease spread dynamics and modeling of infectious disease parameters that can achieve stable adaptation to key characteristics of infectious diseases even when human mobility is affected by policy measures such as travel restrictions. Extensive experimental results show the MSDNet model's superiority for epidemic prediction on graph networks using macro-micro collaboration, achieving a 15%-20% improvement in terms of RMSE and a 15%-30% improvement in terms of SMAPE compared to existing baseline models. In addition, we predict infectious disease parameters under changes in human mobility, and the results show that MSDNet could effectively distinguish between human mobility and infectious disease characteristics, achieving a relative improvement of 76% in terms of RMSE and 80% in terms of SMAPE in predicting infectious disease parameters under changes in human mobility.},
booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
articleno = {43},
numpages = {12},
keywords = {multi-scale, graph neural network, epidemic spreading, human mobility},
location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
series = {SIGSPATIAL '23}
}

@inproceedings{10.1007/978-3-031-48593-0_11,
author = {Wen, Ziqi and Wang, Qingzhong and Bian, Jiang and Li, Xuhong and Liu, Yi and Xiong, Haoyi},
title = {Context Matters: Cross-Domain Cell Detection in&nbsp;Histopathology Images via&nbsp;Contextual Regularization},
year = {2023},
isbn = {978-3-031-48592-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48593-0_11},
doi = {10.1007/978-3-031-48593-0_11},
abstract = {Deep learning-based cell detectors have shown promise in automating cell detection in histopathology images, which can aid in cancer diagnosis and prognosis. Nevertheless, the color variation in stain appearance across histopathology images obtained from diverse locations might degrade the accuracy of cell detection. The limitations of a basic cell detector on a dissimilar target dataset are attributed to its emphasis on domain-specific features while overlooking domain-invariant features. Thus, a cell detector that is trained on a particular source dataset may not perform optimally on a different target dataset. In this work, we propose a domain generalization method contextual regularization&nbsp;(CR) for cell detection in histopathology images, which is derived from the basic object detector and focuses on domain-invariant features to enhance detection performance across varying datasets. Specifically, we involves a reconstruction task that involves masking the high-level semantic features either stochastically or adaptively. Then, a transformer-based reconstruction head is designed to recover the original features based on partial observations. Additionally, CR can be seamlessly integrated with various deep learning-based cell detectors without further modifications, and it does not request additional cost in the inference time. The proposed method was validated on a publicly available dataset that comprises histopathology images acquired at different sites, and the results show that our method can effectively improve the generalization of cell detectors to unseen domains.},
booktitle = {Medical Image Understanding and Analysis: 27th Annual Conference, MIUA 2023, Aberdeen, UK, July 19–21, 2023, Proceedings},
pages = {147–156},
numpages = {10},
keywords = {Contextual regularization, Domain Generalization, Cell detection},
location = {<conf-loc content-type="InPerson">Aberdeen, United Kingdom</conf-loc>}
}

@inproceedings{10.1007/978-3-031-47401-9_28,
author = {Khamankar, Vaibhav and Bera, Sutanu and Bhattacharya, Saumik and Sen, Debashis and Biswas, Prabir Kumar},
title = {Histopathological Image Analysis with&nbsp;Style-Augmented Feature Domain Mixing for&nbsp;Improved Generalization},
year = {2023},
isbn = {978-3-031-47400-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-47401-9_28},
doi = {10.1007/978-3-031-47401-9_28},
abstract = {Histopathological images are essential for medical diagnosis and treatment planning, but interpreting them accurately using machine learning can be challenging due to variations in tissue preparation, staining and imaging protocols. Domain generalization aims to address such limitations by enabling the learning models to generalize to new datasets or populations. Style transfer-based data augmentation is an emerging technique that can be used to improve the generalizability of machine learning models for histopathological images. However, existing style transfer-based methods can be computationally expensive, and they rely on artistic styles, which may negatively impact model accuracy. In this study, we propose a feature domain style mixing technique that uses adaptive instance normalization to estimate style-mixed versions of image features. We compare our proposed method with existing style transfer-based data augmentation methods and found that it performs similarly or better, despite requiring lower computation. Our results demonstrate the potential of feature domain statistics mixing in the generalization of learning models for histopathological image analysis.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023 Workshops : ISIC 2023, Care-AI 2023, MedAGI 2023, DeCaF 2023,  Held in Conjunction with MICCAI 2023,  Vancouver, BC, Canada, October 8–12, 2023, Proceedings},
pages = {285–294},
numpages = {10},
keywords = {Histopathological Image, Feature Domain Augmentation, Style Mixing, Mitotic Figure, Domain Generalization, Domain Shift},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-48047-8_9,
author = {Sala, Emma and Mazzali, Marco and Paraggio, Emilio and Rossetto, Gianluca and Cassiolas, Giorgio and Scalona, Emilia and Negro, Francesco and De Palma, Giuseppe and Piazza, Cesare and Lopomo, Nicola Francesco},
title = {Proposal of a Multi-parametric Ergonomic Assessment Protocol Integrating Intra-operative Use of Wearable Technology to Evaluate Musculoskeletal Discomfort for Surgeon During Laryngeal Surgery},
year = {2023},
isbn = {978-3-031-48046-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48047-8_9},
doi = {10.1007/978-3-031-48047-8_9},
abstract = {Musculoskeletal disorders (MSDs) represent a cross-cutting problem among healthcare workers; particular attention should be given to surgeons who are involved in mentally and physically demanding tasks. This work aimed to propose a multi-parametric ergonomic approach able to exploit different wearable devices to estimate cervical discomfort and the muscular fatigue sustained by an otolaryngology (ENT) surgeon during the execution of laryngeal surgeries. The proposed protocol includes the use of both inertial measurement units (IMUs) and surface electromyography (EMG) probes to monitor head movement and muscle activation during the surgical procedures. IMUs were placed on the forehead and at the C7 level, while EMG probes were positioned on relevant bilateral upper body muscles involved in the surgical tasks. Data analysis encompassed the extraction and examination of flexion/extension, bending, and axial rotation joint angles and EMG signals were scrutinized to assess muscle activation and fatigue. The proposed protocol was preliminary validated involving one expert surgeon, who realized 28 surgeries, employing either a conventional microscope or an advanced exoscope; the setup was well-tolerated, with only minor discomfort reported. The protocol effectively captured detailed information regarding head movement and muscle activation patterns throughout the surgeries, revealing notable features in surgical approaches. The ergonomic assessment protocol provides a solid foundation for future investigations and the development of tailored surgical training programs aimed at mitigating the risk of MSDs among surgeons.},
booktitle = {HCI International 2023 – Late Breaking Papers: 25th International Conference on Human-Computer Interaction, HCII 2023, Copenhagen, Denmark, July 23–28, 2023, Proceedings, Part IV},
pages = {145–154},
numpages = {10},
keywords = {electromyography, inertial measurement units, wearable technologies, ENT, otolaryngology surgery, risk assessment, Ergonomics assessment},
location = {<conf-loc content-type="Hybrid">Copenhagen, Denmark</conf-loc>}
}

@inproceedings{10.1145/3581783.3613812,
author = {Jin, Zhiyu and Yu, Hanyang and Haul, Chen and Wang, Linxiang and Zhu, Zuobin and Shen, Qiu and Cao, Xun},
title = {WormTrack: Dataset and Benchmark for Multi-Object Tracking in Worm Crowds},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613812},
doi = {10.1145/3581783.3613812},
abstract = {Currently, multimedia systems and computer vision algorithms are increasingly playing a crucial role in biological research. However, due to the significant difference between macro and micro scenarios, it is impractical to directly transfer existing computer vision methods to the images captured by microscopes. Taking social behavior analysis of worm for example, it heavily depends on accurate and efficient Multi-object tracking (MOT) methods. Meanwhile, it faces great challenges due to the unique physical characteristics of worm, such as small size, highly uniform appearance, rapid deformation and overlapping movement. This paper studies on the challenges and existing solutions for MOT in worm crowds by building a well-designed dataset ("WormTrack") and a tracking-by-detection benchmark. We observed that the state-of-the-art MOT methods suffers from considerable performance drop on the new dataset. Therefore, we propose a customized MOT method for worm crowds by deeply understanding the physical characteristics of worms and scenes. The method is composed by an instance segmentation based detector, a multiple model fused Kalman filter based tracker and a multi-constraint based trajectory repairer. The experimental results demonstrate that our method can accurately track over 100 worms with almost identical appearance for a long period, which is exceptional compared to existing methods. We hope our work will attract further researches to explore more in this new field, and promote the crossing field researches with biology and medicine. Our code and data is available at https://github.com/Jeerrzy/wormstudio.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {5756–5763},
numpages = {8},
keywords = {worm crowds, physical characteristics, multi-object tracking, biomedical objects},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@inproceedings{10.1007/978-3-031-45676-3_45,
author = {Dubey, Shikha and Kataria, Tushar and Knudsen, Beatrice and Elhabian, Shireen Y.},
title = {Structural Cycle GAN for&nbsp;Virtual Immunohistochemistry Staining of&nbsp;Gland Markers in&nbsp;the&nbsp;Colon},
year = {2023},
isbn = {978-3-031-45675-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-45676-3_45},
doi = {10.1007/978-3-031-45676-3_45},
abstract = {With the advent of digital scanners and deep learning, diagnostic operations may move from a microscope to a desktop. Hematoxylin and Eosin (H &amp;E) staining is one of the most frequently used stains for disease analysis, diagnosis, and grading, but pathologists do need different immunohistochemical (IHC) stains to analyze specific structures or cells. Obtaining all of these stains (H &amp;E and different IHCs) on a single specimen is a tedious and time-consuming task. Consequently, virtual staining has emerged as an essential research direction. Here, we propose a novel generative model, Structural Cycle-GAN (SC-GAN), for synthesizing IHC stains from H &amp;E images, and vice versa. Our method expressly incorporates structural information in the form of edges (in addition to color data) and employs attention modules exclusively in the decoder of the proposed generator model. This integration enhances feature localization and preserves contextual information during the generation process. In addition, a structural loss is incorporated to ensure accurate structure alignment between the generated and input markers. To demonstrate the efficacy of the proposed model, experiments are conducted with two IHC markers emphasizing distinct structures of glands in the colon: the nucleus of epithelial cells (CDX2) and the cytoplasm (CK818). Quantitative metrics such as FID and SSIM are frequently used for the analysis of generative models, but they do not correlate explicitly with higher-quality virtual staining results. Therefore, we propose two new quantitative metrics that correlate directly with the virtual staining specificity of IHC markers.},
booktitle = {Machine Learning in Medical Imaging: 14th International Workshop, MLMI 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 8, 2023, Proceedings, Part II},
pages = {447–456},
numpages = {10},
keywords = {Structural Cycle GAN, Histopathology Images, Generative model, Virtual Immunohistochemistry Staining},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-45857-6_7,
author = {Cechnicka, Sarah and Ball, James and Reynaud, Hadrien and Arthurs, Callum and Roufosse, Candice and Kainz, Bernhard},
title = {Realistic Data Enrichment for&nbsp;Robust Image Segmentation in&nbsp;Histopathology},
year = {2023},
isbn = {978-3-031-45856-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-45857-6_7},
doi = {10.1007/978-3-031-45857-6_7},
abstract = {Poor performance of quantitative analysis in histopathological Whole Slide Images (WSI) has been a significant obstacle in clinical practice. Annotating large-scale WSIs manually is a demanding and time-consuming task, unlikely to yield the expected results when used for fully supervised learning systems. Rarely observed disease patterns and large differences in object scales are difficult to model through conventional patient intake. Prior methods either fall back to direct disease classification, which only requires learning a few factors per image, or report on average image segmentation performance, which is highly biased towards majority observations. Geometric image augmentation is commonly used to improve robustness for average case predictions and to enrich limited datasets. So far no method provided sampling of a realistic posterior distribution to improve stability, e.g. for the segmentation of imbalanced objects within images. Therefore, we propose a new approach, based on diffusion models, which can enrich an imbalanced dataset with plausible examples from underrepresented groups by conditioning on segmentation maps. Our method can simply expand limited clinical datasets making them suitable to train machine learning pipelines, and provides an interpretable and human-controllable way of generating histopathology images that are indistinguishable from real ones to human experts. We validate our findings on two datasets, one from the public domain and one from a Kidney Transplant study. 1(The source code and trained models will be publicly available at the time of the conference, on huggingface and github. )},
booktitle = {Domain Adaptation and Representation Transfer: 5th MICCAI Workshop, DART 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 12, 2023, Proceedings},
pages = {63–72},
numpages = {10},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1145/3616480.3616486,
author = {Wan, Jianbiao and Yar, Kar Peo and Du, Chunling and Low, Malcolm Yoke Hean},
title = {Sensor Data Analytics for Tool Condition Anomaly Detection with Machine Learning Techniques},
year = {2023},
isbn = {9798400708855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616480.3616486},
doi = {10.1145/3616480.3616486},
abstract = {Tool condition anomaly detection is a critical aspect of machining processes, ensuring product quality, cost-effectiveness, and operational safety. This paper presents a study of tool condition anomaly detection in computer numerical control (CNC) machining using multiple sensor signals and machine learning models. The study employed four sensors, namely spindle vibration, tool vibration, tool force, and acoustic emission, to collect data for ten different signals. Eight process parameters, involving different cutting speeds, feed rates, and depths of cut, were tested one at a time until the tool wear size reached 0.15mm. The tool wear size was periodically measured using a digital microscope. The sensor data was collected during each machine run at a sampling rate of 25600Hz. The experiments were conducted in sets of eight, with each set repeated four times. Tool wear exceeding 0.1mm was considered a worn condition, while anything below was considered healthy. The data was processed by computing the Welch Power Spectral Density (PSD) for each signal and summing the PSD within pre-defined frequency ranges. Feature selection was performed using Recursive Feature Elimination (RFE) combined with Random Forest Classifier (RFC). The selected features were then used to train and test Support Vector Machine (SVM), k-Nearest Neighbors (kNN), Multilayer Perceptron (MLP), Naive Bayes (NB), and Decision Tree (DT) models. The prediction performance was evaluated using classification reports and confusion matrices. The study explored the impact of changing the bin size of the sum PSD and repeating the feature selection and model training and testing. The results demonstrated that with force x-axis, a bin size of 50Hz for summing PSD yielded the best performance, with the MLP model achieving high precision, recall, and F1-score values.},
booktitle = {Proceedings of the 2023 5th International Electronics Communication Conference},
pages = {38–45},
numpages = {8},
keywords = {machining, Sensor-based systems, Quality prediction, Data acquisition and analysis},
location = {<conf-loc>, <city>Osaka City</city>, <country>Japan</country>, </conf-loc>},
series = {IECC '23}
}

@inproceedings{10.1007/978-3-031-44992-5_8,
author = {Khanal, Bidur and Bhattarai, Binod and Khanal, Bishesh and Linte, Cristian A.},
title = {Improving Medical Image Classification in&nbsp;Noisy Labels Using only&nbsp;Self-supervised Pretraining},
year = {2023},
isbn = {978-3-031-44991-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44992-5_8},
doi = {10.1007/978-3-031-44992-5_8},
abstract = {Noisy labels hurt deep learning-based supervised image classification performance as the models may overfit the noise and learn corrupted feature extractors. For natural image classification training with noisy labeled data, model initialization with contrastive self-supervised pretrained weights has shown to reduce feature corruption and improve classification performance. However, no works have explored: i) how other self-supervised approaches, such as pretext task-based pretraining, impact the learning with noisy label, and ii) any self-supervised pretraining methods alone for medical images in noisy label settings. Medical images often feature smaller datasets and subtle inter-class variations, requiring human expertise to ensure correct classification. Thus, it is not clear if the methods improving learning with noisy labels in natural image datasets such as CIFAR would also help with medical images.In this work, we explore contrastive and pretext task-based self-supervised pretraining to initialize the weights of a deep learning classification model for two medical datasets with self-induced noisy labels—NCT-CRC-HE-100K tissue histological images and COVID-QU-Ex chest X-ray images. Our results show that models initialized with pretrained weights obtained from self-supervised learning can effectively learn better features and improve robustness against noisy labels.},
booktitle = {Data Engineering in Medical Imaging: First MICCAI Workshop, DEMI 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 8, 2023, Proceedings},
pages = {78–90},
numpages = {13},
keywords = {feature extraction, warm-up obstacle, self-supervised pretraining, learning with noisy labels, label noise, medical image classification},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.1007/978-3-031-44917-8_11,
author = {Xu, Tony and Rozak, Matthew and Ntiri, Edward and Dorr, Adrienne and Mester, James and Stefanovic, Bojana and Martel, Anne and Goubran, Maged},
title = {Masked Image Modeling for&nbsp;Label-Efficient Segmentation in&nbsp;Two-Photon Excitation Microscopy},
year = {2023},
isbn = {978-3-031-47196-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44917-8_11},
doi = {10.1007/978-3-031-44917-8_11},
abstract = {Generating accurate structural segmentation for 3D two-photon excitation microscopy (TPEM) images affords insights into how cellular-scale networks in living animal models respond to disease. Manual creation of dense segmentation masks is, however, very time-consuming image modeling (MIM) has recently emerged as a highly effective self-supervised learning (SSL) formulation for feature extraction in natural images, reducing reliance on human-created labels. Here, we extend MIM to 3D TPEM datasets and show that a model pretrained using MIM obtains improved downstream segmentation performance relative to random initialization. We assessed our novel pipeline using multi-channel TPEM data on two common segmentation tasks, neuronal and vascular segmentation. We also introduce intensity-based and channel-separated masking strategies that respectively aim to exploit the intra-channel correlation of intensity and foreground structures, and inter-channel correlations that are specific to microscopy images. We show that these methods are effective for generating representations of TPEM images, and identify novel insights on how MIM can be modified to yield more salient image representations for microscopy. Our method reaches statistically similar performances to a fully-supervised model (using the entire dataset) when only requiring just 25% of the labeled data for both neuronal and vascular segmentation tasks. To the best of our knowledge, this is the first investigation applying MIM methods to microscopy, and we hope our presented SSL pipeline may both reduce the necessary labeling effort and improve downstream analysis of TPEM images for neuroscience investigations. To this end, we plan to make the SSL pipeline, pretrained models and training code available under the following GitHub organization: .},
booktitle = {Medical Image Learning with Limited and Noisy Data: Second International Workshop, MILLanD 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 8, 2023, Proceedings},
pages = {117–127},
numpages = {11},
keywords = {3D image segmentation, Two photon excitation microscopy, Self-supervised learning},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.1007/978-3-031-43993-3_60,
author = {Zhang, Jingwei and Kapse, Saarthak and Ma, Ke and Prasanna, Prateek and Saltz, Joel and Vakalopoulou, Maria and Samaras, Dimitris},
title = {Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning},
year = {2023},
isbn = {978-3-031-43992-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43993-3_60},
doi = {10.1007/978-3-031-43993-3_60},
abstract = {Whole slide image (WSI) classification is a critical task in computational pathology, requiring the processing of gigapixel-sized images, which is challenging for current deep-learning methods. Current state of the art methods are based on multi-instance learning schemes (MIL), which usually rely on pretrained features to represent the instances. Due to the lack of task-specific annotated data, these features are either obtained from well-established backbones on natural images, or, more recently from self-supervised models pretrained on histopathology. However, both approaches yield task-agnostic features, resulting in performance loss compared to the appropriate task-related supervision, if available. In this paper, we show that when task-specific annotations are limited, we can inject such supervision into downstream task training, to reduce the gap between fully task-tuned and task agnostic features. We propose Prompt-MIL, an MIL framework that integrates prompts into WSI classification. Prompt-MIL adopts a prompt tuning mechanism, where only a small fraction of parameters calibrates the pretrained features to encode task-specific information, rather than the conventional full fine-tuning approaches. Extensive experiments on three WSI datasets, TCGA-BRCA, TCGA-CRC, and BRIGHT, demonstrate the superiority of Prompt-MIL over conventional MIL methods, achieving a relative improvement of 1.49%–4.03% in accuracy and 0.25%–8.97% in AUROC while using fewer than 0.3% additional parameters. Compared to conventional full fine-tuning approaches, we fine-tune less than 1.3% of the parameters, yet achieve a relative improvement of 1.29%–13.61% in accuracy and 3.22%–27.18% in AUROC and reduce GPU memory consumption by 38%–45% while training 21%–27% faster.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part VIII},
pages = {624–634},
numpages = {11},
keywords = {Whole slide image classification, Multiple instance learning, Prompt tuning},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43990-2_52,
author = {Wang, Xiaofei and Price, Stephen and Li, Chao},
title = {Multi-task Learning of&nbsp;Histology and&nbsp;Molecular Markers for&nbsp;Classifying Diffuse Glioma},
year = {2023},
isbn = {978-3-031-43989-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43990-2_52},
doi = {10.1007/978-3-031-43990-2_52},
abstract = {Most recently, the pathology diagnosis of cancer is shifting to integrating molecular makers with histology features. It is a urgent need for digital pathology methods to effectively integrate molecular markers with histology, which could lead to more accurate diagnosis in the real world scenarios. This paper presents a first attempt to jointly predict molecular markers and histology features and model their interactions for classifying diffuse glioma bases on whole slide images. Specifically, we propose a hierarchical multi-task multi-instance learning framework to jointly predict histology and molecular markers. Moreover, we propose a co-occurrence probability-based label correction graph network to model the co-occurrence of molecular markers. Lastly, we design an inter-omic interaction strategy with the dynamical confidence constraint loss to model the interactions of histology and molecular markers. Our experiments show that our method outperforms other state-of-the-art methods in classifying diffuse glioma, as well as related histology and molecular markers on a multi-institutional dataset.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part VII},
pages = {551–561},
numpages = {11},
keywords = {Diffuse Glioma, Digital Pathology, Multi-task learning, Muti-label Classification},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43987-2_77,
author = {Arabyarmohammadi, Sara and Corredor, German and Zhou, Yufei and L\'{o}pez de Rodas, Miguel and Schalper, Kurt and Madabhushi, Anant},
title = {Triangular Analysis of&nbsp;Geographical Interplay of&nbsp;Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in&nbsp;Lung Cancer},
year = {2023},
isbn = {978-3-031-43986-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43987-2_77},
doi = {10.1007/978-3-031-43987-2_77},
abstract = {Quantitative immunofluorescence (QIF) enables identifying immune cell subtypes across histopathology images. There is substantial evidence to show that spatial architecture of immune cell populations (e.g. CD4+, CD8+, CD20+) is associated with therapy response in cancers, yet there is a paucity of approaches to quantify spatial statistics of interplay across immune subtypes. Previously, analyzing spatial cell interplay have been limited to either building subgraphs on individual cell types before feature extraction or capturing the interaction between two cell types. However, looking at the spatial interplay between more than two cell types reveals complex interactions and co-dependencies that might have implications in predicting response to therapies like immunotherapy. In this work we present, Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL), a novel approach involving building of heterogeneous subgraphs to precisely capture the spatial interplay between multiple cell families. Primarily, TriAnGIL focuses on triadic closures, and uses metrics to quantify triads instead of two-by-two relations and therefore considers both inter- and intra-family relationships between cells. The TriaAnGIL’s efficacy for microenvironment characterization from QIF images is demonstrated in problems of predicting (1) response to immunotherapy (N&nbsp;=&nbsp;122) and (2) overall survival (N&nbsp;=&nbsp;135) in patients with lung cancer in comparison with four hand-crafted approaches namely DenTIL, GG, CCG, SpaTIL, and deep learning with GNN. For both tasks, TriaAnGIL outperformed hand-crafted approaches, and GNN with AUC&nbsp;=&nbsp;.70, C-index&nbsp;=&nbsp;.64. In terms of interpretability, TriAnGIL easily beats GNN, by pulling biological insights from immune cells interplay and shedding light on the triadic interaction of CD4+-Tumor-stromal cells.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part VI},
pages = {797–807},
numpages = {11},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43987-2_59,
author = {Wu, Yawen and Zuo, Yingli and Zhu, Qi and Sheng, Jianpeng and Zhang, Daoqiang and Shao, Wei},
title = {Transfer Learning-Assisted Survival Analysis of&nbsp;Breast Cancer Relying on&nbsp;the&nbsp;Spatial Interaction Between Tumor-Infiltrating Lymphocytes and&nbsp;Tumors},
year = {2023},
isbn = {978-3-031-43986-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43987-2_59},
doi = {10.1007/978-3-031-43987-2_59},
abstract = {Whole-Slide Histopathology Image (WSI) is regarded as the gold standard for survival prediction of Breast Cancer (BC) across different subtypes. However, in cancer prognosis applications, the cost of acquiring patients’ survival information is high and can be extremely difficult in practice. By considering that there exists a certain common mechanism for tumor progression among different subtypes of Breast Invasive Carcinoma(BRCA), it becomes critical to utilize data from a related subtype of BRCA to help predict the patients’ survival in the target domain. To address this issue, we proposed a TILs-Tumor interactions guided unsupervised domain adaptation (T2UDA) algorithm to predict the patients’ survival on the target BC subtype. Different from the existing feature-level or instance-level transfer learning strategy, our study considered the fact that the tumor-infiltrating lymphocytes (TILs) and its correlation with tumors reveal similar role in the prognosis of different BRCA subtypes. More specifically, T2UDA first employed the Graph Attention Network (GAT) to learn the node embeddings and the spatial interactions between tumor and TILs patches in WSI. Then, besides aligning the embeddings of different types of nodes across the source and target domains, we proposed a novel Tumor-TILs interaction alignment (TTIA) module to ensure that the distribution of interaction weights are similar in both domains. We evaluated the performance of our method on the BRCA cohort derived from the Cancer Genome Atlas (TCGA), and the experimental results indicated that T2UDA outperformed other domain adaption methods for predicting patients’ clinical outcomes.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part VI},
pages = {612–621},
numpages = {10},
keywords = {Tumor-infiltrating Lymphocytes, Unsupervised Domain Adaption, Prognosis Prediction, Graph Attention Network, Breast Cancer},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43904-9_35,
author = {Luo, Yin and Liu, Wei and Fang, Tao and Song, Qilong and Min, Xuhong and Wang, Minghui and Li, Ao},
title = {CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification},
year = {2023},
isbn = {978-3-031-43903-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43904-9_35},
doi = {10.1007/978-3-031-43904-9_35},
abstract = {Accurately classifying the histological subtype of non-small cell lung cancer (NSCLC) using computed tomography (CT) images is critical for clinicians in determining the best treatment options for patients. Although recent advances in multi-view approaches have shown promising results, discrepancies between CT images from different views introduce various representations in the feature space, hindering the effective integration of multiple views and thus impeding classification performance. To solve this problem, we propose a novel method called cross-aligned representation learning (CARL) to learn both view-invariant and view-specific representations for more accurate NSCLC histological subtype classification. Specifically, we introduce a cross-view representation alignment learning network which learns effective view-invariant representations in a common subspace to reduce multi-view discrepancies in a discriminability-enforcing way. Additionally, CARL learns view-specific representations as a complement to provide a holistic and disentangled perspective of the multi-view CT images. Experimental results demonstrate that CARL can effectively reduce the multi-view discrepancies and outperform other state-of-the-art NSCLC histological subtype classification methods.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part V},
pages = {358–367},
numpages = {10},
keywords = {Cross-view Alignment, Representation Learning, Multi-view, Histologic Subtype Classification, Non-small Cell Lung Cancer},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43898-1_13,
author = {Kumar, Amandeep and Bhunia, Ankan Kumar and Narayan, Sanath and Cholakkal, Hisham and Anwer, Rao Muhammad and Laaksonen, Jorma and Khan, Fahad Shahbaz},
title = {Cross-Modulated Few-Shot Image Generation for&nbsp;Colorectal Tissue Classification},
year = {2023},
isbn = {978-3-031-43897-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43898-1_13},
doi = {10.1007/978-3-031-43898-1_13},
abstract = {In this work, we propose a few-shot colorectal tissue image generation method for addressing the scarcity of histopathological training data for rare cancer tissues. Our few-shot generation method, named XM-GAN, takes one base and a pair of reference tissue images as input and generates high-quality yet diverse images. Within our XM-GAN, a novel controllable fusion block densely aggregates local regions of reference images based on their similarity to those in the base image, resulting in locally consistent features. To the best of our knowledge, we are the first to investigate few-shot generation in colorectal tissue images. We evaluate our few-shot colorectral tissue image generation by performing extensive qualitative, quantitative and subject specialist (pathologist) based evaluations. Specifically, in specialist-based evaluation, pathologists could differentiate between our XM-GAN generated tissue images and real images only  time. Moreover, we utilize these generated images as data augmentation to address the few-shot tissue image classification task, achieving a gain of 4.4% in terms of mean accuracy over the vanilla few-shot classifier. Code: .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part III},
pages = {128–137},
numpages = {10},
keywords = {Few-shot Image generation, Cross Modulation},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43895-0_9,
author = {Qiu, Jingna and Wilm, Frauke and \"{O}ttl, Mathias and Schlereth, Maja and Liu, Chang and Heimann, Tobias and Aubreville, Marc and Breininger, Katharina},
title = {Adaptive Region Selection for&nbsp;Active Learning in&nbsp;Whole Slide Image Semantic Segmentation},
year = {2023},
isbn = {978-3-031-43894-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43895-0_9},
doi = {10.1007/978-3-031-43895-0_9},
abstract = {The process of annotating histological gigapixel-sized whole slide images (WSIs) at the pixel level for the purpose of training a supervised segmentation model is time-consuming. Region-based active learning (AL) involves training the model on a limited number of annotated image regions instead of requesting annotations of the entire images. These annotation regions are iteratively selected, with the goal of optimizing model performance while minimizing the annotated area. The standard method for region selection evaluates the informativeness of all square regions of a specified size and then selects a specific quantity of the most informative regions. We find that the efficiency of this method highly depends on the choice of AL step size (i.e., the combination of region size and the number of selected regions per WSI), and a suboptimal AL step size can result in redundant annotation requests or inflated computation costs. This paper introduces a novel technique for selecting annotation regions adaptively, mitigating the reliance on this AL hyperparameter. Specifically, we dynamically determine each region by first identifying an informative area and then detecting its optimal bounding box, as opposed to selecting regions of a uniform predefined shape and size as in the standard method. We evaluate our method using the task of breast cancer metastases segmentation on the public CAMELYON16 dataset and show that it consistently achieves higher sampling efficiency than the standard method across various AL step sizes. With only 2.6% of tissue area annotated, we achieve full annotation performance and thereby substantially reduce the costs of annotating a WSI dataset. The source code is available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part II},
pages = {90–100},
numpages = {11},
keywords = {Active learning, Region selection, Whole slide images},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-43895-0_71,
author = {Ye, Jiarong and Ni, Haomiao and Jin, Peng and Huang, Sharon X. and Xue, Yuan},
title = {Synthetic Augmentation with&nbsp;Large-Scale Unconditional Pre-training},
year = {2023},
isbn = {978-3-031-43894-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43895-0_71},
doi = {10.1007/978-3-031-43895-0_71},
abstract = {Deep learning based medical image recognition systems often require a substantial amount of training data with expert annotations, which can be expensive and time-consuming to obtain. Recently, synthetic augmentation techniques have been proposed to mitigate the issue by generating realistic images conditioned on class labels. However, the effectiveness of these methods heavily depends on the representation capability of the trained generative model, which cannot be guaranteed without sufficient labeled training data. To further reduce the dependency on annotated data, we propose a synthetic augmentation method called HistoDiffusion, which can be pre-trained on large-scale unlabeled datasets and later applied to a small-scale labeled dataset for augmented training. In particular, we train a latent diffusion model (LDM) on diverse unlabeled datasets to learn common features and generate realistic images without conditional inputs. Then, we fine-tune the model with classifier guidance in latent space on an unseen labeled dataset so that the model can synthesize images of specific categories. Additionally, we adopt a selective mechanism to only add synthetic samples with high confidence of matching to target labels. We evaluate our proposed method by pre-training on three histopathology datasets and testing on a histopathology dataset of colorectal cancer (CRC) excluded from the pre-training datasets. With HistoDiffusion augmentation, the classification accuracy of a backbone classifier is remarkably improved by 6.4% using a small set of the original labels. Our code is available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8–12, 2023, Proceedings, Part II},
pages = {754–764},
numpages = {11},
location = {<conf-loc content-type="InPerson">Vancouver, BC, Canada</conf-loc>}
}

@inproceedings{10.1007/978-3-031-44137-0_35,
author = {Venkataramanan, Aishwarya and Laviale, Martin and Pradalier, C\'{e}dric},
title = {Integrating Visual and&nbsp;Semantic Similarity Using Hierarchies for&nbsp;Image Retrieval},
year = {2023},
isbn = {978-3-031-44136-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44137-0_35},
doi = {10.1007/978-3-031-44137-0_35},
abstract = {Most of the research in content-based image retrieval (CBIR) focus on developing robust feature representations that can effectively retrieve instances from a database of images that are visually similar to a query. However, the retrieved images sometimes contain results that are not semantically related to the query. To address this, we propose a method for CBIR that captures both visual and semantic similarity using a visual hierarchy. The hierarchy is constructed by merging classes with overlapping features in the latent space of a deep neural network trained for classification, assuming that overlapping classes share high visual and semantic similarities. Finally, the constructed hierarchy is integrated into the distance calculation metric for similarity search. Experiments on standard datasets: CUB-200-2011 and CIFAR100, and a real-life use case using diatom microscopy images show that our method achieves superior performance compared to the existing methods on image retrieval.},
booktitle = {Computer Vision Systems: 14th International Conference, ICVS 2023, Vienna, Austria, September 27–29, 2023, Proceedings},
pages = {422–431},
numpages = {10},
keywords = {Content-based image retrieval, Visual Hierarchy, Similarity Search},
location = {VIenna, Austria}
}

@inproceedings{10.1007/978-3-031-44084-7_7,
author = {Kaushik, Bhavana and Vijayvargiya, Ankur and Uppal, Jayant and Gupta, Ankit},
title = {Comparative Analysis of Machine Learning Approaches for Classifying Erythemato-Squamous Skin Diseases},
year = {2023},
isbn = {978-3-031-44083-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44084-7_7},
doi = {10.1007/978-3-031-44084-7_7},
abstract = {In the recent era, Machine Learning and Artificial Intelligence have come to a very great development point as we can use ML algorithms to predict the type of Erythemato-Squamous (Skin) diseases of the skin. In Dermatology, differential diagnosis of skin diseases is quite challenging in real life because most skin diseases share many histopathological features. And in this work, Psoriasis, Lichen Planus, Seborrheic Dermatitis, Chronic Dermatitis, Pityriasis Rosea, and Pityriasis Rubra Pilaris are among the skin illnesses for which eight different algorithm analytical comparison is done. Moreover, each classifier algorithm is discussed in detail with its pros and cons. The machine learning algorithms like Support Vector Machine, Decision tree, Random Forest, KNN, Na\"{\i}ve Bayes, Gradient Boosting, XGBoost, and Multilayer Perception have been proven to be successful in preserving state information through exact segmentation/classification. Random forest, Gradient Boosting, and XGBoost outperform all other methods and give an accuracy of 100% on the given ESD dataset. While Support Vector Machine gives the least accuracy of 72.97%. The paper also discusses the difficulties connected with skin disease segmentation or categorization. Furthermore, the study proposes future potential directions that include real-time analysis.},
booktitle = {Mining Intelligence and Knowledge Exploration: 9th International Conference, MIKE 2023, Kristiansand, Norway, June 28–30, 2023, Proceedings},
pages = {67–77},
numpages = {11},
keywords = {Comparative Analysis, Skin Diseases, Classification, Machine Learning, Erythemato-Squamous Diseases},
location = {Kristiansand, Norway}
}

@inproceedings{10.1007/978-3-031-43148-7_46,
author = {Bontempo, Gianpaolo and Bartolini, Nicola and Lovino, Marta and Bolelli, Federico and Virtanen, Anni and Ficarra, Elisa},
title = {Enhancing PFI Prediction with&nbsp;GDS-MIL: A Graph-Based Dual Stream MIL Approach},
year = {2023},
isbn = {978-3-031-43147-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43148-7_46},
doi = {10.1007/978-3-031-43148-7_46},
abstract = {Whole-Slide Images (WSI) are emerging as a promising resource for studying biological tissues, demonstrating a great potential in aiding cancer diagnosis and improving patient treatment. However, the manual pixel-level annotation of WSIs is extremely time-consuming and practically unfeasible in real-world scenarios. Multi-Instance Learning (MIL) have gained attention as a weakly supervised approach able to address lack of annotation tasks. MIL models aggregate patches (e.g., cropping of a WSI) into bag-level representations (e.g., WSI label), but neglect spatial information of the WSIs, crucial for histological analysis. In the High-Grade Serous Ovarian Cancer (HGSOC) context, spatial information is essential to predict a prognosis indicator (the Platinum-Free Interval, PFI) from WSIs. Such a prediction would bring highly valuable insights both for patient treatment and prognosis of chemotherapy resistance. Indeed, NeoAdjuvant ChemoTherapy (NACT) induces changes in tumor tissue morphology and composition, making the prediction of PFI from WSIs extremely challenging. In this paper, we propose GDS-MIL, a method that integrates a state-of-the-art MIL model with a Graph ATtention layer (GAT in short) to inject a local context into each instance before MIL aggregation. Our approach achieves a significant improvement in accuracy on the “Ome18” PFI dataset. In summary, this paper presents a novel solution for enhancing PFI prediction in HGSOC, with the potential of significantly improving treatment decisions and patient outcomes.},
booktitle = {Image Analysis and Processing – ICIAP 2023: 22nd International Conference, ICIAP 2023, Udine, Italy, September 11–15, 2023, Proceedings, Part I},
pages = {550–562},
numpages = {13},
location = {<conf-loc content-type="InPerson">Udine, Italy</conf-loc>}
}

@inproceedings{10.1007/978-981-99-4749-2_31,
author = {Gao, Yue and Zhang, Dai-Jun and Jiao, Cui-Na and Gao, Ying-Lian and Liu, Jin-Xing},
title = {Spatial Domain Identification Based on Graph Attention Denoising Auto-encoder},
year = {2023},
isbn = {978-981-99-4748-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-4749-2_31},
doi = {10.1007/978-981-99-4749-2_31},
abstract = {One of the great challenges faced by spatial transcriptomics research is to identify spatial domains that have similarities in gene expression and histology. Most research only depends on gene expression information and is incapable of efficiently utilizing spatial information. Auto-encoder has been proven to be an effective foundation for unsupervised learning. However, traditional auto-encoder cannot utilize explicit relationships in structured data. In order to make better use of embedded feature representation and exploit relationships in graph structured data, an improvement has been made to the graph attention auto-encoder: the auto-encoder is made up of three encoder layers and three decoder layers, and random Gaussian noise is added to the encoder’s working process, thereby generating a graph attention denoising auto-encoder (GADAE). Latent embeddings in low dimensions can be learned by merging spatial information with underlying expression patterns to effectively identify spatial domains. Experimental results show that compared to competitive methods, it can identify spatial domains and locate genes with more abundant spatial expression patterns.},
booktitle = {Advanced Intelligent Computing Technology and Applications: 19th International Conference, ICIC 2023, Zhengzhou, China, August 10–13, 2023, Proceedings, Part III},
pages = {359–367},
numpages = {9},
keywords = {Graph Attention Denoising Auto-encoder, Spatial Domain Identification, Single Cell Transcriptomics},
location = {Zhengzhou, China}
}

@inproceedings{10.1007/978-3-031-39539-0_9,
author = {Wang, Zuhui},
title = {Cross-Domain Microscopy Cell Counting By Disentangled Transfer Learning},
year = {2023},
isbn = {978-3-031-39538-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-39539-0_9},
doi = {10.1007/978-3-031-39539-0_9},
abstract = {Microscopy images from different imaging conditions, organs, and tissues often have numerous cells with various shapes on a range of backgrounds. As a result, designing a deep learning model to count cells in a source domain becomes precarious when transferring them to a new target domain. To address this issue, manual annotation costs are typically the norm when training deep learning-based cell counting models across different domains. In this paper, we propose a cross-domain cell counting approach that requires only weak human annotation efforts. Initially, we implement a cell counting network that disentangles domain-specific knowledge from domain-agnostic knowledge in cell images, where they pertain to the creation of domain style images and cell density maps, respectively. We then devise an image synthesis technique capable of generating massive synthetic images founded on a few target-domain images that have been labeled. Finally, we use a public dataset consisting of synthetic cells as the source domain, where no manual annotation cost is present, to train our cell counting network; subsequently, we transfer only the domain-agnostic knowledge to a new target domain of real cell images. By progressively refining the trained model using synthesized target-domain images and several real annotated ones, our proposed cross-domain cell counting method achieves good performance compared to state-of-the-art techniques that rely on fully annotated training images in the target domain. We evaluated the efficacy of our cross-domain approach on two target domain datasets of actual microscopy cells, demonstrating the feasibility of requiring annotations on only a few images in a new domain.},
booktitle = {Trustworthy Machine Learning  for Healthcare: First International Workshop, TML4H 2023, Virtual Event, May 4, 2023,  Proceedings},
pages = {93–105},
numpages = {13},
keywords = {Transfer learning, Knowledge disentangling, Cell counting}
}

@inproceedings{10.1007/978-3-031-37660-3_35,
author = {Mahbub, Taslim and Obeid, Ahmad and Javed, Sajid and Dias, Jorge and Werghi, Naoufel},
title = {Class-Balanced Affinity Loss for&nbsp;Highly Imbalanced Tissue Classification in&nbsp;Computational Pathology},
year = {2023},
isbn = {978-3-031-37659-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-37660-3_35},
doi = {10.1007/978-3-031-37660-3_35},
abstract = {Early detection of cancer, and breast cancer in particular, can have a positive impact on the survival rate of cancer patients. However, visual inspection by expert pathologists of whole-slide-images is subjective and error-prone given the lack of skilled pathologists. To overcome this limitation, many researchers have proposed deep learning driven approaches to detect breast cancer from histopathology images. However, these datasets are often highly imbalanced as patches belonging to the cancerous category is minor in comparison to the healthy cells. Therefore, when trained, the classification performance of the conventional Convolutional Neural Network (CNN) models drastically decreases, particularly for the minor class, which is often the main target of detection. This paper proposes a class balanced affinity loss function which can be injected at the output layer to any deep learning classifier model to address the imbalance learning. In addition to treating the imbalance, the proposal also builds uniformly spread class prototypes to address the fine-grained classification challenge in histopathology datasets, which conventional softmax loss cannot address. We validate our loss function performance by using two publicly available datasets with different levels of imbalance, namely the Invasive Ductal Carcinoma (IDC) and Colorectal cancer (CRC) datasets. In both cases, our method results in better performance, especially for the minority. We also observe a better 2D feature projection in multi-class classification with the proposed loss function, making it more apt to handle imbalanced fine-grained classification challenges.},
booktitle = {Pattern Recognition, Computer Vision, and Image Processing. ICPR 2022 International Workshops and Challenges: Montreal, QC, Canada, August 21–25, 2022, Proceedings, Part I},
pages = {499–513},
numpages = {15},
keywords = {Histopathology Cancer Diagnosis, Cluster-based Feature Learning, Class-Balanced Affinity Loss, Imbalanced Classification},
location = {Montr\'{e}al, QC, Canada}
}

@inproceedings{10.1007/978-3-031-37129-5_22,
author = {Akhtar, Sania and Hanif, Muhammad and Malih, Hamidi},
title = {Automatic Urine Sediment Detection and&nbsp;Classification Based on&nbsp;YoloV8},
year = {2023},
isbn = {978-3-031-37128-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-37129-5_22},
doi = {10.1007/978-3-031-37129-5_22},
abstract = {The identification of urine sediment in human urine samples through microscopic images is a critical part of in vitro testing. Currently, automatic urine sediment analyzers are used by doctors to supplement manual examinations. However, the conventional technique of artificial feature extraction used by most analyzers can be labor-intensive and subjectively dependent on the professional’s prior knowledge. To overcome these limitations, this work employs YoloV8, a recent version of the Yolo algorithm, to accurately detect and categorize urine particles. In addition, a data-centric strategy has been introduced to address difficulties with missing data, incorrect labeling, and class imbalance. This strategy aims to improve labeling reliability and remove noisy data points. Experimental findings on the dataset show that YOLOv8 has a greater detection accuracy than existing state-of-the-art techniques for detecting eleven different categories of urine sediments. The approach presented in this work outperforms other techniques, yielding a mean average precision (mAP) of 91%. Furthermore, the average detection time of the model is 0.6 microseconds.},
booktitle = {Computational Science and Its Applications – ICCSA 2023 Workshops: Athens, Greece, July 3–6, 2023, Proceedings, Part IX},
pages = {269–279},
numpages = {11},
keywords = {Feature Extraction, object detection, YoloV8, Urine Sediments},
location = {Athens, Greece}
}

@inproceedings{10.1145/3555776.3578609,
author = {Sloan, Derek and Dombay, Evelin and Sabiiti, Wilber and Mtafya, Bariki and Arandelovic, Ognjen and Zachariou, Marios},
title = {Estimating Phenotypic Characteristics of Tuberculosis Bacteria},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578609},
doi = {10.1145/3555776.3578609},
abstract = {Microscopy analysis of sputum images for bacilli screening is a common method used for both diagnosis and therapy monitoring of tuberculosis (TB). Nonetheless, it is a challenging procedure, since sputum examination is time-consuming and needs highly competent personnel to provide accurate results which are important for clinical decision-making. In addition, manual fluorescence microscopy examination of sputum samples for tuberculosis diagnosis and treatment monitoring is a subjective operation. In this work, we automate the process of examining fields of view (FOVs) of TB bacteria in order to determine the lipid content, and bacterial length and width. We propose a modified version of the UNet model to rapidly localise potential bacteria inside a FOV. We introduce a novel method that uses Fourier descriptors to exclude contours that do not belong to the class of bacteria, hence minimising the amount of false positives. Finally, we propose a new feature as a means of extracting a representation fed into a support vector multi-regressor in order to estimate the length and width of each bacterium. Using a real-world data corpus, the proposed method i) outperformed previous methods, and ii) estimated the cell length and width with a root mean square error of less than 0.01%.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1110–1113},
numpages = {4},
keywords = {mycobacterium tuberculosis, tuberculosis, treatment monitoring, deep learning, regression, MSVR, shape descriptors, fluorescence, machine learning, microscopy, ACM proceedings},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3555776.3577866,
author = {Andrade, Thiago and Gama, Joao},
title = {Estimating Instantaneous Vehicle Emissions},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577866},
doi = {10.1145/3555776.3577866},
abstract = {Road transportation emissions have increased in the last few decades and have been the primary source of pollutants in urban areas with ever-growing populations. In this context, it is important to have effective measures to monitor road emissions in regions. Creating an emissions inventory over a region that can map road emissions based on vehicle trips can be helpful. In this work, we show that it is possible to use raw GPS data to estimate vehicle-related levels of pollution in a region. By transforming the data using feature engineering and calculating the vehicle-specific power (VSP) as well as various specific pollutants by using a microscopic emissions model, we show the areas with higher emissions levels made by a fleet of taxis in Porto, Portugal.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {422–424},
numpages = {3},
keywords = {air pollution, climate change, vehicle-specific power, microscopic emissions model, road emissions},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3590837.3590839,
author = {Chand, Sunita and Vishwakarma, Virendra P.},
title = {Acute Leukaemia Diagnosis Using Transfer Learning on Resnet-50},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590839},
doi = {10.1145/3590837.3590839},
abstract = {This paper presents an automated model for leukaemia detection that is based on the computational power of a deep pre-trained model Resnet-50. The conventional manual method to detect the disease from microscopic blood cell images is time driven and the diagnosis is subjective due to the variation of technical expertise of the hematologists and may vary from one pathologist to other. Hence a model is proposed that exploits the transfer learning technique on Resnet-50 to learn the features of microscopic blood cell images from the Acute Lymphoblastic Leukemia Image Database for Image Processing (ALL-IDB1) to classify them into diseased and healthy. As the number of images in the dataset is very less for training on deep-network, the model may overfit. As a precautionary measure, augmentation of images is performed during the training. Apart from image augmentation, L2 regularization is also used to reduce overfitting. The proposed model demonstrates 100% accuracy on unseen test images with Resnet50. The comparison of the obtained results is done with state-of-the-art work performed by contemporary researchers.},
booktitle = {Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
articleno = {2},
numpages = {7},
keywords = {Transfer learning, Resnet-50, Machine learning, Deep learning, Classification, Acute lymphoblastic leukemia},
location = {Jaipur, India},
series = {ICIMMI '22}
}

@inproceedings{10.1007/978-3-031-31975-4_38,
author = {Stergiopoulou, Vasiliki and Mukherjee, Subhadip and Calatroni, Luca and Blanc-F\'{e}raud, Laure},
title = {Fluctuation-Based Deconvolution in&nbsp;Fluorescence Microscopy Using Plug-and-Play Denoisers},
year = {2023},
isbn = {978-3-031-31974-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-31975-4_38},
doi = {10.1007/978-3-031-31975-4_38},
abstract = {The spatial resolution of images of living samples obtained by fluorescence microscopes is physically limited due to the diffraction of visible light, which makes the study of entities of size less than the diffraction barrier (around 200 nm in the x-y plane) very challenging. To overcome this limitation, several deconvolution and super-resolution techniques have been proposed. Within the framework of inverse problems, modern approaches in fluorescence microscopy reconstruct a super-resolved image from a temporal stack of frames by carefully designing suitable hand-crafted sparsity-promoting regularisers. Numerically, such approaches are solved by proximal gradient-based iterative schemes. Aiming at obtaining a reconstruction more adapted to sample geometries (e.g.&nbsp;thin filaments), we adopt a plug-and-play denoising approach with convergence guarantees and replace the proximity operator associated with the explicit image regulariser with an image denoiser (i.e. a pre-trained network) which, upon appropriate training, mimics the action of an implicit prior. To account for the independence of the fluctuations between molecules, the model relies on second-order statistics. The denoiser is then trained on covariance images coming from data representing sequences of fluctuating fluorescent molecules with filament structure. The method is evaluated on both simulated and real fluorescence microscopy images, showing its ability to correctly reconstruct filament structures with high values of peak signal-to-noise ratio (PSNR).},
booktitle = {Scale Space and Variational Methods in Computer Vision: 9th International Conference, SSVM 2023, Santa Margherita Di Pula, Italy, May 21–25, 2023, Proceedings},
pages = {498–510},
numpages = {13},
keywords = {Fluorescence microscopy, Image deconvolution, Variational regularisation, Proximal algorithms, Plug-and-Play regularisation},
location = {<conf-loc content-type="InPerson">Santa Margherita di Pula, Italy</conf-loc>}
}

@inproceedings{10.1145/3573942.3574042,
author = {Wang, Qian and Guo, Jie and Fang, Haonan},
title = {Nuclei Segmentation Using Cascaded Bilateral Attention U-Net},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3574042},
doi = {10.1145/3573942.3574042},
abstract = {Medical image segmentation plays an indispensable role in biomedical development, especially in automatic disease diagnosis and treatment. The task of semantic segmentation is to group parts of an image that belong to the same object class together. In neuroscience studies, automated, accurate, and high-throughput nuclear segmentation methods are of high demand to quantify the number of cells. In this paper, we propose a cascaded U-Net network architecture with bilateral attention gate. In order to obtain nuclei edge for followed counting cell, we design the cascaded networks with different segmentation tasks. A U-Net is used as the first layer of the network to roughly segment the nucleus region, and the next layer uses a bilateral attention U-Net model with a gating mechanism to fine-tune the nucleus segmentation (including nuclei, edges, and background). The bilateral gating mechanism can learn importance of the features at different scales. Using the proposed network, experiments are conducted on a dataset of microscopic nuclear images. According to experimental results, the cascaded multi-task model with bilateral attention gate outperforms individual U-Net network and attention U-Net.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {412–418},
numpages = {7},
keywords = {Attention U-Net, Bilateral attention, Cascading learning, Image segmentation},
location = {<conf-loc>, <city>Xiamen</city>, <country>China</country>, </conf-loc>},
series = {AIPR '22}
}

@inproceedings{10.1145/3543507.3583503,
author = {Jin, Yiqiao and Bai, Yunsheng and Zhu, Yanqiao and Sun, Yizhou and Wang, Wei},
title = {Code Recommendation for Open Source Software Developers},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583503},
doi = {10.1145/3543507.3583503},
abstract = {Open Source Software (OSS) is forming the spines of technology infrastructures, attracting millions of talents to contribute. Notably, it is challenging and critical to consider both the developers’ interests and the semantic features of the project code to recommend appropriate development tasks to OSS developers. In this paper, we formulate the novel problem of code recommendation, whose purpose is to predict the future contribution behaviors of developers given their interaction history, the semantic features of source code, and the hierarchical file structures of projects. We introduce CODER, a novel graph-based CODE Recommendation framework for open source software developers, which accounts for the complex interactions among multiple parties within the system. CODER jointly models microscopic user-code interactions and macroscopic user-project interactions via a heterogeneous graph and further bridges the two levels of information through aggregation on file-structure graphs that reflect the project hierarchy. Moreover, to overcome the lack of reliable benchmarks, we construct three large-scale datasets to facilitate future research in this direction. Extensive experiments show that our CODER framework achieves superior performance under various experimental settings, including intra-project, cross-project, and cold-start recommendation.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1324–1333},
numpages = {10},
keywords = {Code recommendation, graph neural networks, multimodal recommendation, open source software development, recommender system},
location = {<conf-loc>, <city>Austin</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
series = {WWW '23}
}

@inproceedings{10.1145/3544549.3577058,
author = {Pasumarthy, Nandini},
title = {Designing Interactive Experiences For Gut Health Engagement and Reflection},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3577058},
doi = {10.1145/3544549.3577058},
abstract = {Human gut health is the epicentre of human health and well-being. We engage in microscopic interactions in our day-to-day lives that influence our gut health, however, our understanding of this relationship is scarce. Current approaches to engage people on gut-related factors are heavily jargonised, lacking real-world application and focus on disease-causing aspects, thus limiting motivation to engage with gut health. Research suggests that games can act as powerful tools for engagement and reflection on this topic. This PhD research explores the design of two games to understand the key game design features that enable engagement with gut health. The results from testing these games will be generalised to inform the design of physical and digital games for engagement and reflection on health.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {494},
numpages = {6},
keywords = {gut health, gut microbiome, human-food interaction, persuasive games, serious games},
location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
series = {CHI EA '23}
}

@inproceedings{10.1007/978-3-540-76725-1_84,
author = {D\'{\i}az, Gloria and Gonzalez, Fabio and Romero, Eduardo},
title = {Infected Cell Identification in Thin Blood Images Based on Color Pixel Classification: Comparison and Analysis},
year = {2023},
isbn = {978-3-540-76724-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-76725-1_84},
doi = {10.1007/978-3-540-76725-1_84},
abstract = {Malaria is an infectious disease which is mainly diagnosed by visual microscopical evaluation of Giemsa-stained thin blood films using a differential analysis of color features. This paper presents the evaluation of a color segmentation technique, based on standard supervised classification algorithms. The whole approach uses a general purpose classifier, which is parameterized and adapted to the problem of separating image pixels into three different classes: parasite, blood red cells and background. Assessment included not only four different supervised classification techniques - KNN, Naive Bayes, SVM and MLP - but different color spaces -RGB, normalized RGB, HSV and YCbCr-. Results show better performance for the KNN classifiers along with an improving feature characterization in the normalized RGB color space.},
booktitle = {Progress in Pattern Recognition, Image Analysis and Applications: 12th Iberoamericann Congress on Pattern Recognition, CIARP 2007, Valparaiso, Chile, November 13-16, 2007. Proceedings},
pages = {812–821},
numpages = {10},
keywords = {Performance comparison, Color spaces, Supervised classification, Cell detection},
location = {Valpara\'{\i}so, Chile}
}

@inproceedings{10.1007/978-3-540-76725-1_79,
author = {D\'{\i}az, Gloria and Gonzalez, Fabio and Romero, Eduardo},
title = {Automatic Clump Splitting for Cell Quantification in Microscopical Images},
year = {2023},
isbn = {978-3-540-76724-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-76725-1_79},
doi = {10.1007/978-3-540-76725-1_79},
abstract = {This paper presents an original method for splitting overlapped cells in microscopical images, based on a template matching strategy. First,a single template cell is estimated using an Expectation Maximization algorithm applied to a collection of correctly segmented cells from the original image. Next, a process based on matching the template against the clumped shape and removing the matched area is applied iteratively. A chain code representation is used for establishing best correlation between these two shapes. Maximal correlation point is used as an landmark point for the registration approach, which finds the affine transformation that maximises the intersection area between both shapes. Evaluation was carried out on 18 images in which 52 clumped shapes were present. The number of found cells was compared with the number of cells counted by an expert and results show agreement on a  of the cases.},
booktitle = {Progress in Pattern Recognition, Image Analysis and Applications: 12th Iberoamericann Congress on Pattern Recognition, CIARP 2007, Valparaiso, Chile, November 13-16, 2007. Proceedings},
pages = {763–772},
numpages = {10},
keywords = {Clump splitting, Segmentation, Overlapping objects, Cell quantification},
location = {Valpara\'{\i}so, Chile}
}

@inproceedings{10.1007/978-3-540-75757-3_75,
author = {Yang, Lin and Chen, Wenjin and Meer, Peter and Salaru, Gratian and Feldman, Michael D. and Foran, David J.},
title = {High Throughput Analysis of Breast Cancer Specimens on the Grid},
year = {2023},
isbn = {978-3-540-75756-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-75757-3_75},
doi = {10.1007/978-3-540-75757-3_75},
abstract = {Breast cancer accounts for about 30% of all cancers and 15% of all cancer deaths in women in the United States. Advances in computer assisted diagnosis (CAD) holds promise for early detecting and staging disease progression. In this paper we introduce a Grid-enabled CAD to perform automatic analysis of imaged histopathology breast tissue specimens. More than 100,000 digitized samples (1200\texttimes{}1200 pixels) have already been processed on the Grid. We have analyzed results for 3744 breast tissue samples, which were originated from four different institutions using diaminobenzidine (DAB) and hematoxylin staining. Both linear and nonlinear dimension reduction techniques are compared, and the best one (ISOMAP) was applied to reduce the dimensionality of the features. The experimental results show that the Gentle Boosting using an eight node CART decision tree as the weak learner provides the best result for classification. The algorithm has an accuracy of 86.02% using only 20% of the specimens as the training set.},
booktitle = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2007: 10th International Conference, Brisbane, Australia, October 29 - November 2, 2007, Proceedings, Part I},
pages = {617–625},
numpages = {9},
keywords = {Locally Linear Embedding, Weak Learner, High Throughput Analysis, Breast Tissue, Breast Cancer},
location = {Brisbane, QLD, Australia}
}

@inproceedings{10.1145/3539597.3573030,
author = {Wei, Jianping and Zhu, Zhibo and Liu, Ziqi and Zhang, Zhiqiang and Zhou, Jun},
title = {AntTS: A Toolkit for Time Series Forecasting in Industrial Scenarios},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3573030},
doi = {10.1145/3539597.3573030},
abstract = {Time series forecasting is an important ingredient in the intelligence of business and decision processes. In industrial scenarios, the time series of interest are mostly macroscopic time series that are aggregated from microscopic time series, e.g., the retail sales is aggregated from the sales of different goods, and that are also intervened by certain treatments on the microscopic individuals, e.g., issuing discount coupons on some goods to increase the retail sales. These characteristics are not considered in existing toolkits, which just focus on the "natural" time series forecasting that predicts the future value based on historical data, regardless of the impact of treatments. In this paper, we present AntTS, a time series toolkit paying more attention on the forecasting of the macroscopic time series with underlying microscopic time series and certain treatments, besides the "natural" time series forecasting. AntTS consists of three decoupled modules, namely Clustering module, Natural Forecasting module, and Effect module, which are utilized to study the homogeneous groups of microscopic individuals, the "natural" time series forecasting of homogeneous groups, and the treatment effect estimation of homogeneous groups. With the combinations of different modules, it can exploit the microscopic individuals and the interventions on them, to help the forecasting of macroscopic time series. We show that AntTS helps address many typical tasks in the industry.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {1192–1195},
numpages = {4},
keywords = {clustering, microscopic individuals, time series forecasting, treatment effect},
location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
series = {WSDM '23}
}

@inproceedings{10.1007/978-3-031-25082-8_50,
author = {Mahapatra, Dwarikanath and Korevaar, Steven and Bozorgtabar, Behzad and Tennakoon, Ruwan},
title = {Unsupervised Domain Adaptation Using Feature Disentanglement and&nbsp;GCNs for&nbsp;Medical Image Classification},
year = {2023},
isbn = {978-3-031-25081-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-25082-8_50},
doi = {10.1007/978-3-031-25082-8_50},
abstract = {The success of deep learning has set new benchmarks for many medical image analysis tasks. However, deep models often fail to generalize in the presence of distribution shifts between training (source) data and test (target) data. One method commonly employed to counter distribution shifts is domain adaptation: using samples from the target domain to learn to account for shifted distributions. In this work we propose an unsupervised domain adaptation approach that uses graph neural networks and, disentangled semantic and domain invariant structural features, allowing for better performance across distribution shifts. We propose an extension to swapped autoencoders to obtain more discriminative features. We test the proposed method for classification on two challenging medical image datasets with distribution shifts - multi center chest Xray images and histopathology images. Experiments show our method achieves state-of-the-art results compared to other domain adaptation methods.},
booktitle = {Computer Vision – ECCV 2022 Workshops: Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VII},
pages = {735–748},
numpages = {14},
keywords = {Unsupervised domain adaptation, Graph convolution networks, Camelyon17, CheXpert, NIH Xray},
location = {<conf-loc content-type="InPerson">Tel Aviv, Israel</conf-loc>}
}

@inproceedings{10.1007/978-3-031-24841-2_5,
author = {van den Berg, Birthe and Schrijvers, Tom and Dedecker, Peter},
title = {: A Domain-Specific Language for&nbsp;Experiments in&nbsp;Fluorescence Microscopy (Application Paper)},
year = {2023},
isbn = {978-3-031-24840-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-24841-2_5},
doi = {10.1007/978-3-031-24841-2_5},
abstract = {Fluorescence microscopy is a true workhorse in the domain of life sciences, essential for unraveling the inner workings of cells and tissue. It is not only used from day to day in industry, also academia push boundaries in research using and doing fluorescence microscopy. It is in the latter context that software that is sufficiently modular in terms of experiments and hardware is desirable. Existing solutions are too closely tailored to their accompanying hardware setup or too limited in terms of expressivity. We present : a domain-specific language (DSL) in Haskell for setting up fluorescence microscopy experiments that can be combined and nested freely. It provides domain-specific features such as stage loops and time lapses, and is modular in terms of hardware connections.  has been operational since 2015 at the Nanobiology Lab. It has not only improved researchers’ efficiency, but has also given rise to novel research results. For example, performing simultaneous F\"{o}rster Resonant Energy Transfer (FRET) measurements, a mechanism for tracking energy transfer between a donor-acceptor pair, uses advanced time-lapse experiments and serves as an example use case in the paper. We reflect on the choice of Haskell as a host language and the usability of the DSL.},
booktitle = {Practical Aspects of Declarative Languages: 25th International Symposium, PADL 2023, Boston, MA, USA, January 16–17, 2023, Proceedings},
pages = {73–82},
numpages = {10},
location = {Boston , MA, USA}
}

@inproceedings{10.1145/3570991.3571046,
author = {Garg, Armaan},
title = {Addressing Data Intrinsic Characteristics for Augmentation for Breast Cancer Classification},
year = {2023},
isbn = {9781450397971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570991.3571046},
doi = {10.1145/3570991.3571046},
abstract = {Breast cancer is the most frequently diagnosed cancer among females worldwide. The task of correctly diagnosing cancer using histopathology in its very earlier stages is a challenging and critical task. Most of the present machine learning techniques require a lot of data to analyze and predict a benign tumour in its early stages, and such data is not available readily. In this paper, we propose the idea of data augmentation of breast cancer tissue images by addressing data intrinsic characteristics. The aim is to detect the micro presence of the tumour cells and highlight it over multiple synthetic images for classifiers to predict benign tumours in very early stages with high accuracy. The initial experimental analysis highlights the proposed technique’s impact and significance in boosting the performance of standard classifier(s).},
booktitle = {Proceedings of the 6th Joint International Conference on Data Science &amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)},
pages = {299–300},
numpages = {2},
keywords = {Tissue images, Histopathology, Data augmentation, Classification., Breast cancer},
location = {Mumbai, India},
series = {CODS-COMAD '23}
}

@inproceedings{10.1007/978-3-031-20500-2_47,
author = {Pei, Zongxiang and Zuo, Yingli and Sun, Liang and Wang, Meiling and Zhang, Daoqiang and Shao, Wei},
title = {Integrative Analysis of&nbsp;Multi-view Histopathological Image Features for&nbsp;the&nbsp;Diagnosis of&nbsp;Lung Cancer},
year = {2023},
isbn = {978-3-031-20499-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-20500-2_47},
doi = {10.1007/978-3-031-20500-2_47},
abstract = {Lung cancer is one of the most widely spread cancers in the world. So far, the histopathological image remains the “gold standard” in diagnosing lung cancers, and multiple types of pathological images features have been associated with lung cancer diagnosis and progression. However, most of the existing studies only utilized single type of image features, which did not take advantages of multiple types of image features. In this paper, we propose a Block based Multi-View Graph Convolutional Network (i.e., BMVGCN), which integrates multiple types of image features from histopathological images for lung cancer diagnosis. Specifically, our method utilizes the block-based bilinear combination model to fuse different types of features. By considering the correlation among different samples, we also introduce the Graph Convolutional Network to exploit the correlations among samples that could lead to better diagnosis performance. To evaluate the effectiveness of the proposed method, we conduct the experiments for the classification of the cancer tissue and non-cancer tissue in both Lung Adenocarcinoma (i.e., LUAD) and Lung Squamous Cell Carcinoma (i.e.,LUSC), and the discrimination between LUAD and LUSC. The results show that our method can achieve superior classification performance than the comparing methods.},
booktitle = {Artificial Intelligence: Second CAAI International Conference, CICAI 2022, Beijing, China, August 27–28, 2022, Revised Selected Papers, Part II},
pages = {577–587},
numpages = {11},
keywords = {Multi-view fusion, Graph neural network, Histopathological image, Lung cancer diagnosis},
location = {Beijing, China}
}

@inproceedings{10.1007/978-3-319-10404-1_9,
author = {Lucchi, Aur\'{e}lien and Becker, Carlos and M\'{a}rquez Neila, Pablo and Fua, Pascal},
title = {Exploiting Enclosing Membranes and Contextual Cues for Mitochondria Segmentation},
year = {2022},
isbn = {978-3-319-10403-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-10404-1_9},
doi = {10.1007/978-3-319-10404-1_9},
abstract = {In this paper, we improve upon earlier approaches to segmenting mitochondria in Electron Microscopy images by explicitly modeling the double membrane that encloses mitochondria, as well as using features that capture context over an extended neighborhood. We demonstrate that this results in both improved classification accuracy and reduced computational requirements for training.},
booktitle = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2014},
pages = {65–72},
numpages = {8}
}

@inproceedings{10.1007/978-3-319-10404-1_11,
author = {Kainmueller, Dagmar and Jug, Florian and Rother, Carsten and Myers, Gene},
title = {Active Graph Matching for Automatic Joint Segmentation and Annotation of C. elegans
},
year = {2022},
isbn = {978-3-319-10403-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-10404-1_11},
doi = {10.1007/978-3-319-10404-1_11},
abstract = {In this work we present a novel technique we term active graph matching, which integrates the popular active shape model into a sparse graph matching problem. This way we are able to combine the benefits of a global, statistical deformation model with the benefits of a local deformation model in form of a second-order random field. We present a new iterative energy minimization technique which achieves empirically good results. This enables us to exceed state-of-the art results for the task of annotating nuclei in 3D microscopic images of C. elegans. Furthermore with the help of the generalized Hough transform we are able to jointly segment and annotate a large set of nuclei in a fully automatic fashion for the first time.},
booktitle = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2014},
pages = {81–88},
numpages = {8},
keywords = {Active Graph, Nucleus Location, Active Shape Model, Graph Match, Manual Segmentation}
}

@inproceedings{10.1145/3565291.3565320,
author = {Xu, Xiaofeng and Lan, Yihua and Li, He and Qi, Qinglei and Liu, Quan},
title = {An Automatic Method for Fast Synaptic Signal Detection and Count},
year = {2022},
isbn = {9781450396875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565291.3565320},
doi = {10.1145/3565291.3565320},
abstract = {Synapses are the special connection structure between brain neurons, and they are the core element of brain plasticity and information transmission. In order to better understand the process of brain information transmission, it is necessary to quantitatively calculate the synapse, but the automatic identification of a large range of synaptic images is still a difficult problem. According to the characteristics of synaptic signal in microscopy optical mouse brain images, this paper proposes the automatic fast synapse recognition method based on density peak clustering, which solves the identification problem of dense small synaptic signal, and realizes the rapid synapse positioning and counting in a large brain area. The results show that the proposed methods in accuracy and efficiency bring great convenience to the researchers.CCS CONCEPTS•Applied computing},
booktitle = {Proceedings of the 5th International Conference on Big Data Technologies},
pages = {178–182},
numpages = {5},
keywords = {Synapse, Positioning, Count},
location = {Qingdao, China},
series = {ICBDT '22}
}

@inproceedings{10.1007/978-3-031-21517-9_6,
author = {Kadry, Seifedine and Rajinikanth, Venkatesan and Srivastava, Gautam and Meqdad, Maytham N.},
title = {Mayfly-Algorithm Selected Features for Classification of Breast Histology Images into Benign/Malignant Class},
year = {2022},
isbn = {978-3-031-21516-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-21517-9_6},
doi = {10.1007/978-3-031-21517-9_6},
abstract = {Incidence rate of Breast Cancer (BC) is rising globally and the early detection is important to cure the disease. The detection of BC consist different phases from verification to clinical level diagnosis. Confirmation of the cancer and its stage is performed normally with breast biopsy. This research aims to develop a framework to identify Benign/Malignant class images from the Breast Histology Slide (BHS). This technique consist the following phases; (i) Cropping and resizing the image slice, (ii) Deep-feature extraction using pre-trained network, (iii) Discrete Wavelet Transform (DWT) feature mining, (iv) Optimal feature selection with Mayfly algorithm, (v) Serial feature concatenation, and (vi) Binary classification and validation. This work considered the test image with dimension 896\texttimes{}768\texttimes{}3 pixels. During the investigation, every picture is cropped into 25 slices and resized to 224\texttimes{}224\texttimes{}3 pixels. This work implements the following stages; (i) BC detection with deep-features and (ii) BC recognition with concatenated features. In both the cases, a 5-fold cross validation is employed and the experimental investigation of this research confirms that the proposed work helped to achieve an accuracy of 91.39% with deep-feature and 95.56% with concatenation features.},
booktitle = {Mining Intelligence and Knowledge Exploration: 9th International Conference, MIKE 2021, Hammamet, Tunisia, November 1–3, 2021, Proceedings},
pages = {57–66},
numpages = {10},
keywords = {Classification, DWT features, ResNet18, Histology slide, Breast cancer},
location = {Hammamet, Tunisia}
}

@inproceedings{10.1145/3570773.3570881,
author = {Li, Lin and Yan, Liang and Wang, Xupeng and Gao, Xinqin},
title = {Study on the effect of plantar orthosis on plantar pressure and gait &amp; posture of high arched foot},
year = {2022},
isbn = {9781450398442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570773.3570881},
doi = {10.1145/3570773.3570881},
abstract = {Because the medial longitudinal arch of high arched foot is too high, the elasticity of the foot arch is weakened. Compared with the balanced force on the sole of normal foot, the stability and buffering of the high arched foot are poor. Foot orthosis can effectively improve the dynamic plantar load of patients with high arch, reduce foot injury and relieve foot fatigue. In this paper, the effect of plantar orthosis on plantar pressure distribution and the characteristics of gait and posture of patients with high arched foot are analyzed and verified. The JasencoJsp (jasenco, JSP-C5, France) plantar pressure test system and the Sennotech Insole X dynamic gait analysis system are used to test a representative patient with high arched foot. The data of plantar pressure distribution, peak pressure, gait phase within a gait cycle, as well as foot microscopic characteristics before and after wearing the plantar orthosis are collected. The results show that the plantar orthosis can effectively reduce the plantar peak pressure and improve the plantar pressure distribution, so that the plantar pressure is relatively evenly distributed in the front, middle and rear foot. At the same time, the insufficient pronation during foot movement has been improved, the excessive tension of plantar fascia has been alleviated and the risk of abnormal gait such as varus has been reduced.},
booktitle = {Proceedings of the 3rd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {286–293},
numpages = {8},
keywords = {Plantar pressure, Plantar orthosis, High arched foot, Gait &amp;Posture},
location = {<conf-loc>, <city>Amsterdam</city>, <country>Netherlands</country>, </conf-loc>},
series = {ISAIMS '22}
}

@inproceedings{10.1145/3570773.3570845,
author = {Tian, Geng and Li, Xiaohang and Wu, Yi and Liu, Ao and Zhang, Ying and Ma, Yifei and Guo, Wenhui and Sun, Xiaoli and Fu, Bangze and Li, Da},
title = {Recognition effect of models based on different microscope objectives},
year = {2022},
isbn = {9781450398442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570773.3570845},
doi = {10.1145/3570773.3570845},
abstract = {Lonicerae japonicae flos, a common clinical Chinese medicine, is widely used in proprietary traditional Chinese medicine for the treatment of various conditions, such as fever, cough, and influenza. The microscopic features of honeysuckle pollen grains significantly correlate with their medicinal effects. In this study, deep learning using artificial intelligence was cross-combined with microscopic images of Chinese herbal medicines, and we proposed microscopic identification through an intelligent recognition method of honeysuckle pollen grains using microscopic images based on YOLO v5. The expandability of the microscopic feature recognition of different magnification models was verified based on different microscopic objectives. The honeysuckle pollen grains model based on YOLO v5 can quickly and accurately identify the microscopic images of pollen grains, which can provide a reference for the quality improvement and quality standardization of traditional Chinese herbs and has good application prospects.},
booktitle = {Proceedings of the 3rd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {133–141},
numpages = {9},
keywords = {traditional Chinese herbs, microscopic images, YOLO v5, Lonicerae japonicae flos},
location = {<conf-loc>, <city>Amsterdam</city>, <country>Netherlands</country>, </conf-loc>},
series = {ISAIMS '22}
}

@inproceedings{10.1007/978-3-031-19897-7_11,
author = {Jimenez, Maria-Jose and Medrano, Belen},
title = {Topological Analysis of&nbsp;Simple Segmentation Maps},
year = {2022},
isbn = {978-3-031-19896-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-19897-7_11},
doi = {10.1007/978-3-031-19897-7_11},
abstract = {In this paper, we propose a geometry-aware topological analysis of a segmentation of an image into regions which might correspond, for example, to a geographical map or to segmented cells in a microscopic image of a biological packed tissue. The regions must satisfy that the centroid of each one lies inside the region itself. We propose a novel simplicial complex modeling such data, for persistent homology computation, that better respects the geometry of the regions than existing techniques. More specifically, our approach joins benefits from previous models by encoding both neighbouring relations between the regions, as well as spatial distribution of the set of centroids. In addition, we introduce geometric information regarding distances between centroids and boundaries delimiting each region.},
booktitle = {Discrete Geometry and Mathematical Morphology: Second International Joint Conference, DGMM 2022, Strasbourg, France, October 24–27, 2022, Proceedings},
pages = {123–135},
numpages = {13},
keywords = {Topological organization of regions, Shape descriptor of regions, Segmentation map, Persistent homology},
location = {Strasbourg, France}
}

@inproceedings{10.1109/ITSC55140.2022.9922078,
author = {Zhao, Junxuan and Sartipi, Mina},
title = {Automatic Identification of Anomalous Driving Events from Trajectory Data},
year = {2022},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ITSC55140.2022.9922078},
doi = {10.1109/ITSC55140.2022.9922078},
abstract = {The high-resolution high-accuracy time-series trajectory data preserves rich motion status and characteristic information of vehicles in a microscopic manner. In order to analyze novel driving events or broadcast potentially hazardous situations to nearby road users, the initial step is how to identify them. Traditional approaches of manually checking trajectory datasets are extremely time-consuming and error-prone. Therefore, how to identify very few anomalous or atypical vehicle trajectories efficiently and effectively from a huge number of regular or typical trajectories is a problem that needs to be solved. This paper introduces an algorithm to automatically identify anomalous trajectories from recorded trajectory datasets in the order of trajectory clustering, template trajectory extraction, trajectory similarity comparison, and anomaly detection. A Dynamic Time Warping (DTW)-based hierarchical clustering model is leveraged to extract critical template trajectories from historical trajectories. By comparing the distance similarity with the templates, trajectories can be assigned a normal or anomalous label. A case study using a public vehicle trajectory dataset (inD dataset) demonstrates the effectiveness of the proposed method in anomaly detection at four urban intersections.},
booktitle = {2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)},
pages = {851–856},
numpages = {6},
location = {Macau, China}
}

@inproceedings{10.1007/978-3-031-16876-5_5,
author = {Wang, Ziyang and Voiculescu, Irina},
title = {Triple-View Feature Learning for&nbsp;Medical Image Segmentation},
year = {2022},
isbn = {978-3-031-16875-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16876-5_5},
doi = {10.1007/978-3-031-16876-5_5},
abstract = {Deep learning models, e.g. supervised Encoder-Decoder style networks, exhibit promising performance in medical image segmentation, but come with a high labelling cost. We propose TriSegNet, a semi-supervised semantic segmentation framework. It uses triple-view feature learning on a limited amount of labelled data and a large amount of unlabeled data. The triple-view architecture consists of three pixel-level classifiers and a low-level shared-weight learning module. The model is first initialized with labelled data. Label processing, including data perturbation, confidence label voting and unconfident label detection for annotation, enables the model to train on labelled and unlabeled data simultaneously. The confidence of each model gets improved through the other two views of the feature learning. This process is repeated until each model reaches the same confidence level as its counterparts. This strategy enables triple-view learning of generic medical image datasets. Bespoke overlap-based and boundary-based loss functions are tailored to the different stages of the training. The segmentation results are evaluated on four publicly available benchmark datasets including Ultrasound, CT, MRI, and Histology images. Repeated experiments demonstrate the effectiveness of the proposed network compared against other semi-supervised algorithms, across a large set of evaluation measures.},
booktitle = {Resource-Efficient Medical Image Analysis: First MICCAI Workshop, REMIA 2022, Singapore, September 22, 2022, Proceedings},
pages = {42–54},
numpages = {13},
location = {Singapore, Singapore}
}

@inproceedings{10.1007/978-3-031-16852-9_4,
author = {Raipuria, Geetank and Shrivastava, Anu and Singhal, Nitin},
title = {Stain-AgLr: Stain Agnostic Learning for&nbsp;Computational Histopathology Using Domain Consistency and&nbsp;Stain Regeneration Loss},
year = {2022},
isbn = {978-3-031-16851-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16852-9_4},
doi = {10.1007/978-3-031-16852-9_4},
abstract = {Stain color variations between Whole Slide Images (WSIs) is a key challenge in the application of Computational Histopathology. Deep learning-based algorithms are susceptible to domain shift and degrade in performance on the WSIs captured from a different source than the training data due to stain color variations. We propose a training methodology Stain-AgLr, that achieves high invariance to stain color changes on unseen test data. In addition to task loss, Stain-AgLr training is supervised with a consistency regularization loss that enforces consistent predictions for training samples and their stain altered versions. An additional decoder is used to regenerate stain color from feature representation of the stain altered images. We compare the proposed approach to state-of-the-art strategies using two histopathology datasets and show significant improvement in model performance on unseen stain variations. We also visualize the feature space distribution of test samples from multiple diagnostic labs and show that Stain-AgLr achieves a significant overlap between the distributions.},
booktitle = {Domain Adaptation and Representation Transfer: 4th MICCAI Workshop, DART 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings},
pages = {33–44},
numpages = {12},
keywords = {Stain invariance, Domain generalization, Histopathology},
location = {<conf-loc content-type="InPerson">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16760-7_11,
author = {Zhang, Bodong and Knudsen, Beatrice and Sirohi, Deepika and Ferrero, Alessandro and Tasdizen, Tolga},
title = {Stain Based Contrastive Co-training for&nbsp;Histopathological Image Analysis},
year = {2022},
isbn = {978-3-031-16759-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16760-7_11},
doi = {10.1007/978-3-031-16760-7_11},
abstract = {We propose a novel semi-supervised learning approach for classification of histopathology images. We employ strong supervision with patch-level annotations combined with a novel co-training loss to create a semi-supervised learning framework. Co-training relies on multiple conditionally independent and sufficient views of the data. We separate the hematoxylin and eosin channels in pathology images using color deconvolution to create two views of each slide that can partially fulfill these requirements. Two separate CNNs are used to embed the two views into a joint feature space. We use a contrastive loss between the views in this feature space to implement co-training. We evaluate our approach in clear cell renal cell and prostate carcinomas, and demonstrate improvement over state-of-the-art semi-supervised learning methods.},
booktitle = {Medical Image Learning with Limited and Noisy Data: First International Workshop, MILLanD 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings},
pages = {106–116},
numpages = {11},
keywords = {Co-training, Semi-supervised learning, Histopathology},
location = {Singapore, Singapore}
}

@inproceedings{10.1007/978-3-031-21083-9_10,
author = {Sims, Joe and Grabsch, Heike I. and Magee, Derek},
title = {Using Hierarchically Connected Nodes and Multiple GNN Message Passing Steps to Increase the Contextual Information in Cell-Graph Classification},
year = {2022},
isbn = {978-3-031-21082-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-21083-9_10},
doi = {10.1007/978-3-031-21083-9_10},
abstract = {Graphs are useful in analysing histopathological images as they are able to represent neighbourhood interactions and spatial relationships. Typically graph nodes represent cells and the vertices are constructed by applying a nearest neighbor algorithm to cell’s locations. When passing these graphs through one graph neural network (GNN) message passing step, each node can only utilise features from nodes within its immediate neighbourhood to make a classification. To overcome this, we introduce two levels of hierarchically connected nodes that we term “supernodes”. These supernodes, used in conjunction with at least four GNN message passing steps, allow for cell node classifications to be influenced by a wider area, enabling the entire graph to learn tissue-level structures. The method is evaluated on a supervised task to classify individual cells as belonging to a specific tissue class. Results demonstrate that the inclusion of supernodes with multiple GNN message passing steps increases model accuracy.},
booktitle = {Imaging Systems for GI Endoscopy, and Graphs in Biomedical Image Analysis: First MICCAI Workshop, ISGIE 2022, and Fourth MICCAI Workshop, GRAIL 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings},
pages = {99–107},
numpages = {9},
keywords = {Digital pathology, Node classification, Graph neural network},
location = {Singapore, Singapore}
}

@inproceedings{10.1007/978-3-031-17266-3_8,
author = {Pan, Xiaoxi and Zhang, Hanyun and Grapa, Anca-Ioana and AbdulJabbar, Khalid and Raza, Shan E Ahmed and Cheung, Ho Kwan Alvin and Karasaki, Takahiro and Quesne, John Le and Moore, David A. and Swanton, Charles and Yuan, Yinyin},
title = {Cross-Stream Interactions: Segmentation of&nbsp;Lung Adenocarcinoma Growth Patterns},
year = {2022},
isbn = {978-3-031-17265-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-17266-3_8},
doi = {10.1007/978-3-031-17266-3_8},
abstract = {Lung adenocarcinoma has histologically distinct growth patterns that have been associated with patient prognosis. Precision segmentation of growth patterns in routine histology samples is challenging due to the complexity of patterns and high intra-class variability. In this paper, we present a novel model with a multi-stream architecture, Cross-Stream Interactions (CroSIn), which fully considers crucial interactions across scales to gather abundant information. The first-order attention introduces contextual information at an early stage to guide low-level feature encoding. The second-order attention then focuses on learning high-level feature relations among scales to extract discriminative features. Experimental results show interactions at both low- and high-level feature learning stages are crucial in performance improvement. The proposed method outperforms state-of-the-art networks, achieving an average Dice of 60.34% at patch level, and an average accuracy of 65.31% at sample level, which is also verified in an independent cohort.},
booktitle = {Computational Mathematics Modeling in Cancer Analysis: First International Workshop, CMMCA 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings},
pages = {78–90},
numpages = {13},
keywords = {Semantic segmentation, Growth patterns, Histology},
location = {<conf-loc content-type="InPerson">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16449-1_68,
author = {Li, Canran and Liu, Dongnan and Li, Haoran and Zhang, Zheng and Lu, Guangming and Chang, Xiaojun and Cai, Weidong},
title = {Domain Adaptive Nuclei Instance Segmentation and&nbsp;Classification via&nbsp;Category-Aware Feature Alignment and&nbsp;Pseudo-Labelling},
year = {2022},
isbn = {978-3-031-16448-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16449-1_68},
doi = {10.1007/978-3-031-16449-1_68},
abstract = {Unsupervised domain adaptation (UDA) methods have been broadly utilized to improve the models’ adaptation ability in general computer vision. However, different from the natural images, there exist huge semantic gaps for the nuclei from different categories in histopathology images. It is still under-explored how could we build generalized UDA models for precise segmentation or classification of nuclei instances across different datasets. In this work, we propose a novel deep neural network, namely Category-Aware feature alignment and Pseudo-Labelling Network (CAPL-Net) for UDA nuclei instance segmentation and classification. Specifically, we first propose a category-level feature alignment module with dynamic learnable trade-off weights. Second, we propose to facilitate the model performance on the target data via self-supervised training with pseudo labels based on nuclei-level prototype features. Comprehensive experiments on cross-domain nuclei instance segmentation and classification tasks demonstrate that our approach outperforms state-of-the-art UDA methods with a remarkable margin.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part VII},
pages = {715–724},
numpages = {10},
keywords = {Computational pathology, Nuclear segmentation, Nuclear classification, Unsupervised domain adaption, Deep learning},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16446-0_32,
author = {Tian, Xuanyu and Wu, Qing and Wei, Hongjiang and Zhang, Yuyao},
title = {Noise2SR: Learning to&nbsp;Denoise from&nbsp;Super-Resolved Single Noisy Fluorescence Image},
year = {2022},
isbn = {978-3-031-16445-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16446-0_32},
doi = {10.1007/978-3-031-16446-0_32},
abstract = {Fluorescence microscopy is a key driver to promote discoveries of biomedical research. However, with the limitation of microscope hardware and characteristics of the observed samples, the fluorescence microscopy images are susceptible to noise. Recently, a few self-supervised deep learning (DL) denoising methods have been proposed. However, the training efficiency and denoising performance of existing methods are relatively low in real scene noise removal. To address this issue, this paper proposed self-supervised image denoising method Noise2SR (N2SR) to train a simple and effective image denoising model based on single noisy observation. Our Noise2SR denoising model is designed for training with paired noisy images of different dimensions. Benefiting from this training strategy, Noise2SR is more efficiently self-supervised and able to restore more image details from a single noisy observation. Experimental results of simulated noise and real microscopy noise removal show that Noise2SR outperforms two blind-spot based self-supervised deep learning image denoising methods. We envision that Noise2SR has the potential to improve more other kind of scientific imaging quality.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part VI},
pages = {334–343},
numpages = {10},
keywords = {Image denoising, Self-supervised learning, Fluorescence microscopy image},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16437-8_71,
author = {Salehi, Raheleh and Sadafi, Ario and Gruber, Armin and Lienemann, Peter and Navab, Nassir and Albarqouni, Shadi and Marr, Carsten},
title = {Unsupervised Cross-Domain Feature Extraction for&nbsp;Single Blood Cell Image Classification},
year = {2022},
isbn = {978-3-031-16436-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16437-8_71},
doi = {10.1007/978-3-031-16437-8_71},
abstract = {Diagnosing hematological malignancies requires identification and classification of white blood cells in peripheral blood smears. Domain shifts caused by different lab procedures, staining, illumination, and microscope settings hamper the re-usability of recently developed machine learning methods on data collected from different sites. Here, we propose a cross-domain adapted autoencoder to extract features in an unsupervised manner on three different datasets of single white blood cells scanned from peripheral blood smears. The autoencoder is based on an R-CNN architecture allowing it to focus on the relevant white blood cell and eliminate artifacts in the image. To evaluate the quality of the extracted features we use a simple random forest to classify single cells. We show that thanks to the rich features extracted by the autoencoder trained on only one of the datasets, the random forest classifier performs satisfactorily on the unseen datasets, and outperforms published oracle networks in the cross-domain task. Our results suggest the possibility of employing this unsupervised approach in more complicated diagnosis and prognosis tasks without the need to add expensive expert labels to unseen data.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part III},
pages = {739–748},
numpages = {10},
keywords = {Unsupervised learning, Feature extraction, Autoencoders, Single cell classification, Microscopy, Domain adaptation},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16434-7_39,
author = {Daroach, Gagandeep B. and Duenweg, Savannah R. and Brehler, Michael and Lowman, Allison K. and Iczkowski, Kenneth A. and Jacobsohn, Kenneth M. and Yoder, Josiah A. and LaViolette, Peter S.},
title = {Prostate Cancer Histology Synthesis Using StyleGAN Latent Space Annotation},
year = {2022},
isbn = {978-3-031-16433-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16434-7_39},
doi = {10.1007/978-3-031-16434-7_39},
abstract = {The latent space of a generative adversarial network (GAN) may model pathologically-significant semantics with unsupervised learning. To explore this phenomenon, we trained and tested a StyleGAN2 on a high quality prostate histology dataset covering the prostate cancer (PCa) diagnostic spectrum. Our pathologist annotated synthetic images to identify learned PCa regions in the GAN latent space. New points were drawn from these regions, synthesized into images, and given to a pathologist for annotation. 77% of the new points received the same annotation, and 98% of the latent points received the same or adjacent diagnostic stage annotation. This confirms the GAN network can accurately disentangle and model PCa features without exposure to labels in the training process.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part II},
pages = {398–408},
numpages = {11},
keywords = {Generative adversarial networks, Prostate cancer, Histology, Latent space, Unsupervised deep learning},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16434-7_3,
author = {Qu, Linhao and Luo, Xiaoyuan and Liu, Shaolei and Wang, Manning and Song, Zhijian},
title = {DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification},
year = {2022},
isbn = {978-3-031-16433-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16434-7_3},
doi = {10.1007/978-3-031-16434-7_3},
abstract = {Multiple Instance Learning (MIL) is widely used in analyzing histopathological Whole Slide Images (WSIs). However, existing MIL methods do not explicitly model the data distribution, and instead they only learn a bag-level or instance-level decision boundary discriminatively by training a classifier. In this paper, we propose DGMIL: a feature distribution guided deep MIL framework for WSI classification and positive patch localization. Instead of designing complex discriminative network architectures, we reveal that the inherent feature distribution of histopathological image data can serve as a very effective guide for instance classification. We propose a cluster-conditioned feature distribution modeling method and a pseudo label-based iterative feature space refinement strategy so that in the final feature space the positive and negative instances can be easily separated. Experiments on the CAMELYON16 dataset and the TCGA Lung Cancer dataset show that our method achieves new SOTA for both global classification and positive patch localization tasks.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part II},
pages = {24–34},
numpages = {11},
keywords = {Histopathological images, Multiple Instance Learning},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16434-7_19,
author = {Zhang, Jingwei and Zhang, Xin and Ma, Ke and Gupta, Rajarsi and Saltz, Joel and Vakalopoulou, Maria and Samaras, Dimitris},
title = {Gigapixel Whole-Slide Images Classification Using Locally Supervised Learning},
year = {2022},
isbn = {978-3-031-16433-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16434-7_19},
doi = {10.1007/978-3-031-16434-7_19},
abstract = {Histopathology whole slide images (WSIs) play a very important role in clinical studies and serve as the gold standard for many cancer diagnoses. However, generating automatic tools for processing WSIs is challenging due to their enormous sizes. Currently, to deal with this issue, conventional methods rely on a multiple instance learning (MIL) strategy to process a WSI at patch level. Although effective, such methods are computationally expensive, because tiling a WSI into patches takes time and does not explore the spatial relations between these tiles. To tackle these limitations, we propose a locally supervised learning framework which processes the entire slide by exploring the entire local and global information that it contains. This framework divides a pre-trained network into several modules and optimizes each module locally using an auxiliary model. We also introduce a random feature reconstruction unit (RFR) to preserve distinguishing features during training and improve the performance of our method by 1% to 3%. Extensive experiments on three publicly available WSI datasets: TCGA-NSCLC, TCGA-RCC and LKS, highlight the superiority of our method on different classification tasks. Our method outperforms the state-of-the-art MIL methods by 2% to 5% in accuracy, while being 7 to 10 times faster. Additionally, when dividing it into eight modules, our method requires as little as 20% of the total gpu memory required by end-to-end training. Our code is available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part II},
pages = {192–201},
numpages = {10},
keywords = {Locally supervised learning, Whole slide image, Multiple instance learning, Classification},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16434-7_13,
author = {Hu, Yang and Sirinukunwattana, Korsuk and Gaitskell, Kezia and Wood, Ruby and Verrill, Clare and Rittscher, Jens},
title = {Predicting Molecular Traits from&nbsp;Tissue Morphology Through Self-interactive Multi-instance Learning},
year = {2022},
isbn = {978-3-031-16433-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16434-7_13},
doi = {10.1007/978-3-031-16434-7_13},
abstract = {Previous efforts to learn histology features that correlate with specific genetic/molecular traits resort to tile-level multi-instance learning (MIL) which relies on a fixed pretrained model for feature extraction and an instance-bag classifier. We argue that such a two-step approach is not optimal at capturing both fine-grained features at tile level and global features at slide level optimal to the task. We propose a self-interactive MIL that iteratively feedbacks training information between the fine-grained and global context features. We validate the proposed approach on 4 subtyping tasks: EMT status (ovarian), KRAS mutation (colon and lung), EGFR mutation (colon), and HER2 status (breast). Our approach yields an average improvement of 7.05%-8.34% (in terms of AUC) over the baseline.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part II},
pages = {130–139},
numpages = {10},
keywords = {Histopathological subtyping, Genetic mutation detection, Limited data, Self-interactive learning, Fine-grained features},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16434-7_10,
author = {Tran, Manuel and Wagner, Sophia J. and Boxberg, Melanie and Peng, Tingying},
title = {S5CL: Unifying Fully-Supervised, Self-supervised, and&nbsp;Semi-supervised Learning Through Hierarchical Contrastive Learning},
year = {2022},
isbn = {978-3-031-16433-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16434-7_10},
doi = {10.1007/978-3-031-16434-7_10},
abstract = {In computational pathology, we often face a scarcity of annotations and a large amount of unlabeled data. One method for dealing with this is semi-supervised learning which is commonly split into a self-supervised pretext task and a subsequent model fine-tuning. Here, we compress this two-stage training into one by introducing S5CL, a unified framework for fully-supervised, self-supervised, and semi-supervised learning. With three contrastive losses defined for labeled, unlabeled, and pseudo-labeled images, S5CL can learn feature representations that reflect the hierarchy of distance relationships: similar images and augmentations are embedded the closest, followed by different looking images of the same class, while images from separate classes have the largest distance. Moreover, S5CL allows us to flexibly combine these losses to adapt to different scenarios. Evaluations of our framework on two public histopathological datasets show strong improvements in the case of sparse labels: for a H &amp;E-stained colorectal cancer dataset, the accuracy increases by up to 9% compared to supervised cross-entropy loss; for a highly imbalanced dataset of single white blood cells from leukemia patient blood smears, the F1-score increases by up to 6% (Code: ).},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part II},
pages = {99–108},
numpages = {10},
keywords = {Contrastive learning, Self-supervision, Semi-supervision},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1007/978-3-031-16434-7_1,
author = {Jin, Qiangguo and Cui, Hui and Sun, Changming and Zheng, Jiangbin and Wei, Leyi and Fang, Zhenyu and Meng, Zhaopeng and Su, Ran},
title = {Semi-supervised Histological Image Segmentation via Hierarchical Consistency Enforcement},
year = {2022},
isbn = {978-3-031-16433-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16434-7_1},
doi = {10.1007/978-3-031-16434-7_1},
abstract = {Acquiring pixel-level annotations for histological image segmentation is time- and labor- consuming. Semi-supervised learning enables learning from the unlabeled and limited amount of labeled data. A challenging issue is the inconsistent and uncertain predictions on unlabeled data. To enforce invariant predictions over the perturbations applied to the hidden feature space, we propose a Mean-Teacher based hierarchical consistency enforcement (HCE) framework and a novel hierarchical consistency loss (HC-loss) with learnable and self-guided mechanisms. Specifically, the HCE takes the perturbed versions of the hierarchical features from the encoder as input to the auxiliary decoders, and encourages the predictions of the auxiliary decoders and the main decoder to be consistent. The HC-loss facilitates the teacher model to generate reliable guidance and enhances the consistency among all the decoders of the student model. The proposed method is simple, yet effective, which can easily be extended to other frameworks. The quantitative and qualitative experimental results indicate the effectiveness of the hierarchical consistency enforcement on the MoNuSeg and CRAG datasets.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2022: 25th International Conference, Singapore, September 18–22, 2022, Proceedings, Part II},
pages = {3–13},
numpages = {11},
keywords = {Hierarchical consistency enforcement, Semi-supervised learning, Histological image segmentation},
location = {<conf-loc content-type="Hybrid">Singapore, Singapore</conf-loc>}
}

@inproceedings{10.1145/3546607.3546617,
author = {Zhang, Zehua and Liu, Bailing and Zhou, Gaohao},
title = {Semantic Segmentation Model of Fluorescent Neuronal Cells in Mouse Brain Slices Under Few Samples.},
year = {2022},
isbn = {9781450387330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546607.3546617},
doi = {10.1145/3546607.3546617},
abstract = {Image of mouse neuronal cells is an important tool for studying mice. Examining mouse neuronal cells is an essential step in both pharmacological and toxicological tests. The morphology of individual neurons and the state of connections between multiple neurons are essential measures of the physiological state of the mouse. Staining neuronal cells samples and observation through microscopy is the mainstay of this field. However, this step is tedious, monotonous, and requires a high level of practical experience from the researcher. In recent years, the analysis of cell morphology using computer vision has proven to be an efficient and accurate solution. This paper presents a deep neural network-based semantic segmentation model; it does not use the popular attention mechanism but breaks down the process into two steps to achieve satisfactory performance while maintaining the number of parameters at a low level.},
booktitle = {Proceedings of the 6th International Conference on Virtual and Augmented Reality Simulations},
pages = {64–70},
numpages = {7},
keywords = {Sematic Segmentation, Medical Image Process, Deep Neural Network},
location = {Brisbane, QLD, Australia},
series = {ICVARS '22}
}

@inproceedings{10.1145/3534678.3539313,
author = {Fu, Dongqi and Fang, Liri and Maciejewski, Ross and Torvik, Vetle I. and He, Jingrui},
title = {Meta-Learned Metrics over Multi-Evolution Temporal Graphs},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539313},
doi = {10.1145/3534678.3539313},
abstract = {Graph metric learning methods aim to learn the distance metric over graphs such that similar (e.g., same class) graphs are closer and dissimilar (e.g., different class) graphs are farther apart. This is of critical importance in many graph classification applications such as drug discovery and epidemics categorization. Most, if not all, graph metric learning techniques consider the input graph as static, and largely ignore the intrinsic dynamics of temporal graphs. However, in practice, a graph typically has heterogeneous dynamics (e.g., microscopic and macroscopic evolution patterns). As such, labeling a temporal graph is usually expensive and also requires background knowledge. To learn a good metric over temporal graphs, we propose a temporal graph metric learning framework, Temp-GFSM. With only a few labeled temporal graphs, Temp-GFSM outputs a good metric that can accurately classify different temporal graphs and be adapted to discover new subspaces for unseen classes. Each proposed component in Temp-GFSM answers the following questions: What patterns are evolving in a temporal graph? How to weigh these patterns to represent the characteristics of different temporal classes? And how to learn the metric with the guidance from only a few labels? Finally, the experimental results on real-world temporal graph classification tasks from various domains show the effectiveness of our Temp-GFSM.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {367–377},
numpages = {11},
keywords = {temporal graph classification, metric learning, meta-learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1007/978-3-031-13841-6_50,
author = {Yu, Minming and Lei, Yanjing and Shi, Wenyan and Xu, Yujie and Chan, Sixian},
title = {An Improved YOLOX for&nbsp;Detection in&nbsp;Urine Sediment Images},
year = {2022},
isbn = {978-3-031-13840-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-13841-6_50},
doi = {10.1007/978-3-031-13841-6_50},
abstract = {In clinical medicine, the detection of human urine sediment is usually a basic test. The indicators of test can effectively analyze whether the patient has the disease. The traditional method of chemical, physical and microscopic analysis of samples artificially is time-consuming and inefficient. With the development of deep learning technology, many detectors now can replace traditional manual work and play a good detection effect. So it is of great value to put deep learning technology into the medical field. Usually, the Urine Microscopic Image has challenges for research, in which many detectors can not detect the cells well due to their small scale and heavy overlap occlusion. Therefore, we propose a novel detector for the detection of urine sediment in this paper. Firstly, considering the friendliness of YOLOX to the small objects, we adopt the framework from the YOLOX. Secondly, we add spatial, channel and position attention to enhance the feature information to achieve more accurate detection results. Then, the better Giouloss is also applied to make a better regression of the bounding box. Finally, the experimental results show that our improved model based on YOLOX achieves 44.5%AP50-90 and 80.1%AP50 on the public dataset Urine Microscopic Image Dataset, which is far better than other detectors.},
booktitle = {Intelligent Robotics and Applications: 15th International Conference, ICIRA 2022, Harbin, China, August 1–3, 2022, Proceedings, Part IV},
pages = {556–567},
numpages = {12},
keywords = {YOLOX, Urine sediment, Object detection, Deep learning},
location = {Harbin, China}
}

@inproceedings{10.1007/978-3-031-13841-6_19,
author = {Li, Yilong and Wang, Yaqi and Zhou, Huiyu and Wang, Huaqiong and Jia, Gangyong and Zhang, Qianni},
title = {DU-Net Based Unsupervised Contrastive Learning for&nbsp;Cancer Segmentation in&nbsp;Histology Images},
year = {2022},
isbn = {978-3-031-13840-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-13841-6_19},
doi = {10.1007/978-3-031-13841-6_19},
abstract = {In this paper, we introduce an unsupervised cancer segmentation framework for histology images. The framework involves an effective contrastive learning scheme for extracting distinctive visual representations for segmentation. The encoder is a Deep U-Net (DU-Net) structure which contains an extra fully convolution layer compared to the normal U-Net. A contrastive learning scheme is developed to solve the problem of lacking training sets with high-quality annotations on tumour boundaries. A specific set of data augmentation techniques are employed to improve the discriminability of the learned colour features from contrastive learning. Smoothing and noise elimination are conducted using convolutional Conditional Random Fields. The experiments demonstrate competitive performance in segmentation even better than some popular supervised networks.},
booktitle = {Intelligent Robotics and Applications: 15th International Conference, ICIRA 2022, Harbin, China, August 1–3, 2022, Proceedings, Part IV},
pages = {201–210},
numpages = {10},
keywords = {Tumour segmentation, Data augmentation, Contrastive learning, Unsupervised},
location = {Harbin, China}
}

@inproceedings{10.1007/978-3-031-12053-4_32,
author = {Bashir, Raja Muhammad Saad and Shaban, Muhammad and Raza, Shan E. Ahmed and Khurram, Syed Ali and Rajpoot, Nasir},
title = {A Novel Framework for&nbsp;Coarse-Grained Semantic Segmentation of&nbsp;Whole-Slide Images},
year = {2022},
isbn = {978-3-031-12052-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-12053-4_32},
doi = {10.1007/978-3-031-12053-4_32},
abstract = {Semantic segmentation of multi-gigapixel whole-slide images (WSI) is fundamental to computational pathology, as segmentation of different tissue types and layers is a prerequisite for several downstream histology image analysis, such as morphometric analysis, cancer grading, and survival. Both patch-based classification and pixel-wise segmentation have been used for these tasks, where patch-based classification outputs only one label per patch while pixel-wise segmentation is more accurate and precise but it requires a large number of pixel-wise precise annotated ground truth. In this paper, we propose coarse segmentation as a new middle ground to both techniques for leveraging more context without requiring pixel-level annotations. Our proposed coarse segmentation network is a convolutional neural network (CNN) with skip connections but does not contain any decoder and utilizes sparsely annotated images during training. It takes an input patch of size  and outputs a dense prediction map of size , which is coarser than pixel-wise segmentation methods but denser than patch-based classification methods. We compare our proposed method with its counterparts and demonstrate its superior performance for both pixel-based segmentation and patch-based classification tasks. In addition, we also compared the impact on performance of coarse-grained and pixel-wise semantic segmentation in downstream analysis tasks and showed coarse-grained semantic segmentation has no/marginal impact on the final results.},
booktitle = {Medical Image Understanding and Analysis: 26th Annual Conference, MIUA 2022, Cambridge, UK, July 27–29, 2022, Proceedings},
pages = {425–439},
numpages = {15},
location = {<conf-loc content-type="InPerson">Cambridge, United Kingdom</conf-loc>}
}

@inproceedings{10.1007/978-3-031-12053-4_16,
author = {Wagner, Royden and Rohr, Karl},
title = {CellCentroidFormer: Combining Self-attention and&nbsp;Convolution for&nbsp;Cell Detection},
year = {2022},
isbn = {978-3-031-12052-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-12053-4_16},
doi = {10.1007/978-3-031-12053-4_16},
abstract = {Cell detection in microscopy images is important to study how cells move and interact with their environment. Most recent deep learning-based methods for cell detection use convolutional neural networks (CNNs). However, inspired by the success in other computer vision applications, vision transformers (ViTs) are also used for this purpose. We propose a novel hybrid CNN-ViT model for cell detection in microscopy images to exploit the advantages of both types of deep learning models. We employ an efficient CNN, that was pre-trained on the ImageNet dataset, to extract image features and utilize transfer learning to reduce the amount of required training data. Extracted image features are further processed by a combination of convolutional and transformer layers, so that the convolutional layers can focus on local information and the transformer layers on global information. Our centroid-based cell detection method represents cells as ellipses and is end-to-end trainable. Furthermore, we show that our proposed model can outperform fully convolutional one-stage detectors on four different 2D microscopy datasets. Code is available at:},
booktitle = {Medical Image Understanding and Analysis: 26th Annual Conference, MIUA 2022, Cambridge, UK, July 27–29, 2022, Proceedings},
pages = {212–222},
numpages = {11},
keywords = {Cell detection, Transformer, Self-attention, Convolution},
location = {<conf-loc content-type="InPerson">Cambridge, United Kingdom</conf-loc>}
}

@inproceedings{10.1007/978-3-031-12053-4_11,
author = {Khalid, Nabeel and Schmeisser, Fabian and Koochali, Mohammadmahdi and Munir, Mohsin and Edlund, Christoffer and Jackson, Timothy R and Trygg, Johan and Sj\"{o}gren, Rickard and Dengel, Andreas and Ahmed, Sheraz},
title = {Point2Mask: A Weakly Supervised Approach for&nbsp;Cell Segmentation Using Point Annotation},
year = {2022},
isbn = {978-3-031-12052-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-12053-4_11},
doi = {10.1007/978-3-031-12053-4_11},
abstract = {Identifying cells in microscopic images is a crucial step toward studying image-based cell biology research. Cell instance segmentation provides an opportunity to study the shape, structure, form, and size of cells. Deep learning approaches for cell instance segmentation rely on the instance segmentation mask for each cell, which is a labor-intensive and expensive task. An ample amount of unlabeled microscopic data is available in the cell biology domain, but due to the tedious and exorbitant nature of the annotations needed for the cell instance segmentation approaches, the full potential of the data is not explored. This paper presents a weakly supervised approach, which can perform cell instance segmentation by using only point and bounding box-based annotation. This enormously reduces the annotation efforts. The proposed approach is evaluated on a benchmark dataset i.e., LIVECell, whereby only using a bounding box and randomly generated points on each cell, it achieved the mean average precision score of 43.53% which is as good as the full supervised segmentation method trained with complete segmentation mask. In addition, it is 3.71 times faster to annotate with a bounding box and point in comparison to full mask annotation.},
booktitle = {Medical Image Understanding and Analysis: 26th Annual Conference, MIUA 2022, Cambridge, UK, July 27–29, 2022, Proceedings},
pages = {139–153},
numpages = {15},
keywords = {Weakly supervised, Cell segmentation, Point annotation, Deep learning},
location = {<conf-loc content-type="InPerson">Cambridge, United Kingdom</conf-loc>}
}

@inproceedings{10.1007/978-3-031-12053-4_10,
author = {Liu, Lihao and Hong, Chenyang and Aviles-Rivero, Angelica I. and Sch\"{o}nlieb, Carola-Bibiane},
title = {Simultaneous Semantic and&nbsp;Instance Segmentation for&nbsp;Colon Nuclei Identification and&nbsp;Counting},
year = {2022},
isbn = {978-3-031-12052-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-12053-4_10},
doi = {10.1007/978-3-031-12053-4_10},
abstract = {Nucleus segmentation and classification within the Haematoxylin and Eosin stained histology images is a key component in computer-aided image analysis, which helps to extract features with rich information for cellular estimation and following diagnosis. Therefore, it is of great relevance for several downstream computational pathology applications In this work, we address the problem of automatic nuclear segmentation and classification. Our solution is to cast as a simultaneous semantic and instance segmentation framework, and it is part of the Colon Nuclei Identification and Counting (CoNIC) Challenge. Our framework is a carefully designed ensemble model. We first train a semantic and an instance segmentation model separately, where we use as backbone HoverNet and Cascade Mask-RCNN models. We then ensemble the results with a customized Non-Maximum Suppression embedding algorithm. From our experiments, we observe that the semantic segmentation part can achieve an accurate class prediction for the cells whilst the instance information provides a refined segmentation. We enforce a robust segmentation and classification result through our customized embedding algorithm. We demonstrate, through our visual and numerical experimental, that our model outperforms the provided baselines by a large margin. Our solution ranked as the  solution on the Grand Challenge CoNIC&nbsp;2022.},
booktitle = {Medical Image Understanding and Analysis: 26th Annual Conference, MIUA 2022, Cambridge, UK, July 27–29, 2022, Proceedings},
pages = {130–138},
numpages = {9},
keywords = {Semantic segmentation, Instance segmentation, Histology images, Colon nuclei identification and counting},
location = {<conf-loc content-type="InPerson">Cambridge, United Kingdom</conf-loc>}
}

@inproceedings{10.1145/3538950.3538966,
author = {Wu, Chunzhi and Xue, Xiaofei and Song, Yongtao},
title = {Research on Cancer Diagnosis Method Based on LightGBM-Gridsearchcv},
year = {2022},
isbn = {9781450395632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538950.3538966},
doi = {10.1145/3538950.3538966},
abstract = {Cancer has become a non-negligible problem that threatens human health in today's society. The traditional methods of cancer diagnosis usually use cell morphology, histopathology and other methods. Nowadays, the use of machine learning technology to predict cancer has become a new actionable way. This paper proposes the use of machine learning algorithms to assist breast cancer diagnosis, using a variety of algorithms such as LightGBM, Random Forests (RF), Support Vector Machines (SVM), Linear SVM, K-Nearest Neighbor (KNN), and combined with grid search algorithms, respectively constructed intelligent predictive and diagnostic models for malignant breast cancer. Finally, using the breast cancer data set of the University of Wisconsin (WCBD) hospital to conduct experiments, a classification model based on LightGBM-Gridsearchcv is proposed. Compared with the models, the LightGBM-Gridsearchcv model has a recognition accuracy of 95.9% for malignant cancer cases. The machine learning method has put forward a new research idea and method for the diagnosis of breast cancer, and provided a research direction for the promotion of intelligent medical treatment, which has very important practical significance and application value.},
booktitle = {Proceedings of the 4th International Conference on Big Data Engineering},
pages = {122–126},
numpages = {5},
keywords = {Machine learning, LightGBM, Classification, Cancer},
location = {Beijing, China},
series = {BDE '22}
}

@inproceedings{10.1145/3535782.3535828,
author = {Muttamara, Apiwat and Nakwong, Patittar},
title = {Effect of Electrical Discharge Machining on Surface Characteristics and Microstructure of Aluminum Alloy 2024},
year = {2022},
isbn = {9781450395816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535782.3535828},
doi = {10.1145/3535782.3535828},
abstract = {Electrical Discharge Machine (EDM) is a machine used to remove material from a work piece by a series of rapidly recurring current discharges between two electrodes, separated by a dielectric liquid and subject to an electric voltage. This research has been conducted on Aluminum alloy 2024. Characteristics of electric discharge machined (EDM) surfaces in kerosene were investigated. Optical microscopy, scanning electron microscopy (SEM) were employed to analyze the machined surfaces. Surface cracks are examined in terms of discharge current. The present results reveal that base material properties and white layer composition have a distinctive effect on crack formation that results in different crack network layouts on the surface. Surface cracks initiate at the surface. Such cracks are usually encountered when machining is performed using high pulse-on duration and low average discharge current. Cracks are mainly formed due to contraction of the recast structure joined to the circumferential edge of a crater rim during solidification. Crack density was found to be proportional to the used discharge current used.},
booktitle = {Proceedings of the 4th International Conference on Management Science and Industrial Engineering},
pages = {350–357},
numpages = {8},
keywords = {Crack, EDM, Microstructure, Surface},
location = {<conf-loc>, <city>Chiang Mai</city>, <country>Thailand</country>, </conf-loc>},
series = {MSIE '22}
}

@inproceedings{10.1145/3535694.3535729,
author = {Sun, Xuenan and Zhang, Jiajing and Sheng, Dan and Zhang, Chunhua},
title = {Surface Modification of Polyimide Fibers and The Bending Property of Polyimide Fiber/Polyimide Composite},
year = {2022},
isbn = {9781450395779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535694.3535729},
doi = {10.1145/3535694.3535729},
abstract = {Alkali/DMAc was used to modify the surface of high-performance polyimide (PI) fibers. The surface chemical composition, morphology, roughness, and thermal properties of the modified PI fabric were characterized using Fourier transform infrared spectroscopy (FTIR), scanning electron microscopy (SEM), atomic force microscopy (AFM), and thermogravimetric analysis (TGA). These results suggest that the surface roughness was improved. The modified PI retained its original chemical structure and exhibited good thermal stability. Meanwhile, the wettability of the PI fabric was increased, which was conducive to forming a good interface between the fiber and matrix. Compared with the pristine state, the bending property of the modified PI composite was obviously increased.},
booktitle = {Proceedings of the 12th International Conference on Biomedical Engineering and Technology},
pages = {210–213},
numpages = {4},
keywords = {surface modification, composite, bending property, Polyimide fiber},
location = {Tokyo, Japan},
series = {ICBET '22}
}

@inproceedings{10.1007/978-3-031-11203-4_25,
author = {Goyal, Neha and Hussain, Yahiya and Yang, Gianna G. and Haehn, Daniel},
title = {Real-Time Alignment for&nbsp;Connectomics},
year = {2022},
isbn = {978-3-031-11202-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-11203-4_25},
doi = {10.1007/978-3-031-11203-4_25},
abstract = {In Connectomics, researchers are creating the brain’s wiring diagram at nanometer resolution. As part of this processing workflow, 2D electron microscopy (EM) images must be aligned to 3D volumes. However, existing alignment methods are computationally expensive and can take a long time. We hypothesize that adding biological features improve and accelerate the alignment procedure. Since especially mitochondria can be detected accurately and fast, we propose a new alignment method, MITO, that uses these structures as landmark points. With MITO, we can decrease the alignment time by 27%, and our experiments indicate a throughput of 33 Megapixels/s, which is faster than the acquisition speed of current microscopes. We can align an image volume of 1268\texttimes{}1524\texttimes{}160 voxels in less than 12&nbsp;s. We compare our method to the following feature generators: ORB, BRISK, FAST, and FREAK.},
booktitle = {Biomedical Image Registration: 10th International Workshop, WBIR 2022, Munich, Germany, July 10–12, 2022, Proceedings},
pages = {211–214},
numpages = {4},
keywords = {Image alignment, Registration, Feature matching},
location = {<conf-loc content-type="InPerson">Munich, Germany</conf-loc>}
}

@inproceedings{10.1109/IST55454.2022.9827735,
author = {Tsai, Hsin-Yi and Su, Yin-Ting and Lin, Yu-Hsuan and Huang, Kuo-Cheng and Hsu, Chih-Ning and Liu, Ming-Li},
title = {Enhanced Parasite Egg Images with Multiregion Light Source},
year = {2022},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IST55454.2022.9827735},
doi = {10.1109/IST55454.2022.9827735},
abstract = {Worms are human intestinal parasites; worm infections are often caused by roundworms, tapeworms, whipworms, or hookworms. These pathogens are prevalent in tropical and subtropical regions, and worm infections pose a major threat to public health in developing and underdeveloped countries. Parasitic infections are typically diagnosed through optical microscope observation with the human eye. However, parasite samples are often hazardous because they contain feces or blood smears, and diagnosis requires substantial time and effort from a professional medical examiner. This study proposed a multiquadrant light source and corresponding image processing methods to enhance the optical microscope images of parasite eggs. A light source with nine independently controlled areas was installed in an optical microscope to illuminate a parasite egg sample on glass substrate. Composite images additively constructed from single-axis images had improved characteristics because of the illumination with a suitable light intensity and illumination angle. A center-subtraction method enhanced the inner contours of parasite eggs. The light source and image processing methods could be employed to construct three-dimensional profiles of parasite eggs, enabling rapid screening for parasites in clinical applications.},
booktitle = {2022 IEEE International Conference on Imaging Systems and Techniques (IST)},
pages = {1–5},
numpages = {5},
location = {Kaohsiung, Taiwan}
}

@inproceedings{10.1007/978-3-031-08757-8_3,
author = {Moreno-Barea, Francisco J. and Jerez, Jos\'{e} M. and Franco, Leonardo},
title = {GAN-Based Data Augmentation for&nbsp;Prediction Improvement Using Gene Expression Data in&nbsp;Cancer},
year = {2022},
isbn = {978-3-031-08756-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08757-8_3},
doi = {10.1007/978-3-031-08757-8_3},
abstract = {Within the area of bioinformatics, Deep Learning (DL) models have shown exceptional results in applications in which histological images, scans and tomographies are used. However, when gene expression data is under analysis, the performance is often limited, further hampered by the complexity of these models that require several instances, in the order of thousands, to provide good results. Due to the difficulty and the costs involved in the collection of medical data, the application of Data Augmentation (DA) techniques to alleviate the lack of samples is a topic of great relevance. State-of-the-art models based on Conditional Generative Adversarial Networks (CGAN) and some introduced modifications are used in this work to investigate the effect of DA for prediction of the vital status of patients from RNA-Seq gene expression data. Experimental results on several real-world data sets demonstrate the effectiveness and efficiency of the proposed models. The application of DA methods significantly increase prediction accuracy, leading by 12% with respect to benchmark data sets and 3.15% with respect to data processed with feature selection. Results based on CGAN models outperform in most cases, alternative methods like the SMOTE or noise injection techniques.},
booktitle = {Computational Science – ICCS 2022: 22nd International Conference, London, UK, June 21–23, 2022, Proceedings, Part III},
pages = {28–42},
numpages = {15},
keywords = {CGAN, Deep Learning, Bioinformatics, Gene expression, Data Augmentation},
location = {London, United Kingdom}
}

@inproceedings{10.1007/978-3-031-08751-6_23,
author = {Borowa, Adriana and Kruczek, Szczepan and Tabor, Jacek and Zieliundefinedski, Bartosz},
title = {Weakly-Supervised Cell Classification for&nbsp;Effective High Content Screening},
year = {2022},
isbn = {978-3-031-08750-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08751-6_23},
doi = {10.1007/978-3-031-08751-6_23},
abstract = {High Content Screening (HCS) allows for a complex cell analysis by  combining fluorescent microscopy with the capability to automatically create a large number of images. Example of such cell analysis is examination of cell morphology under influence of a compound. Nevertheless, classical approaches bring the need for manual labeling of cell examples in order to train a machine learning model. Such methods are time- and resource-consuming. To accelerate the analysis of HCS data, we propose a new self-supervised model for cell classification: Self-Supervised Multiple Instance Learning (SSMIL). Our model merges Contrastive Learning with Multiple Instance Learning to analyze images with weak labels. We test SSMIL using our own dataset of microglia cells that present different morphology due to compound-induced inflammation. Representation provided by our model obtains results comparable to supervised methods proving feasibility of the method and opening the path for future experiments using both HCS and other types of medical images.},
booktitle = {Computational Science – ICCS 2022: 22nd International Conference, London, UK, June 21–23, 2022, Proceedings, Part I},
pages = {318–330},
numpages = {13},
keywords = {Multiple Instance Learning, Self-supervised learning, Weakly-supervised learning, High Content Screening},
location = {London, United Kingdom}
}

@inproceedings{10.1007/978-3-031-09342-5_26,
author = {Wei, Jerry and Torresani, Lorenzo and Wei, Jason and Hassanpour, Saeed},
title = {Calibrating Histopathology Image Classifiers Using Label Smoothing},
year = {2022},
isbn = {978-3-031-09341-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-09342-5_26},
doi = {10.1007/978-3-031-09342-5_26},
abstract = {The classification of histopathology images fundamentally differs from traditional image classification tasks because histopathology images naturally exhibit a range of diagnostic features, resulting in a diverse range of annotator agreement levels. However, examples with high annotator disagreement are often either assigned the majority label or discarded entirely when training histopathology image classifiers. This widespread practice often yields classifiers that do not account for example difficulty and exhibit poor model calibration. In this paper, we ask: can we improve model calibration by endowing histopathology image classifiers with inductive biases about example difficulty?We propose several label smoothing methods that utilize per-image annotator agreement. Though our methods are simple, we find that they substantially improve model calibration, while maintaining (or even improving) accuracy. For colorectal polyp classification, a common yet challenging task in gastrointestinal pathology, we find that our proposed agreement-aware label smoothing methods reduce calibration error by almost 70%. Moreover, we find that using model confidence as a proxy for annotator agreement also improves calibration and accuracy, suggesting that datasets without multiple annotators can still benefit from our proposed label smoothing methods via our proposed confidence-aware label smoothing methods.Given the importance of calibration (especially in histopathology image analysis), the improvements from our proposed techniques merit further exploration and potential implementation in other histopathology image classification tasks.},
booktitle = {Artificial Intelligence in Medicine: 20th International Conference on Artificial Intelligence in Medicine, AIME 2022, Halifax, NS, Canada, June 14–17, 2022, Proceedings},
pages = {273–282},
numpages = {10},
keywords = {Calibration, Histopathology images, Label smoothing},
location = {Halifax, NS, Canada}
}

@inproceedings{10.1007/978-3-031-09342-5_25,
author = {Eastwood, Mark and Marc, Silviu Tudor and Gao, Xiaohong and Sailem, Heba and Offman, Judith and Karteris, Emmanouil and Fernandez, Angeles Montero and Jonigk, Danny and Cookson, William and Moffatt, Miriam and Popat, Sanjay and Minhas, Fayyaz and Robertus, Jan Lukas},
title = {Malignant Mesothelioma Subtyping of Tissue Images via Sampling Driven Multiple Instance Prediction},
year = {2022},
isbn = {978-3-031-09341-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-09342-5_25},
doi = {10.1007/978-3-031-09342-5_25},
abstract = {Malignant Mesothelioma is a difficult to diagnose and highly lethal cancer usually associated with asbestos exposure. It can be broadly classified into three subtypes: Epitheliod, Sarcomatoid, and Biphasic. Early diagnosis and identification of the subtype informs treatment and can help improve patient outcome. However, the subtyping of malignant mesothelioma, and specifically the recognition of transitional features from routine histology slides has a high level of inter-observer variablity. In this work, we propose the first end-to-end multiple instance learning (MIL) approach for malignant mesothelioma subtyping. This uses an instance-based sampling scheme for training deep convolutional neural networks on this task that allows learning on a wider range of relevant instances compared to max or top-N based MIL approaches. The proposed MIL approach enables identification of malignant mesothelial subtypes of specific tissue regions. From this a continuous characterization of a sample according to predominance of sarcomatoid vs epithelioid regions is possible, thus avoiding the arbitrary and highly subjective categorisation by currently used subtypes. Instance scoring also enables studying tumor heterogeneity and identifying patterns associated with different subtypes. We have evaluated the proposed method on a dataset of 243 tissue micro-array cores with an AUROC of 0.87±0.04 for this task. The dataset and developed methodology is available for the community at:},
booktitle = {Artificial Intelligence in Medicine: 20th International Conference on Artificial Intelligence in Medicine, AIME 2022, Halifax, NS, Canada, June 14–17, 2022, Proceedings},
pages = {263–272},
numpages = {10},
keywords = {Deep learning, Computational pathology, Multiple instance learning, Malignant mesothelioma},
location = {Halifax, NS, Canada}
}

@inproceedings{10.1109/IV51971.2022.9827410,
author = {Rasouli, Amir and Kotseruba, Iuliia},
title = {Intend-Wait-Cross: Towards Modeling Realistic Pedestrian Crossing Behavior},
year = {2022},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IV51971.2022.9827410},
doi = {10.1109/IV51971.2022.9827410},
abstract = {In this paper, we present a microscopic agent-based pedestrian behavior model Intend-Wait-Cross. The model is comprised of rules representing behaviors of pedestrians as a series of decisions that depend on their individual characteristics (e.g. demographics, walking speed, law obedience) and environmental conditions (e.g. traffic flow, road structure). The model’s main focus is on generating realistic crossing decision-model, which incorporates an improved formulation of time-to-collision (TTC) computation accounting for context, vehicle dynamics, and perceptual noise. Our model generates a diverse population of agents acting in a highly configurable environment. All model components, including individual characteristics of pedestrians, types of decisions they make, and environmental factors, are motivated by studies on pedestrian traffic behavior. Model parameters are calibrated using a combination of naturalistic driving data and estimates from the literature to maximize the realism of the simulated behaviors. A number of experiments validate various aspects of the model, such as pedestrian crossing patterns, and individual characteristics of pedestrians.},
booktitle = {2022 IEEE Intelligent Vehicles Symposium (IV)},
pages = {83–90},
numpages = {8},
location = {Aachen, Germany}
}

@inproceedings{10.1007/978-3-031-09282-4_16,
author = {Gr\"{a}bel, Philipp and Crysandt, Martina and Klinkhammer, Barbara M. and Boor, Peter and Br\"{u}mmendorf, Tim H. and Merhof, Dorit},
title = {Ordinal Classification and&nbsp;Regression Techniques for&nbsp;Distinguishing Neutrophilic Cell Maturity Stages in&nbsp;Human Bone Marrow},
year = {2022},
isbn = {978-3-031-09281-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-09282-4_16},
doi = {10.1007/978-3-031-09282-4_16},
abstract = {An automated classification of hematopoietic cells in bone marrow whole slide images would be very beneficial to the workflow of diagnosing diseases such as leukemia. However, the large number of cell types and particularly their continuous maturation process makes this task challenging: the boundaries of cell type classes in this process are fuzzy, leading to inter-rater disagreement and noisy annotations. The data qualifies as ordinal data, as the order of classes is well defined. However, a sensible “distance” between them is difficult to establish.In this work, we propose several classification and regression techniques for ordinal data, which alter the encoding of network output and ground-truth. For classification, we propose using the Gray code or decreasing weights. For regression, we propose encodings inspired by biological properties or characteristics of the dataset. We analyze their performance on a challenging dataset with neutrophilic granulocytes from human bone marrow microscopy images. We show that for a sensible evaluation, it is of utmost importance to take into account the relation between cell types as well as the annotation noise. The proposed methods are straight-forward to implement with any neural network and outperform common classification and regression methods.},
booktitle = {Pattern Recognition and Artificial Intelligence: Third International Conference, ICPRAI 2022, Paris, France, June 1–3, 2022, Proceedings, Part II},
pages = {186–195},
numpages = {10},
keywords = {Cell classification, Regression, Ordinal classification},
location = {Paris, France}
}

@inproceedings{10.1007/978-3-031-13321-3_48,
author = {Del Rio, Mauro and Lianas, Luca and Aspegren, Oskar and Busonera, Giovanni and Versaci, Francesco and Zelic, Renata and Vincent, Per H. and Leo, Simone and Pettersson, Andreas and Akre, Olof and Pireddu, Luca},
title = {AI Support for&nbsp;Accelerating Histopathological Slide Examinations of&nbsp;Prostate Cancer in&nbsp;Clinical Studies},
year = {2022},
isbn = {978-3-031-13320-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-13321-3_48},
doi = {10.1007/978-3-031-13321-3_48},
abstract = {While studies in pathology are essential for the progress in the diagnostic and prognostic techniques in the field, pathologist time is becoming an increasingly scarce resource, and can indeed become the limiting factor in the feasibility of studies to be performed. In this work, we demonstrate how the Digital Pathology platform by CRS4, for supporting research studies in digital pathology, has been augmented by the addition of AI-based features to accelerate image examination to reduce the pathologist time required for clinical studies. The platform has been extended to provide computationally generated annotations and visual cues to help the pathologist prioritize high-interest image areas. The system includes an image annotation pipeline with DeepHealth-based deep learning models for tissue identification and prostate cancer identification. Annotations are viewed through the platform’s virtual microscope and can be controlled interactively (e.g., thresholding, coloring). Moreover, the platform captures inference provenance information and archives it as RO-Crate artifacts containing data and metadata required for reproducibility. We evaluate the models and the inference pipeline, achieving AUC of 0.986 and 0.969 for tissue and cancer identification, respectively, and verifying linear dependence of execution speed on image tissue content. Finally, we describe the ongoing clinical validation of the contribution, including preliminary results, and discuss feedback from clinical professionals regarding the overall approach.},
booktitle = {Image Analysis and Processing. ICIAP 2022 Workshops: ICIAP International Workshops, Lecce, Italy, May 23–27, 2022, Revised Selected Papers, Part I},
pages = {545–556},
numpages = {12},
keywords = {Workflows, Artificial intelligence, Digital pathology},
location = {Lecce, Italy}
}

@inproceedings{10.1007/978-3-031-06430-2_52,
author = {Pandey, Vaidehi and Brune, Christoph and Strisciuglio, Nicola},
title = {Self-supervised Learning Through Colorization for&nbsp;Microscopy Images},
year = {2022},
isbn = {978-3-031-06429-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-06430-2_52},
doi = {10.1007/978-3-031-06430-2_52},
abstract = {Training effective models for segmentation or classification of microscopy images is a hard task, complicated by the scarcity of adequately labeled data sets. In this context, self-supervised learning strategies can be deployed to learn suitable image representations from the available large quantity of unlabeled data, e.g. the 500k electron microscopy images that compose the CEM500k data sets.In this work, we investigate a self-supervised strategy for representation learning based on a colorization pre-text task on microscopy images. We integrate the colorization task into the BYOL (Bootstrap your own latent) self-supervised contrastive pre-training strategy. We train the self-supervised architecture on the CEM500k data set of electron microscopy images. As backbone of the BYOL framework, we investigate the use of Resnet50 and a Stand-alone Self-Attention network, and subsequently test them as feature extractors for downstream classification and segmentation tasks.The Self-Attention encoders pre-trained with the colorization-based BYOL method are able to learn effective features for segmentation of microscopy images, achieving higher results than those of encoders, both Resnet- and Self-Attention-based, trained with the original BYOL. This shows the effectiveness of colorization as pre-text for a downstream segmentation task on microscopy images. We release the code at .},
booktitle = {Image Analysis and Processing – ICIAP 2022: 21st International Conference, Lecce, Italy, May 23–27, 2022, Proceedings, Part II},
pages = {621–632},
numpages = {12},
keywords = {BYOL, Colorization, Microscopy images, Pre-training, Self-supervised learning},
location = {<conf-loc content-type="Hybrid">Lecce, Italy</conf-loc>}
}

@inproceedings{10.1109/IRPS48227.2022.9764525,
author = {Shankar, Bhawani and Bian, Zhengliang and Zeng, Ke and Meng, Chuanzhe and Martinez, Rafael Perez and Chowdhury, Srabanti and Gunning, Brendan and Flicker, Jack and Binder, Andrew and Dickerson, Jeramy Ray and Kaplar, Robert},
title = {Study of Avalanche Behavior in 3 kV GaN Vertical P-N Diode Under UIS Stress for Edge-termination Optimization},
year = {2022},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IRPS48227.2022.9764525},
doi = {10.1109/IRPS48227.2022.9764525},
abstract = {This work investigates both avalanche behavior and failure mechanism of 3 kV GaN-on-GaN vertical P-N diodes, that were fabricated and later tested under unclamped inductive switching (UIS) stress. The goal of this study is to use the particular avalanche characteristics and the failure mechanism to identify issues with the field termination and then provide feedback to improve the device design. DC breakdown is measured at the different temperatures to confirm the avalanche breakdown. Diode’s avalanche robustness is measured on-wafer using a UIS test set-up which was integrated with a wafer chuck and CCD camera. Post failure analysis of the diode is done using SEM and optical microscopy to gain insight into the device failure physics.},
booktitle = {2022 IEEE International Reliability Physics Symposium (IRPS)},
pages = {2B.2–1–2B.2-4},
location = {Dallas, TX, USA}
}

@inproceedings{10.1007/978-3-319-10443-0_48,
author = {Mualla, Firas and Sch\"{o}ll, Simon and Sommerfeldt, Bj\"{o}rn and Maier, Andreas and Steidl, Stefan and Buchholz, Rainer and Hornegger, Joachim},
title = {Unsupervised Unstained Cell Detection by SIFT Keypoint Clustering and Self-labeling Algorithm},
year = {2022},
isbn = {978-3-319-10442-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-10443-0_48},
doi = {10.1007/978-3-319-10443-0_48},
abstract = {We propose a novel unstained cell detection algorithm based on unsupervised learning. The algorithm utilizes the scale invariant feature transform (SIFT), a self-labeling algorithm, and two clustering steps in order to achieve high performance in terms of time and detection accuracy. Unstained cell imaging is dominated by phase contrast and bright field microscopy. Therefore, the algorithm was assessed on images acquired using these two modalities. Five cell lines having in total 37 images and 7250 cells were considered for the evaluation: CHO, L929, Sf21, HeLa, and Bovine cells. The obtained F-measures were between 85.1 and 89.5. Compared to the state-of-the-art, the algorithm achieves very close F-measure to the supervised approaches in much less time.},
booktitle = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2014},
pages = {377–384},
numpages = {8},
keywords = {Supervise Approach, Centeredness Error, Cell Detection, Scale Invariant Feature Transform, Interest Point}
}

@inproceedings{10.1145/3476123.3487875,
author = {Blackistone, Kevin and Bastan, Amir},
title = {Microbiospheric engineering},
year = {2022},
isbn = {9781450386869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3476123.3487875},
doi = {10.1145/3476123.3487875},
abstract = {Microbiology invokes features of our environment that are often unseen - interacting without our direct intent or involvement, while automation conjures views of large-scale, tightly controlled mass-production. As our technology has progressed, our abilities to manufacture have extended into the micro world, aided by ever more refined industrial machinery. At the same time, these technologies have allowed us to further populate our own world while extracting from it ever greater resources. I seek to explore this convergence through a merged visual metaphor, involving human bacterial colonies, their interactions amongst wild-spawning microflora, and the automated systems that will be used for their surveillance. This is to be achieved through a clear sphere upon which shall be a layer of sculpted microbial growth media. Human seeded bacterial populations monitored by automated microscope shall present visual landscapes and satellite-style vistas of their expansion. Each day, further growth shall be visible upon the mountains, valleys and planes of the agar topography. The combined built and spontaneous cartographies shall provide means to internalize population expansions and resource depletions of our own biosphere, while the proximal automata presenting these unseen worlds draws focus on the approaching micro : macro interactions of mechanical : biological manufacture and our own potential technological limits of growth.},
booktitle = {SIGGRAPH Asia 2021 Art Gallery},
articleno = {8},
numpages = {1},
location = {Tokyo, Japan},
series = {SA '21}
}

@inproceedings{10.1145/3498731.3498761,
author = {Zhao, Xuanning},
title = {An overview of Alzheimer's disease and its diagnosis using conventional and novel methods},
year = {2022},
isbn = {9781450384308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498731.3498761},
doi = {10.1145/3498731.3498761},
abstract = {Alzheimer's Disease (AD), a progressive neurodegenerative disease caused by abundant abnormal extracellular amyloid-β plaques and intracellular assemblage of tau inclusions, can severely affect the living of patients as it causes dementia. As aging is the most important risk factor of AD, an increasing aging global population will lead to more serious problems in the future. The progression of AD includes three stages: the asymptomatic stage, Mild Cognitive Impairment and dementia. At present, most diagnoses methods are focused on MCI and dementia stages of the disease as disease during the asymptomatic stage is challenging. At present, effective diagnostic methods include neuropathological diagnosis that focus on the macroscopic features and microscopic features of AD, biomarkers such as cerebrospinal fluid biomarkers and other biomarkers in the human plasma, imaging techniques including structural magnetic resonance imaging and F-fluorodeoxyglucose-position emission tomography, psychological and behavioral tests, etc., but most of them have obvious deficiencies. Novel diagnostic methods, such as saliva biomarkers, which cost less and are more accurate in diagnosing AD can have promises for early diagnosis of AD, leading better patient outcomes. With the advancement of emergence of technologies, such as artificial intelligence, more effective diagnosis methods will be available, and the efficiency of diagnosing AD will increase, benefiting patients and their families. This paper provides an overview of AD and investigates conventional and novel methods for its diagnosis.},
booktitle = {Proceedings of the 2021 10th International Conference on Bioinformatics and Biomedical Science},
pages = {187–195},
numpages = {9},
keywords = {Diagnosis, Dementia, Cognition, Alzheimer's Disease},
location = {Xiamen, China},
series = {ICBBS '21}
}

@inproceedings{10.1145/3485314.3485331,
author = {Cheng, Jijun and Mai, Jinhai and Pan, Xipeng and Chen, Xin and Han, Chu and Liu, Zaiyi and Liang, Changhong},
title = {Curriculum Self-supervised Learning for Weakly-supervised Histopathological Image Segmentation},
year = {2022},
isbn = {9781450384957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485314.3485331},
doi = {10.1145/3485314.3485331},
abstract = {Histopathological image segmentation is the basic upstream task of pathomics cancer research. However, acquiring pixel-level annotations is extremely difficult and time-consuming. Moreover, it is also expertise-dependent. Researchers pay a lot of attention on how to reduce the annotation effort. One feasible solution is self-supervised learning. In this paper, we proposed a novel self-supervised learning strategy for histopathological image segmentation, which called Curriculum Self-supervised Learning (CSSL). In CSSL, we set up several pretext tasks which are highly related to the characteristics of histopathological images for the CNN model. And then, these pretext tasks are trained in a curriculum manner (easy to hard). By combining CSSL with Class Activation Map (CAM), we can achieve outstanding performance on a weakly-supervised tissue segmentation task, with no need of pixel-level annotation. We achieved Frequency-weighted Intersection over Union (FIoU) score of 0.6886 and Mean Intersection over Union (MIoU) score of 0.6985 for semantic segmentation, which exceeded the common self-supervised learning methods. Experiments were conducted to prove the effectiveness of proposed CSSL.},
booktitle = {2021 10th International Conference on Internet Computing for Science and Engineering},
pages = {116–123},
numpages = {8},
keywords = {Self-supervised learning, Histopathological image segmentation, Computational pathology},
location = {Guilin, China},
series = {ICICSE 2021}
}

@inproceedings{10.1145/3500931.3501001,
author = {Liu, Nanyan and Li, Zhongpin},
title = {Cell Image Stitching Algorithm Based on Retinex And Improved ORB},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3501001},
doi = {10.1145/3500931.3501001},
abstract = {Due to the limited shooting range of microscopic imaging equipment, as well as the complicated factors such as the shooting environment and the cell tissue itself, the microscopic cell image is not clear in detail and the brightness is not equal. An algorithm based on Retinex and improved ORB for microscopic cell image registration and stitching is proposed. Firstly, the multi-scale Retinex algorithm is improved by homomorphic high-low-pass filtering to enhance the image, and the image feature extraction area is specified to improve the efficiency of algorithm execution. Then the improved ORB algorithm can be used to extract feature points, which can reduce feature point redundancy while reducing feature extraction time. Finally, the Laplacian multi-resolution fusion algorithm based on the optimal seam algorithm was used to eliminate the large color difference and ghosting on both sides of the stitch. The experimental results show that the algorithm has achieved better results in the quality and clarity of microscopic cell image stitching, and the efficiency of the algorithm has been improved.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {408–415},
numpages = {8},
keywords = {Retinex, ORB, Image stitching, Image fusion, Image enhancement},
location = {Beijing, China},
series = {ISAIMS '21}
}

@inproceedings{10.1145/3500931.3500983,
author = {Chen, Yujie and Guan, Rongfa and Huang, Haizhi and Zhong, Hao and Sun, Yujing and Zhang, Wei},
title = {Optimization of Conditions for Ginsenoside Re liposomes Production by Response Surface Methodology},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3500983},
doi = {10.1145/3500931.3500983},
abstract = {In this study, we aimed to optimize the formulation of Ginsenoside Re liposomes using response surface methodology. A transmission electron microscope observed the morphology of G-Re liposomes. Additionally, the microstructure and size of G-Re liposomes were determined by a particle size analyzer. The film dispersion method is a simple and efficient method for liposome preparation. The results showed that the optimal preparation conditions for this method were: phosphatidylcholine/Re ratio of 33.68, phosphatidylcholine/cholesterol ratio of 10.19, and rotary evaporation temperature of 48.50 °C. Under the optimal conditions, the particle size and EE of G-Re liposomes were 173.87 ± 4.74 nm and 60.91% ±1.53, respectively. The G-Re liposomes showed a spherical structure with a smooth surface. The liposomes as carriers improved the stability of G-Re.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {296–300},
numpages = {5},
keywords = {response surface methodology, liposome, Stability, Ginsenoside Re},
location = {Beijing, China},
series = {ISAIMS '21}
}

@inproceedings{10.1145/3492547.3492604,
author = {Hassan Kara, Mohsen and A. Gouda, Khaled},
title = {Fabrication and Characterization of Bundle Carbon Nanotubes Transmission Lines for MMIC Applications},
year = {2021},
isbn = {9781450390446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492547.3492604},
doi = {10.1145/3492547.3492604},
abstract = {We report here an investigation on the possibility of using bundle carbon nanotubes via in-situ process as a new conductive material for monolithic microwave integrated circuit (MMIC) transmission lines to replace existing materials, because they have potential to offer better performance. Multi-wall carbon nanotubes (MWCNTs) with high purity and density grown on SiO2/ Si substrates have been successfully synthesized at 800 - 950 °C by using a novel thermal chemical vapour deposition (TCVD) method with methane (CH4) gas as the carbon source and nickel (Ni) as catalyst. Transmission lines (TL) with different effective CNT lengths, as well as transmission lines without CNT were then fabricated. The microstructure of the samples was examined using scanning electron microscopy (SEM), field emission scanning electron microscopy (FESEM) and energy dispersive X-ray analysis (EDS) to investigate the surface morphology and element composition of the samples respectively, whereas Raman spectroscopy was used to analyze their crystallinity. The SEM observations reveal the condition of high dense MWNTs crossing both islands. The micro Raman spectroscopy results showed that even though CNT bundle in the TLs are not aligned, they have high crystallinity and density. Further RF characterization was done on the CNT transmission line test structures to study the electrical properties at high frequency using Cascade Microtech probes in conjunction with vector network analyzer. The samples were designed to have symmetric two port networks (i.e.│S12│= │S21│and │S11│ = │S22│). Two port reflection measurements were performed to obtain the reflection coefficients (S11) and transmission coefficients (S21) in the range of 6 to 20 GHz. The resistivity of CNT was 6.3 \texttimes{} 10-5 Ω-m and the conductivity was found to be 1.6 \texttimes{} 104 S/m. Even though the resistivity value is arguably high, it is still lower than previously mentioned values of other works.},
booktitle = {The 7th International Conference on Engineering &amp; MIS 2021},
articleno = {28},
numpages = {8},
location = {Almaty, Kazakhstan},
series = {ICEMIS'21}
}

@inproceedings{10.1145/3485114.3485120,
author = {Fossdal, Frikk and Heldal, Rogardt and Peek, Nadya},
title = {Interactive Digital Fabrication Machine Control Directly Within a CAD Environment},
year = {2021},
isbn = {9781450390903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485114.3485120},
doi = {10.1145/3485114.3485120},
abstract = {Interactive fabrication aims to close the gap between design and fabrication, allowing for rich interactions with materials and reflection in action. Drawing from craft practice, we contribute software that enables real-time control of digital fabrication machines from a Computer-Aided Design (CAD) environment. Our software not only allows interactive control of toolpath geometry, but also enables the control of machine parameters such as speed, acceleration, or jerk. This creates new opportunities for toolpath and material exploration. We evaluate our software with a professional glass artist on a custom digital fabrication machine that can accommodate multiple tools such as brushes, engraving bits, or microscopes. Finally, we reflect on implications for machine control.},
booktitle = {Proceedings of the 6th Annual ACM Symposium on Computational Fabrication},
articleno = {8},
numpages = {15},
keywords = {Machine Design, Machine Control, Interactive Fabrication, Digital Craft, Computational Fabrication, CNC, CAD/CAM},
location = {Virtual Event, USA},
series = {SCF '21}
}

@inproceedings{10.1109/SMC52423.2021.9658801,
author = {Kato, Sota and Hotta, Kazuhiro},
title = {Automatic Preprocessing and Ensemble Learning for Cell Segmentation with Low Quality},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC52423.2021.9658801},
doi = {10.1109/SMC52423.2021.9658801},
abstract = {We propose an automatic preprocessing and ensemble learning for segmentation of cell images with low quality. It is difficult to capture cells with strong light. Therefore, the microscopic images of cells tend to have low image quality but these images are not good for semantic segmentation. Here we propose a method to translate an input image to the images that are easy to recognize by deep learning. The proposed method consists of two deep neural networks. The first network is the usual training for semantic segmentation, and penultimate feature maps of the first network are used as filters to translate an input image to the images that emphasize each class. This is the automatic preprocessing and translated cell images are easily classified. The input cell image with low quality is translated by the feature maps in the first network, and the translated images are fed into the second network for semantic segmentation. Since the outputs of the second network are multiple segmentation results, we conduct the weighted ensemble of those segmentation images. Two networks are trained by end-to-end manner, and we do not need to prepare images with high quality for the translation. We confirmed that our proposed method can translate cell images with low quality to the images that are easy to segment, and segmentation accuracy has improved using the weighted ensemble learning.},
booktitle = {2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
pages = {1836–1841},
numpages = {6},
location = {Melbourne, Australia}
}

@inproceedings{10.1145/3450337.3483490,
author = {Wang, Tiange and Huang, I-Yang},
title = {Viruscape: A Microscopic Adventure Game to Guide Conceptual Learning of SARS-CoV-2 Mechanisms},
year = {2021},
isbn = {9781450383561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450337.3483490},
doi = {10.1145/3450337.3483490},
abstract = {This paper presents the pilot version of Viruscape, a novel adventure PC and console game designed to facilitate the conceptual learning of Coronavirus mechanisms and expand the civic science education toolkit. Medical illustration using 3D modeling and motion graphics software has long been a niche practice to help advance education, research, and practice in healthcare-related fields. In Viruscape, we leverage the unexploited potential of such software to create interactive virtual worlds and gameplay experience to promote a more systematic learning of human health concepts and advance medical literacy for the general public. Viruscape immerses players in a microscopic world through the “eyes” of viruses whose goal is to defeat the human immune system. We translate the relevant immunology mechanisms into game play mechanisms that players learn to master, while providing a unique and interactive gaming experience. Future development of the game could focus on involving diverse users and health experts in co-design or consultation sessions and customizing the game world to represent players’ real-world health condition. The concept of Viruscape can be used to further develop parallel episodes of the game that feature other human health mechanisms such as other types of infection, heart disease or cancer.},
booktitle = {Extended Abstracts of the 2021 Annual Symposium on Computer-Human Interaction in Play},
pages = {209–215},
numpages = {7},
keywords = {Public health, Microscopic universe, Meaningful play, Interactive medical game, Game design, Educational game, Citizen science},
location = {<conf-loc>, <city>Virtual Event</city>, <country>Austria</country>, </conf-loc>},
series = {CHI PLAY '21}
}

@inproceedings{10.1007/978-3-030-98260-7_14,
author = {Cowan, Mark Anthony},
title = {Using SUMO to Construct Dynamic Urban Modeling Scenarios for Military Transport},
year = {2021},
isbn = {978-3-030-98259-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98260-7_14},
doi = {10.1007/978-3-030-98260-7_14},
abstract = {SUMO (Simulation of Urban MObility) is a microscopic simulator of continuous multimodal vehicular and pedestrian traffic along large urban road networks. First released in 2001 by Berlin’s Institute of Transportation Systems, SUMO has been used to investigate the effects of vehicle pollution and noise, to generate traffic forecasts during large athletic events, and to model in-vehicle telephony to assess performance of traffic surveillance devices. A SUMO scenario can quickly be initiated by importing street network topologies from OpenStreetMap (OSM), a freely-available, constantly updated, crowd-sourced and -tagged mapping service. With the OSM network as a foundation, one can easily add, delete, and modify traffic lanes and the timing of traffic lights to explore the effects upon local traffic over time as the populace attempts to route between its origin and destination pairs, making adjustments to their routes as necessary. The tags within OSM add the possibility of choosing origin-destination regions for the motorists and pedestrians based upon city zoning, from which we can likely infer some features of the demographic layout and thereby add more realism to the traffic simulation.In this paper, we will focus on building these scenarios for military transport across large urban areas and collecting the results across many runs of the SUMO software, varied by the random seeding of the model and changing the open/close times of some important lanes. While limited visualization tools exist for single runs, we will fortify these with a more global view and summarize the analytic results for military decision-makers, enabling them to anticipate potential traffic bottlenecks that could interfere with their mission and to choose optimally among alternate routes on-the-fly as new information arrives.},
booktitle = {Modelling and Simulation  for Autonomous Systems: 8th International Conference, MESAS 2021, Virtual Event, October 13–14, 2021, Revised Selected Papers},
pages = {227–248},
numpages = {22},
keywords = {Data analysis, Urban, SUMO, Simulation, Mobility, Scenario, Military transport}
}

@inproceedings{10.1007/978-3-030-95467-3_5,
author = {Zaleshina, Margarita and Zaleshin, Alexander},
title = {Topological Properties of Mouse Neuronal Populations in Fluorescence Microscopy Images},
year = {2021},
isbn = {978-3-030-95466-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-95467-3_5},
doi = {10.1007/978-3-030-95467-3_5},
abstract = {In this work, we processed sets of images obtained by the light-sheet fluorescence microscopy method. We selected different cell groups and determined areas occupied by ensembles of cell groups in mouse brain tissue. Recognition of mouse neuronal populations was performed on the basis of visual properties of fluorescence-activated cells. Individual elements were selected based on their brightness in grayscale mode. Methods of spatial data processing were applied to identify border areas between ensembles and to calculate topological characteristics of cell groups. By applying cell statistics operations, we obtained the localization of the regions of interest, for subsequent identification of samples with specified topological characteristics. Based on the topological properties of the cell groups, we constructed training samples, and then used these to detect typical sets of ensembles in multi-page TIFF files with optogenetics datasets.},
booktitle = {Machine Learning, Optimization, and Data Science: 7th International Conference, LOD 2021, Grasmere, UK, October 4–8, 2021, Revised Selected Papers, Part I},
pages = {69–80},
numpages = {12},
keywords = {Brain mapping, Pattern recognition, Optogenetics, Mouse brain},
location = {Grasmere, United Kingdom}
}

@inproceedings{10.1007/978-3-030-90439-5_32,
author = {Dhillon, Daljit Singh J.},
title = {Physically Based Rendering of Simple Thin Volume Natural Nanostructures},
year = {2021},
isbn = {978-3-030-90438-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90439-5_32},
doi = {10.1007/978-3-030-90439-5_32},
abstract = {Thin volumes of semi-transparent nanostructures present on outer layers of organic embodiments often interact coherently with incident light waves to produce nuanced structural coloration. Such mechanisms are further complicated through incoherent scattering by accompanying micro-geometries. We present a simple physically based approach to directly use the sub-microscopic scans of quasi-periodic, one-dimensional modulations in such volumes to render them realistically. Our method relies on prior knowledge of quasi-periodicity to process the scan data in the Fourier space for recreating nuanced coloration effects. We demonstrate the working of our method with the actual scanned data of an egg-sac that shows coloration only when immersed in water. Proposed method can be used by bio-physicists for visual conformity of such mechanisms at a macro-scale as well as graphical rendering pipelines can employ it for scientific recreations or artistic renditions.},
booktitle = {Advances in Visual Computing: 16th International Symposium, ISVC 2021, Virtual Event, October 4-6, 2021, Proceedings, Part I},
pages = {400–413},
numpages = {14},
keywords = {Volumetric rendering, Sub surface scattering, Natural phenomena, Photonics, Diffraction, Wave interference, Wave optics, Physically based rendering, Structural colors}
}

@inproceedings{10.1007/978-3-030-87722-4_5,
author = {Mahapatra, Dwarikanath and Bozorgtabar, Behzad and Kuanar, Shiba and Ge, Zongyuan},
title = {Self-supervised Multimodal Generalized Zero Shot Learning for Gleason Grading},
year = {2021},
isbn = {978-3-030-87721-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87722-4_5},
doi = {10.1007/978-3-030-87722-4_5},
abstract = {Gleason grading from histopathology images is essential for accurate prostate cancer (PCa) diagnosis. Since such images are obtained after invasive tissue resection quick diagnosis is challenging under the existing paradigm. We propose a method to predict Gleason grades from magnetic resonance (MR) images which are non-interventional and easily acquired. We solve the problem in a generalized zero-shot learning (GZSL) setting since we may not access training images of every disease grade. Synthetic MRI feature vectors of unseen grades (classes) are generated by exploiting Gleason grades’ ordered nature through a conditional variational autoencoder (CVAE) incorporating self-supervised learning. Corresponding histopathology features are generated using cycle GANs, and combined with MR features to predict Gleason grades of test images. Experimental results show our method outperforms competing feature generating approaches for GZSL, and comes close to performance of fully supervised methods.},
booktitle = {Domain Adaptation and Representation Transfer, and Affordable Healthcare and AI for Resource Diverse Global Health: Third MICCAI Workshop, DART 2021, and First MICCAI Workshop, FAIR 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27 and October 1, 2021, Proceedings},
pages = {46–56},
numpages = {11},
keywords = {MRI, Histopathology, Gleason grading, CVAE, GZSL},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87615-9_14,
author = {Rafael-Patino, Jonathan and Girard, Gabriel and Truffet, Rapha\"{e}l and Pizzolato, Marco and Thiran, Jean-Philippe and Caruyer, Emmanuel},
title = {The Microstructural Features of the Diffusion-Simulated Connectivity (DiSCo) Dataset},
year = {2021},
isbn = {978-3-030-87614-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87615-9_14},
doi = {10.1007/978-3-030-87615-9_14},
abstract = {We present a detailed description of the structural characteristics of the MICCAI 2021 Diffusion Simulated Connectivity (DiSCo) Challenge synthetic dataset. The DiSCo dataset are one of a kind numerical phantoms for the simulation of the diffusion-weighted images (DWIs) via Monte-Carlo diffusion simulations. The microscopic and macroscopic complexity of the synthetic substrates allows the evaluation of processing pipelines for the estimation of the quantitative structural connectivity. The diffusion-weighted signal in each voxel of the DWIs is obtained from Monte-Carlo simulations of particle dynamics within a substrate of an unprecedented size of 1&nbsp;mm3, allowing for an image matrix size up&nbsp;to 40\texttimes{}40\texttimes{}40 voxels (isotropic voxel sizes of 25&nbsp;μm). In this paper, we provide a characterization of the microstructural properties of the DiSCo dataset, which is composed of three numerical phantoms with comparable microstructure. We report the ground-truth tissue volume fractions (“intra-axonal”, “extra-axonal”, “myelin”), the fibre density, the bundle density and the fibre orientation distributions (FODs). We believe that this characterization will be beneficial for validating quantitative structural connectivity processing pipelines, and that could eventually find use in microstructural modelling based on machine learning approaches.},
booktitle = {Computational Diffusion MRI: 12th International Workshop, CDMRI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, October 1, 2021, Proceedings},
pages = {159–170},
numpages = {12},
keywords = {Microstructure, Tractography, Phantoms, DW-MRI, Monte-Carlo simulations},
location = {Strasbourg, France}
}

@inproceedings{10.1109/IROS51168.2021.9636853,
author = {Pumphrey, Michael and Al-Tamimi, Mahmoud and Abouzarkhanifard, Aylar and Janaideh, Mohammad Al and R\'{e}gnier, St\'{e}phane and Boudaoud, Mokrane},
title = {Analysis of the Effect of Clearance in Spherical Joints on the Rotation Accuracy of Parallel Type Micro-Robotic Systems},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IROS51168.2021.9636853},
doi = {10.1109/IROS51168.2021.9636853},
abstract = {The spherical joint is an effective solution to design parallel micro-robotic systems with rotation capabilities in the three-dimensional space. This type of joint has however some non-linear characteristics, such as the clearance, which affect the positioning accuracy in micro-robotic tasks. The starting point of this study lies in experimental observations of rotation errors from a 3-PPPS 6-DOF parallel micro-robotic systems operating inside a scanning electron microscope. The objective of the paper is to assess the role of the spherical joints in the rotation errors and to evaluate whether the joints non-linearities can cause errors with the same order of magnitude as those observed experimentally. To this end, the first part of the study addresses the modeling of 3-PPPS 6-DOF parallel micro-robotic systems with spherical joints including the clearance. This model allows for analysing the effect of the clearance on position and rotation accuracies of the micro-robotic system. It is found by simulations that the same positioning behavior as in the experiments occurs when the clearance of the spherical joint is included in the model, supporting the hypothesis. Therefore, it is concluded that clearance in spherical joints has a significant effect on the precision of parallel type micro-robotic systems which opens new challenges in the control of poly-articulated micro-robotic systems with clearance compensation.},
booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {8551–8556},
numpages = {6},
location = {Prague, Czech Republic}
}

@inproceedings{10.1007/978-3-030-97281-3_3,
author = {Chung, Youjin and Cho, Jihoon and Park, Jinah},
title = {Domain-Robust Mitotic Figure Detection with&nbsp;Style Transfer},
year = {2021},
isbn = {978-3-030-97280-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-97281-3_3},
doi = {10.1007/978-3-030-97281-3_3},
abstract = {Recent studies for mitotic figure identification have shown performance comparable to that of human experts; however, the challenge to develop strategies invariant to image variance in different microscope slide scanners still remains. In this paper, we propose a method for domain generalization in mitotic figure detection by considering the scanner as a domain and the characteristic specified for each domain as a style. The method aims to make the mitosis detection network robust to scanner types by using various style images. To expand the style variance, the style of the training image is transferred into arbitrary styles by the proposed style transfer module based on StarGAN. Furthermore, we propose patch selection criteria to resolve the imbalance problem. Our model with the proposed training scheme has obtained satisfactory detection performance on the MIDOG Challenge containing scanners that have not been seen.},
booktitle = {Biomedical Image Registration, Domain Generalisation and Out-of-Distribution Analysis: MICCAI 2021 Challenges: MIDOG 2021, MOOD 2021, and Learn2Reg 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27–October 1, 2021, Proceedings},
pages = {23–31},
numpages = {9},
keywords = {Style Transfer, Mitosis Detection, Domain Generalization},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87589-3_58,
author = {Perakis, Alexis and Gorji, Ali and Jain, Samriddhi and Chaitanya, Krishna and Rizza, Simone and Konukoglu, Ender},
title = {Contrastive Learning of Single-Cell Phenotypic Representations for&nbsp;Treatment Classification},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_58},
doi = {10.1007/978-3-030-87589-3_58},
abstract = {Learning robust representations to discriminate cell phenotypes based on microscopy images is important for drug discovery. Drug development efforts typically analyse thousands of cell images to screen for potential treatments. Early works focus on creating hand-engineered features from these images or learn such features with deep neural networks in a fully or weakly-supervised framework. Both require prior knowledge or labelled datasets. Therefore, subsequent works propose unsupervised approaches based on generative models to learn these representations. Recently, representations learned with self-supervised contrastive loss-based methods have yielded state-of-the-art results on various imaging tasks compared to earlier unsupervised approaches. In this work, we leverage a contrastive learning framework to learn appropriate representations from single-cell fluorescent microscopy images for the task of Mechanism-of-Action classification. The proposed work is evaluated on the annotated BBBC021 dataset, and we obtain state-of-the-art results in NSC, NCSB and drop metrics for an unsupervised approach. We observe an improvement of 10% in NCSB accuracy and 11% in NSC-NSCB drop over the previously best unsupervised method. Moreover, the performance of our unsupervised approach ties with the best supervised approach. Additionally, we observe that our framework performs well even without post-processing, unlike earlier methods. With this, we conclude that one can learn robust cell representations with contrastive learning. We make the code available on GitHub ().},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {565–575},
numpages = {11},
keywords = {Contrastive learning, Unsupervised learning, Cell representations, Cell images, Profiling, Phenotypes, Fluorescent microscopy},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87589-3_5,
author = {Wu, Zhuoyue and Li, Hansheng and Cui, Lei and Kang, Yuxin and Liu, Jianye and Ali, Haider and Feng, Jun and Yang, Lin},
title = {Interpretable Histopathology Image Diagnosis via Whole Tissue Slide Level Supervision},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_5},
doi = {10.1007/978-3-030-87589-3_5},
abstract = {The deep learning methods supervised by annotating different regions of histopathology images (patch-level labels) have achieved promising outcomes in assisting pathologic diagnosis. However, most clinical data only contains label information for the whole slide image (WSI-level labels), so the methods supervised by WSI-level labels are more necessary than the ones supervised by patch-level labels. Additionally, various methods supervised by WSI-level labels ignore the contextual relations among patches extracted from a WSI, making incorrect predictions for some patches in a WSI and further misclassifying the WSI. In this paper, we propose to utilize an interpretable dual encoder network with a context-capturing RNN module to capture the contextual relations among all patches extracted from a WSI. Besides, we propose to utilize a feature attention module to weigh the importance of each patch automatically. More importantly, visualization of weight for each patch in a WSI demonstrates that our approach matches the concerns of pathologists. Furthermore, extensive experiments demonstrate the superiority of the interpretable dual encoder network.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {40–49},
numpages = {10},
keywords = {WSI-level labels., Patch-level labels, Interpretable, Contextual relations, Whole slide image},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87589-3_29,
author = {Tlusty, Tal and Ozery-Flato, Michal and Barros, Vesna and Barkan, Ella and Amit, Mika and Gruen, David and Guindy, Michal and Arazi, Tal and Rozin, Mona and Rosen-Zvi, Michal and Hexter, Efrat},
title = {Pre-biopsy Multi-class Classification of Breast Lesion Pathology in Mammograms},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_29},
doi = {10.1007/978-3-030-87589-3_29},
abstract = {Characterization of lesions by artificial intelligence (AI) has been the subject of extensive research. In recent years, many studies demonstrated the ability of convolution neural networks (CNNs) to successfully distinguish between malignant and benign breast lesions in mammography (MG) images. However, to date, no study has assessed the specific sub-type of lesions in MG images, as detailed in histolopathology reports. We present a method for finer classification of breast lesions in MG images into multiple pathology sub-types. Our approach works well with radiologists’ diagnostic workflow, and uses data available in radiology reports. The proposed Dual-Radiology Dual-Resolution Network (Du-Rad Du-Res Net) receives dual input from the radiologist and dual image resolutions. The radiologist input includes annotation of the lesion area and semantic radiology features; the dual image resolutions comprise a low resolution of the entire mammogram and a high resolution of the lesion area. The network estimates the likelihood of malignancy, as well as the associated pathological sub-type. We show that the combined input of the lesion region of interest (ROI) and the entire mammogram is important for optimizing the model’s performance. We tested the AI in a reader study on a dataset of 100 heldout cases. The AI outperformed three breast radiologists in the task of lesion histopathology sub-typing.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {277–286},
numpages = {10},
keywords = {Deep neural networks, Mammography, Breast cancer},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87240-3_15,
author = {Yin, Chong and Liu, Siqi and Shao, Rui and Yuen, Pong C.},
title = {Focusing on Clinically Interpretable Features: Selective Attention Regularization for Liver Biopsy Image Classification},
year = {2021},
isbn = {978-3-030-87239-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87240-3_15},
doi = {10.1007/978-3-030-87240-3_15},
abstract = {Liver biopsy image analysis is the gold standard for early diagnosis of non-alcoholic fatty liver disease (NAFLD) worldwide. Deep neural networks offer an effective tool for image analysis. However, when applying deep learning methods to smaller histological image datasets, the model may be distracted by dominant normal tissues and ignore critical tissue alterations that pathologists focus on. In this paper, we propose a selective attention regularization module (SAttenReg) to mimic the diagnosis process of pathologists. Specifically, to explicitly encourage the model to focus on clinically interpretable features (e.g., nuclei and fat droplets), SAttenReg learns the attention map with the regularization of clinically interpretable features. Furthermore, with the different contributions of histological features, the model can selectively focus on different histological features based on the distribution of nuclei in each instance. Experiments conducted on the in-house Liver-NAS and public Biopsy4Grading biopsy image datasets show that our method achieves superior classification performance with promising localization results.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part V},
pages = {153–162},
numpages = {10},
keywords = {Selective attention regularization, Liver biopsy images},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87237-3_51,
author = {Li, Hang and Yang, Fan and Xing, Xiaohan and Zhao, Yu and Zhang, Jun and Liu, Yueping and Han, Mengxue and Huang, Junzhou and Wang, Liansheng and Yao, Jianhua},
title = {Multi-modal Multi-instance Learning Using Weakly Correlated Histopathological Images and Tabular Clinical Information},
year = {2021},
isbn = {978-3-030-87236-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87237-3_51},
doi = {10.1007/978-3-030-87237-3_51},
abstract = {The fusion of heterogeneous medical data is essential in precision medicine to assist medical experts in treatment decision-making. However, there is often little explicit correlation between data from different modalities such as histopathological images and tabular clinical data. Besides, attention-based multi-instance learning (MIL) often lacks sufficient supervision to assign appropriate attention weights for informative image patches and thus generates a good global representation for the whole image. In this paper, we propose a novel multi-modal multi-instance joint learning method, which fuses different modalities and magnification scales as a cross-modal representation to capture the potential complementary information and recalibrate the features in each modality. Furthermore, we leverage the information from tabular clinical data to optimize the MIL bag representation in the imaging modality. The proposed method is evaluated on a challenging medical task, i.e., lymph node metastasis (LNM) prediction of breast cancer, and achieves the state-of-the-art performance with AUC of 0.8844, outperforming the AUC of 0.7111 using histopathological images or the AUC of 0.8312 using tabular clinical data alone. An open-source implementation of our approach can be found at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part VIII},
pages = {529–539},
numpages = {11},
keywords = {Multi-scale, Histopathological image analysis, Multi-modal multi-instance learning},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87237-3_44,
author = {Lin, Canfeng and Wu, Huisi and Wen, Zhenkun and Qin, Jing},
title = {Automated Malaria Cells Detection from Blood Smears Under Severe Class Imbalance via Importance-Aware Balanced Group Softmax},
year = {2021},
isbn = {978-3-030-87236-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87237-3_44},
doi = {10.1007/978-3-030-87237-3_44},
abstract = {Malaria is one of the main threats to global health. Manual examination of thick and thin blood smears is the current gold standard for diagnosing malaria. However, it is of extremely low throughput and susceptible to human bias, and hence, automated detection tools are highly demanded in practice. Developing an automated detection algorithm is a quite challenging due to (1) the wide range of variations in bright field microscopy images, and (2) more importantly, the severe class imbalance problem in this task. While recently proposed balanced group softmax is somehow able to alleviate the problem of class imbalance, the crucial prerequisite for its success is that the samples can be correctly categorized into different classes. We present a novel importance-aware BGS (IaBGS) to address the class imbalance problem and thereby improve the detection performance. Our main idea is to introduce a relation module (RM) before the group softmax module in the network to learn the relationships between different cells. We then figure out the feature of a cell by considering the relationships between this cell and other cells in the input image with different cells having different learned weights. In the RM module, we leverage both the appearance features and locations to calculate the feature of each cell to take full advantage of the relationships to obtain more discriminative features for BGS. We conducted extensive experiments on a famous dataset to evaluate the proposed IaBGS. Experimental results demonstrate the effectiveness of the proposed approach, consistently outperforming state-of-the-art methods. Codes will be released upon publication.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part VIII},
pages = {455–465},
numpages = {11},
keywords = {Cells detection, Relationship, Class imbalance, Malaria},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87237-3_43,
author = {Huang, Jinghan and Shen, Yiqing and Shen, Dinggang and Ke, Jing},
title = {CA2.5-Net Nuclei Segmentation Framework with a Microscopy Cell Benchmark Collection},
year = {2021},
isbn = {978-3-030-87236-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87237-3_43},
doi = {10.1007/978-3-030-87237-3_43},
abstract = {Nuclei segmentation is an indispensable prerequisite for microscope image analyses. However, a successful instance segmentation result is still challenging attributable to the ubiquitous presence of clustered nuclei, as well as the morphological variation among dissimilar phenotype of cells. In this paper, a novel contour-aware 2.5-path decoder network (CA2.5-Net) is proposed for nuclei segmentation in microscope images. In contrast to the regular two-path decoders in many previous contour-aware networks, a shared decoder path is employed when the clustered-edge problem is severe. The range of recognizability difficulty generated by the extra half path also serves as a natural proxy to construct a curriculum-learning model, where training samples are sequenced for a better segmentation performance. Last, in this paper, we publicize two well-annotated privately-owned datasets covering a wide range of difficulty in the nuclei segmentation task, comprising 500 confocal microscopy image patches of deep-sea archaea and drosophila embryos obtained from 2013 to 2020. In the benchmark test of these two own datasets and one open-source set, our model outperforms the state-of-the-art nuclei segmentation approaches by a large margin, evaluated by the metrics of Average Jaccard Index and Dice score. Empirically, the proposed structure triples the training convergence speed in comparison with the competing CIA-net and BRP-net structures in nuclei segmentation.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part VIII},
pages = {445–454},
numpages = {10},
keywords = {Curriculum learning, Fluorescence microscopy dataset, Clustered nuclei segmentation},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87237-3_39,
author = {Wang, Zuhui and Yin, Zhaozheng},
title = {Annotation-Efficient Cell Counting},
year = {2021},
isbn = {978-3-030-87236-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87237-3_39},
doi = {10.1007/978-3-030-87237-3_39},
abstract = {Recent advances in deep learning have achieved impressive results on microscopy cell counting tasks. The success of deep learning models usually needs sufficient training data with manual annotations, which can be time-consuming and costly. In this paper, we propose an annotation-efficient cell counting approach which injects cell counting networks into an active learning framework. By designing a multi-task learning in the cell counter network model, we leverage unlabeled data for feature representation learning and use deep clustering to group unlabeled data. Rather than labeling every cell in each training image, the deep active learning only suggests the most uncertain, diverse, representative and rare image regions for annotation. Evaluated on four widely used cell counting datasets, our cell counter trained by a small subset of training data suggested by the deep active learning, achieves superior performance compared to state-of-the-arts with full training or other suggestive annotations. Our code is available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part VIII},
pages = {405–414},
numpages = {10},
keywords = {Active learning, Suggestive annotation, Cell counting},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-87237-3_25,
author = {Wagner, Sophia J. and Khalili, Nadieh and Sharma, Raghav and Boxberg, Melanie and Marr, Carsten and de Back, Walter and Peng, Tingying},
title = {Structure-Preserving Multi-domain Stain Color Augmentation Using Style-Transfer with Disentangled Representations},
year = {2021},
isbn = {978-3-030-87236-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87237-3_25},
doi = {10.1007/978-3-030-87237-3_25},
abstract = {In digital pathology, different staining procedures and scanners cause substantial color variations in whole-slide images (WSIs), especially across different laboratories. These color shifts result in a poor generalization of deep learning-based methods from the training domain to external pathology data. To increase test performance, stain normalization techniques are used to reduce the variance between training and test domain. Alternatively, color augmentation can be applied during training leading to a more robust model without the extra step of color normalization at test time. We propose a novel color augmentation technique, HistAuGAN, that can simulate a wide variety of realistic histology stain colors, thus making neural networks stain-invariant when applied during training. Based on a generative adversarial network (GAN) for image-to-image translation, our model disentangles the content of the image, i.e., the morphological tissue structure, from the stain color attributes. It can be trained on multiple domains and, therefore, learns to cover different stain colors as well as other domain-specific variations introduced in the slide preparation and imaging process. We demonstrate that HistAuGAN outperforms conventional color augmentation techniques on a classification task on the publicly available dataset Camelyon17 and show that it is able to mitigate present batch effects (Code and model weights are available at .).},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part VIII},
pages = {257–266},
numpages = {10},
keywords = {Disentangled representations, Style-transfer, Color augmentation},
location = {Strasbourg, France}
}

@inproceedings{10.1109/ITSC48978.2021.9564943,
author = {Ma, Yining and Song, Shiyao and Zhang, Lingtong and Xiong, Lu and Chen, Junyi},
title = {Lane Change Analysis and Prediction Using Mean Impact Value Method and Logistic Regression Model},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ITSC48978.2021.9564943},
doi = {10.1109/ITSC48978.2021.9564943},
abstract = {The analysis and estimation of lane change (LC) behavior are essential for autonomous vehicles (AVs) to predict other vehicles' intentions and avoid accidents. Since the LC intention is easily affected by various features, the feature selection and LC modeling greatly influence the prediction accuracy and interpretability. Therefore, a binary logistic regression LC model with a mean impact value (MIV) method to select features is proposed for accurate prediction. First, the related features are classified as individual, microscopic, and macroscopic levels. Then they are ranked and analyzed by the MIV method. Next, the closely related features are selected and used as input to the logistic regression model for LC intention prediction. As a result, a highly interpretable LC model is built with a prediction performance of around 80%. This paper benefits the quantification and explanation of the influences of different levels' features on LC intention and lays a solid foundation for the AVs to predict the LC behavior.},
booktitle = {2021 IEEE International Intelligent Transportation Systems Conference (ITSC)},
pages = {1346–1352},
numpages = {7},
location = {Indianapolis, IN, USA}
}

@inproceedings{10.1109/ITSC48978.2021.9564745,
author = {Ruan, Pingbo and Wu, Guoyuan and Wei, Zhensong and Barth, Matthew J.},
title = {A Modularized Electric Vehicle Model-in-the-Loop Simulation for Transportation Electrification Modeling and Analysis},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ITSC48978.2021.9564745},
doi = {10.1109/ITSC48978.2021.9564745},
abstract = {With the increasing number of electric vehicles (EVs), it is necessary to take their effects into account when modeling and analyzing the transportation system. Due to the unique characteristics of powertrains, not only the energy consumption model but also vehicle dynamics of EVs are quite different from internal combustion engine vehicles (ICEVs). In this paper, a modularized EV model (for both vehicle dynamics and energy consumption) is first developed using high resolution real-world data, and then integrated into the Simulation for Urban Mobility (SUMO), an open-sourced microscopic traffic simulator. The proposed model-in-the-loop simulation approach can well balance the EV model fidelity (e.g., motor efficiency map) and computational load for traffic simulation with decent scale. Based on this approach, energy impacts of two scenarios: a) EV mass adoption; and b) EV eco-driving, are evaluated as example studies. Results indicate that the differences between energy estimation from our proposed approach and that from SUMO default models may be as high as 42.5% and 5.2%, respectively, for these two scenarios.},
booktitle = {2021 IEEE International Intelligent Transportation Systems Conference (ITSC)},
pages = {1685–1690},
numpages = {6},
location = {Indianapolis, IN, USA}
}

@inproceedings{10.1145/3451421.3451438,
author = {Sun, Changhao and Li, Chen and Xu, Hao and Zhang, Jinghua and Ai, Shiliang and Zhou, Xiaomin and Li, Xiaoyan},
title = {A Comparison of Segmentation Methods in Gastric Histopathology Images},
year = {2021},
isbn = {9781450389686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451421.3451438},
doi = {10.1145/3451421.3451438},
abstract = {Gastric Cancer is one of the five most common types of malignant tumors among men and women worldwide and it is very important to make precise diagnosis for the early stage of gastric cancer. In this paper, we compare eight methods in Gastric Histopathology Image Segmentation (GHIS) including most classical and state-of-the-art ones. For estimating the segmentation result, we use seven evaluation indexes. Our study carries out that deep learning method shows the effectiveness in GHIS and the DenseCRF using the U-Net feature map performs best overall.},
booktitle = {The Fourth International Symposium on Image Computing and Digital Medicine},
pages = {75–79},
numpages = {5},
keywords = {Machine Learning, Image Segmentation, Histopathology Image, Gastric Cancer},
location = {Shenyang, China},
series = {ISICDM 2020}
}

@inproceedings{10.1109/AIM46487.2021.9517484,
author = {Hoogesteger, M.M. and Sadeghian, H. and Nijmeijer, H.},
title = {The Sensitivity of Subsurface Contact Resonance Atomic Force Microscopy to Changes in the Depth of Buried Features: a Nonlinear Approach},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AIM46487.2021.9517484},
doi = {10.1109/AIM46487.2021.9517484},
abstract = {The development of non-destructive subsurface imaging methods has been a major research topic in the past two decades. One of these is the use of Contact Resonance Atomic Force Microscopy (CR-AFM) to measure changes in contact stiffness to detect buried features. The depth of the buried feature has a large influence (or is a limiting factor) on the sensitivity and resolution of this method, which has been shown in theoretical, numerical and experimental research. However, the sensitivity of the method to feature depth itself has not yet been thoroughly investigated. This sensitivity is the first step towards a method capable of measuring the depth of features in addition to detecting them. In this work, the influence of depth on the probe dynamics is investigated using both a linear and nonlinear model and simulation based results of the two are compared. It is argued that a nonlinear approach is essential in estimating the potential performance of subsurface measurements using CRAFM methods.},
booktitle = {2021 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)},
pages = {110–115},
numpages = {6},
location = {Delft, Netherlands}
}

@inproceedings{10.1145/3457682.3457733,
author = {Li, Yixin and Wu, Xinran and Li, Chen and Sun, Changhao and Li, Xiaoyan and Rahaman, Md and Zhang, Yong},
title = {Intelligent Gastric Histopathology Image Classification Using Hierarchical Conditional Random Field based Attention Mechanism},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457733},
doi = {10.1145/3457682.3457733},
abstract = {In this paper, an Intelligent Hierarchical Conditional Random Field based Attention Mechanism (HCRF-AM) model is proposed, which can be applied to the Gastric Histopathology Image Classification (GHIC) tasks to assist pathologists in medical diagnosis. However, there exists redundant information in a weakly supervised learning mission. Thus, designing the network that can extract effective distinguishing features has become the focus of research. The HCRF-AM model consists of attention mechanism (AM) module and image classification (IC) module. First, in the AM module, an HCRF model is built to extract attention areas. Then, a convolutional neural network (CNN) model is trained with the attention region selected. Thirdly, an algorithm called classification probability based Ensemble Learning (EL) is used to obtain the image-level result from patch-level output of the CNN. In the experiment, a classification specificity of 96.67% is achieved on a gastric histopathological dataset with 700 images.},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {330–335},
numpages = {6},
keywords = {Image Classification, Histopathology Image, Gastric Cancer, Conditional Random Field, Attention Mechanism},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@inproceedings{10.1109/I2MTC50364.2021.9460048,
author = {Sun, Chang and Wang, Yanqiu and Yue, Shihong and Chen, Jun and Li, Qi},
title = {Prediction of ki-67 expression level based on non-small cell lung cancer},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/I2MTC50364.2021.9460048},
doi = {10.1109/I2MTC50364.2021.9460048},
abstract = {The tumor immunohistochemical marker ki-67 is an important marker of cell proliferation, and an important prognostic factor for non-small cell lung cancer (NSCLC). CT radiomics features can reflect potential histopathological changes without trauma. The lesions are segmented by a radiomics physician, the 3D reconstructions are performed and 107 radiomics features are extracted. The correlation between CT radiomics features based on NSCLC and ki-67 is analyzed and 45 features are selected. Then we perform factor analysis to further select features. Significantly correlated with ki-67 expression level, nine features are selected. Finally, we use support vector machines (SVM) optimized by particle swarm optimization (PSO) to build a prediction classifier. SVM has a fast convergence speed and strong resistance to overfitting. Meanwhile, PSO has a strong global search capability. The prediction accuracy of ki-67 expression level conducted by PSO-SVM classifier is 91.30%.},
booktitle = {2021 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)},
pages = {1–6},
numpages = {6},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3411763.3451770,
author = {Kim, Raphael},
title = {Virus as Quasi-Living Bio-Material for Interaction Design: Practical, Ethical, and Philosophical Implications},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451770},
doi = {10.1145/3411763.3451770},
abstract = {The interaction design research community continues to benefit from material-focused approaches, and from the diversity of materials under investigation. One category of such material is bio-materials of microbial origin, such as bacteria, mycelium, moulds, and Euglena. However, despite the increasing momentum towards bio-material based research, one type that is yet to be investigated in HCI, is viruses; an infectious, sub-microscopic, quasi-living, computational bio-agent. This paper initiates exploration of Human-Virus Interaction (HVI), through a material lens. This was achieved first by generating a literature-based material profile sketch of viruses, highlighting some of their distinct and/or unique material properties, characteristics, composition, and meaning. The components of the profile were then used as anchor points, to unpack the practical, ethical, and philosophical implications that are associated with viruses, and those that could be considered by researchers to help in their preparation of working with viruses in interaction design.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {321},
numpages = {7},
keywords = {Virus, Virology, Pathogens, Materiality, Human-Virus Interaction, Biological-HCI, Bio-materials, Bio-Digital},
location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
series = {CHI EA '21}
}

@inproceedings{10.1145/3453187.3453366,
author = {He, Zhanwen and Chen, Jibing},
title = {Research on Microstructure and Morphology of Non-stoichiometric ratio of TiCx},
year = {2021},
isbn = {9781450389099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453187.3453366},
doi = {10.1145/3453187.3453366},
abstract = {TiC possesses many excellent physical and chemical properties, Nevertheless, the high melting point and difficulty for sintering limit its application. In this experiment, the non-stoichiometric TiC (TiCx) powders are fabricated by means of mechanical alloying (MA) synthesis method, in addition, atoms diffuse use in the region of two phases by High pressure sintering, which can reduce the sintering temperature to optimize properties of bulk materials. In the experiment Ti powder and TiC powder in accordance with Ti, C molar mass ratio 1:0.3, 1:0.4, 1:0.5 (respective code TC3, TC4, TC5) synthetic non-stoichiometric TiCx by Mechanical alloying (MA). The X-ray Diffraction (XRD) and a scanning electron microscope (SEM) are used to analyze the structures and observe the morphologies of the milled powders. The results of the X-Ray Diffraction (XRD) analysis showed that the diffraction peaks of Ti disappeared, but amorphous structure appeared after milling for 3 hours. The Ti peak disappeared completely after milling for 6 hours and the grain size is up to 8nm by milling 21 hours. The SEM fracture morphologies of TiC by MA after different time showed that the particle size reached 50-100 nm. At the same time, The particle size distribution is uniform and the agglomeration phenomenon is weakened},
booktitle = {Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science},
pages = {394–397},
numpages = {4},
keywords = {Non-stoichiometric Titanium, Mechanical alloying, Interface diffusion, Carbide},
location = {Wuhan, China},
series = {EBIMCS '20}
}

@inproceedings{10.1145/3439706.3446898,
author = {Scheffer, Louis K.},
title = {The Physical Design of Biological Systems - Insights from the Fly Brain},
year = {2021},
isbn = {9781450383004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439706.3446898},
doi = {10.1145/3439706.3446898},
abstract = {Many different physical substrates can support complex computation. This is particularly apparent when considering human made and biological systems that perform similar functions, such as visually guided navigation. In common, however, is the need for good physical design, as such designs are smaller, faster, lighter, and lower power, factors in both the jungle and the marketplace. Although the physical design of man-made systems is relatively well understood, the physical design of biological computation has remained murky due to a lack of detailed information on their construction. The recent EM (electron microscope) reconstruction of the central brain of the fruit fly now allows us to start to examine these issues. Here we look at the physical design of the fly brain, including such factors as fan-in and fanout, logic depth, division into physical compartments and how this affects electrical response, pin to computation ratios (Rent's rule), and other physical characteristics of at least one biological computation substrate. From this we speculate on how physical design algorithms might change if the target implementation was a biological neural network.},
booktitle = {Proceedings of the 2021 International Symposium on Physical Design},
pages = {101–108},
numpages = {8},
keywords = {physical design, neuron operation, brain architecture},
location = {Virtual Event, USA},
series = {ISPD '21}
}

@inproceedings{10.1109/IRPS46558.2021.9405106,
author = {Wu, Zhicheng and Franco, Jacopo and Truijen, Brecht and Roussel, Philippe and Tyaginov, Stanislav and Vandemaele, Michiel and Bury, Erik and Groeseneken, Guido and Linten, Dimitri and Kaczer, Ben},
title = {Physics-based device aging modelling framework for accurate circuit reliability assessment},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IRPS46558.2021.9405106},
doi = {10.1109/IRPS46558.2021.9405106},
abstract = {An analytical device aging modelling framework, ranging from microscopic degradation physics up to the aged I-V characteristics, is demonstrated. We first expand our reliability oriented I-V compact model, now including temperature and body-bias effects; second, we propose an analytical solution for channel carrier profiling which-compared to our previous work-circumvents the need of TCAD aid; third, through Poisson's equation, we convert the extracted carrier density profile into channel lateral and oxide electric fields; fourth, we represent the device as an equivalent ballistic MOSFETs chain to enable channel “slicing” and propagate local degradation into the aged I-V characteristics, without requiring computationally-intensive self-consistent calculations. The local degradation in each channel “slice” is calculated with physics-based reliability models (2-state NMP, SVE/MVE). The demonstrated aging modelling framework is verified against TCAD and validated across a broad range of V&lt;inf&gt;G&lt;/inf&gt;/V&lt;inf&gt;D&lt;/inf&gt;/T stress conditions in a scaled finFET technology.},
booktitle = {2021 IEEE International Reliability Physics Symposium (IRPS)},
pages = {1–6},
numpages = {6},
location = {Monterey, CA, USA}
}

@inproceedings{10.1145/3440067.3440079,
author = {Kostromina, Elena and Eremin, Petr and Kondratev, Denis and Veremeev, Alexey and Gilmutdinova, Ilmira},
title = {Characterisation of the cell product obtained with the ‘ESVIEF System’ kit for isolation of stromal vascular fraction from human adipose tissue},
year = {2021},
isbn = {9781450388139},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440067.3440079},
doi = {10.1145/3440067.3440079},
abstract = {Production technology and characteristics of the cell product obtained using the ‘ESVIEF System’ kit (developed by JoinTechCell LLC, Russian Federation) for isolation of a stromal vascular fraction from human adipose tissue are described. The use of subcutaneous fat as a source of stem and progenitor cells for regenerative medicine has become widespread during the last decade. The main advantage of using adipose tissue as a source of stem cells when compared with bone marrow is the lower invasiveness of the material sampling procedure during liposuction in addition to the significantly larger number of multipotent mesenchymal stromal cells obtained per unit of tissue volume. The development and implementation of devices for automation and standardisation of stem cell isolation from adipose tissue are important for the widespread use of stem cells in clinical practice. This work aimed to evaluate the effectiveness and safety of the cell production technology using the ‘ESVIEF System’ kit for isolation of a stromal vascular fraction from human adipose tissue. Adipose tissue samples obtained from patients during liposuction were used as clinical material. The obtained cell fractions were studied using microscopy, flow cytometry and cell culture methods. The viability of the stromal vascular fraction cells (nucleated) was 90.9 ± 0.3% with a total number of 0.81 ± 0.08 \texttimes{} 106 cells/ml of adipose tissue. The study showed that the ‘ESVIEF System’ kit that was developed for isolating stromal vascular fractions from human adipose tissue is a promising and safe technology for producing cell products.},
booktitle = {Proceedings of the 7th International Conference on Bioinformatics Research and Applications},
pages = {66–69},
numpages = {4},
keywords = {stromal vascular fraction, regenerative medicine, cell product, adipose tissue, Stem cell technology},
location = {Berlin, Germany},
series = {ICBRA '20}
}

@inproceedings{10.1145/3438872.3439114,
author = {Lu, Chunchi and Wang, Ying and Ren, Ya},
title = {Synergistic effect of mixed cathode materials on lithium-ion batteries},
year = {2020},
isbn = {9781450388306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3438872.3439114},
doi = {10.1145/3438872.3439114},
abstract = {During the charging and discharging of lithium-ion batteries, the internal reaction of the battery changes in temperature, because the thermal stability of the ternary cathode material is not stable, it affects the normal operation of the battery and also affects the available capacity of the battery. Many researchers have mixed two positive electrode materials to prepare electrodes and then improved the rate performance and polarization of the electrodes. The active material of the mixed electrode is mainly made by physical mixing of two or more lithium intercalation compounds. Compared with a single active material, the purpose of a mixed electrode is to balance the performance of two or more active materials. Taking the ternary material as the research object, the modification method is to use the grinding method to mix and stir the lithium iron phosphate (LiFePO4) and the ternary cathode material LiNi1/3co1/3Mn1/3 (NCM) with an appropriate mass ratio to the electrode plates to assemble a lithium ion battery and then explore their various characteristics. The mixed material electrodes uses field emission scanning electron microscopy, infrared spectroscopy and X-ray diffraction to characterize the surface structure, chemical composition, then uses blue electricity test equipment to measure the electrochemical performance of the mixed cathode material, test cycle performance and rate, and use electrochemical workstation to test the volt-ampere curve and electrochemical impedance test of the lithium-ion battery to study the capacity reversibility and electrochemical polarization degree of the mixed cathode material.},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {398–402},
numpages = {5},
keywords = {Ternary cathode material, Mixed electrode, Lithium-ion battery},
location = {Shanghai, China},
series = {RICAI '20}
}

@inproceedings{10.5555/3495724.3497272,
author = {Pielawski, Nicolas and Wetzer, Elisabeth and \"{O}fverstedt, Johan and Lu, Jiahao and W\"{a}hlby, Carolina and Lindblad, Joakim and Sladoje, Nata\v{s}a},
title = {CoMIR: contrastive multimodal image representation for registration},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose contrastive coding to learn shared, dense image representations, referred to as CoMIRs (Contrastive Multimodal Image Representations). CoMIRs enable the registration of multimodal images where existing registration methods often fail due to a lack of sufficiently similar image structures. CoMIRs reduce the multimodal registration problem to a monomodal one, in which general intensity-based, as well as feature-based, registration algorithms can be applied. The method involves training one neural network per modality on aligned images, using a contrastive loss based on noise-contrastive estimation (InfoNCE). Unlike other contrastive coding methods, used for, e.g., classification, our approach generates image-like representations that contain the information shared between modalities. We introduce a novel, hyperparameter-free modification to InfoNCE, to enforce rotational equivariance of the learnt representations, a property essential to the registration task. We assess the extent of achieved rotational equivariance and the stability of the representations with respect to weight initialization, training set, and hyperparameter settings, on a remote sensing dataset of RGB and near-infrared images. We evaluate the learnt representations through registration of a biomedical dataset of bright-field and second-harmonic generation microscopy images; two modalities with very little apparent correlation. The proposed approach based on CoMIRs significantly outperforms registration of representations created by GAN-based image-to-image translation, as well as a state-of-the-art, application-specific method which takes additional knowledge about the data into account.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1548},
numpages = {12},
location = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
series = {NIPS '20}
}

@inproceedings{10.1145/3433996.3434488,
author = {Feng, Jiaojiao and Li, Ming and Wu, Yongfei and Li, Xinyu and Zhou, Xiaoshuang},
title = {Urinary Red Blood Cells Extraction Based on Multiple Instance Learning and Watershed Segmentation},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434488},
doi = {10.1145/3433996.3434488},
abstract = {Doctors can diagnose many diseases by urinary red blood cells morphological analysis, and cell extraction is an important step in morphological analysis, which can help the subsequent classification of cells. However, it is time-consuming and inefficient for doctors to observe cells under a microscope, which easily leads to misdiagnosis and missed diagnosis. This paper proposes a method of cell extraction: firstly, the weak supervised multiple instance learning (MIL) is combined with resnet for cell localization, and the probability value of each pixel in urinary red blood cell image can be obtained through neural network, we set threshold value for probability value to obtain mask, finally, a marker-based watershed segmentation method was used to extract single urinary red blood cells. The experiment showed that the model achieved good results, with this method, doctors can directly analyze segmented cells, which can improve the efficiency of analysis and reduce the workload.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {281–285},
numpages = {5},
keywords = {Urinary Red Blood Cells, Multiple Instance Learning, Marker-based Watershed Segmentation, Cell Extraction},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/3433996.3434037,
author = {Li, Keshu and Li, Ming and Wu, Yongfei and Li, Xinyu and Zhou, Xiaoshuang},
title = {An Accurate Urine Erythrocytes Detection Model Coupled Faster RCNN with VggNet},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434037},
doi = {10.1145/3433996.3434037},
abstract = {In the process of kidney disease diagnosis, it is particularly important to screen the morphology of erythrocytes in urine. Because of the small and similar morphology of erythrocytes in urine, manual labeling and observation is time-consuming and labor-consuming. Deep learning can not only quickly and accurately locate urine erythrocytes, but also effectively identify the types of cells. In this paper, we employ the Faster RCNN combined with VggNet to detect urine erythrocytes. We collected images of urine erythrocytes under optical microscope and used data enhancement method to obtain total 3969 images of urine erythrocytes, which filled in the blank of domestic urine erythrocytes data sets. Experimental results show that the presented method can identify five kinds of urine erythrocytes and the recall rate achieve up to 99.8%, which greatly helped doctors better identify and judge urine erythrocytes.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {224–228},
numpages = {5},
keywords = {VGGNET, Urine erythrocytes, Morphological recognition, Faster RCNN},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/3429889.3429922,
author = {Xiao, Yuan and Yun, Weibo and Zhang, Chengkun},
title = {Fabric-based ECG Electorde Jet Printing Formation and Performance Test},
year = {2020},
isbn = {9781450388603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429889.3429922},
doi = {10.1145/3429889.3429922},
abstract = {Given the complicated process and high cost in the fabric electrode preparation process, this paper uses a combination of droplet jet technology and chemical deposition technology to directly print the conductive layer on the surface of the fabric to realize the preparation of the fabric electrode. The microscopic morphology and conductivity of the conductive layer before packaging are studied, and the AC impedance test of the fabric electrode after packaging and the acquisition of human ECG signals are carried out. The results show that the average diameter of the silver particles in the conductive layer of the fabric electrode is about 15 μ m, the thickness is about 100 μ m, and the conductivity of the conductive layer is 8.658 \texttimes{} 105 S/m. The trend of the AC impedance of the fabric electrode is similar to that of the standard Ag-AgCl electrode, and the impedance of the plain weave fabric electrode is lower than the standard gel Ag-AgCl electrode at 10~45 Hz; the collected ECG signal is similar to the R wave amplitude measured by the standard Ag-AgCl electrode, and the ECG signal has a clear spectral composition, which verifies the feasibility of the fabric electrode to collect ECG signal.},
booktitle = {Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences},
pages = {171–176},
numpages = {6},
keywords = {Jet printing, Fabric electrode, ECG},
location = {Beijing, China},
series = {ISAIMS '20}
}

@inproceedings{10.1007/978-3-030-50120-4_16,
author = {Aggrawal, Hari Om and Andersen, Martin S. and Modersitzki, Jan},
title = {An Image Registration Framework for Discontinuous Mappings Along Cracks},
year = {2020},
isbn = {978-3-030-50119-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-50120-4_16},
doi = {10.1007/978-3-030-50120-4_16},
abstract = {A novel crack capable image registration framework is proposed. The approach is designed for registration problems suffering from cracks, gaps, or holes. The approach enables discontinuous transformation fields and also features an automatically computed crack indicator function and therefore does not require a pre-segmentation. The new approach is a generalization of the commonly used variational image registration approach. New contributions are an additional dissipation term in the overall energy, a proper balancing of different ingredients, and a joint optimization for both, the crack indicator function and the transformation. Results for histological serial sectioning of marmoset brain images demonstrate the potential of the approach and its superiority as compared to a standard registration.},
booktitle = {Biomedical Image Registration: 9th International Workshop, WBIR 2020, Portoro\v{z}, Slovenia, December 1–2, 2020, Proceedings},
pages = {163–173},
numpages = {11},
keywords = {Histology, Digital pathology, Holes, Cracks, Image registration, Non-smooth deformations, Discontinuous deformations},
location = {Portoro\v{z}, Slovenia}
}

@inproceedings{10.1007/978-3-030-63836-8_1,
author = {Panta, Adhish and Khushi, Matloob and Naseem, Usman and Kennedy, Paul and Catchpoole, Daniel},
title = {Classification of Neuroblastoma Histopathological Images Using Machine Learning},
year = {2020},
isbn = {978-3-030-63835-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63836-8_1},
doi = {10.1007/978-3-030-63836-8_1},
abstract = {Neuroblastoma is the most common cancer in young children accounting for over 15% of deaths in children due to cancer. Identification of the class of neuroblastoma is dependent on histopathological classification performed by pathologists which are considered the gold standard. However, due to the heterogeneous nature of neuroblast tumours, the human eye can miss critical visual features in histopathology. Hence, the use of computer-based models can assist pathologists in classification through mathematical analysis. There is no publicly available dataset containing neuroblastoma histopathological images. So, this study uses dataset gathered from The Tumour Bank at Kids Research at The Children’s Hospital at Westmead, which has been used in previous research. Previous work on this dataset has shown maximum accuracy of 84%. One main issue that previous research fails to address is the class imbalance problem that exists in the dataset as one class represents over 50% of the samples. This study explores a range of feature extraction and data undersampling and over-sampling techniques to improve classification accuracy. Using these methods, this study was able to achieve accuracy of over 90% in the dataset. Moreover, significant improvements observed in this study were in the minority classes where previous work failed to achieve high level of classification accuracy. In doing so, this study shows importance of effective management of available data for any application of machine learning.},
booktitle = {Neural Information Processing: 27th International Conference, ICONIP 2020, Bangkok, Thailand, November 23–27, 2020, Proceedings, Part III},
pages = {3–14},
numpages = {12},
location = {<conf-loc content-type="InPerson">Bangkok, Thailand</conf-loc>}
}

@inproceedings{10.1145/3388440.3412430,
author = {Giuste, Felipe and Venkatesan, Mythreye and Zhao, Conan and Tong, Li and Zhu, Yuanda and Deshpande, Shriprasad R. and Wang, May D.},
title = {Automated Classification of Acute Rejection from Endomyocardial Biopsies},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3412430},
doi = {10.1145/3388440.3412430},
abstract = {Heart transplant rejection must be quickly and accurately identified to optimize anti-rejection therapies and prevent organ loss. Expert evaluation of endomyocardial biopsies is labor-intensive, and prone to human bias, and suffers from low inter-rater agreement. Additionally, the increased utility of digital pathology for biopsy examination has exacerbated the need for additional image quality control. To meet these challenges, we developed a novel transplant rejection detection pipeline which automatically identifies histology slides in need of rescanning and highlights biopsy regions showing potential signs of rejection. Our system leverages a fast and effective automated patch-level quality filter as well as state-of-the-art feature extraction techniques to provide quality whole-slide level labeling of early rejection signs. We successfully identified digital pathology images with poor image quality and leveraged this quality gain to improve our novel weakly-supervised learning model leading to significant transplant rejection classification performance of AUC: 70.12 (±20.74) %.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {6},
numpages = {9},
keywords = {transplant rejection, heart transplant, digital pathology, deep learning, Histology},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1007/978-3-030-60365-6_7,
author = {Tam, Ka Ho and Sirinukunwattana, Korsuk and Soares, Maria F. and Kaisar, Maria and Ploeg, Rutger and Rittscher, Jens},
title = {Improving Pathological Distribution Measurements with Bayesian Uncertainty},
year = {2020},
isbn = {978-3-030-60364-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60365-6_7},
doi = {10.1007/978-3-030-60365-6_7},
abstract = {Deep learning assisted histopathology has the potential to extract reproducible and accurate measurements from digitised slides in a scalable fashion. A typical workflow of such analysis may involve instance segmentation of relevant tissues followed by feature measurements. Inherent segmentation uncertainties produced by these deep models, however, could propagate to the downstream measurements, causing biased distribution estimate of the whole slide. One challenging aspect when handling ambiguous tissues is that the number of instances could differ as the instance segmentation step may not generalise well to these tissues. As an attempt to address this problem, we propose to derive a confidence score from the segmentation uncertainties obtained from Bayesian Neural Networks (BNNs) and utilise these as weights to improve the distribution estimate. We generate a synthetic dataset that mimics the diverse and varying visual features of the original data to enable systematic experiments. With this dataset we demonstrate the robustness of the method by extracting several clinically relevant measurements with two different BNNs. Our results indicate that the distribution estimates are consistently improved when the instances are weighted by the entropy-derived confidence measure. In addition, we provide results on applying the method to the original data.},
booktitle = {Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, and Graphs in Biomedical Image Analysis: Second International Workshop, UNSURE 2020, and Third International Workshop, GRAIL 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 8, 2020, Proceedings},
pages = {61–70},
numpages = {10},
keywords = {Digital pathology, Uncertainty propagation, Bayesian Neural Network},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-66843-3_21,
author = {Booth, Thomas C. and Akpinar, Bernice and Roman, Andrei and Shuaib, Haris and Luis, Aysha and Chelliah, Alysha and Al Busaidi, Ayisha and Mirchandani, Ayesha and Alparslan, Burcu and Mansoor, Nina and Ashkan, Keyoumars and Ourselin, Sebastien and Modat, Marc},
title = {Machine Learning and Glioblastoma: Treatment Response Monitoring Biomarkers in 2021},
year = {2020},
isbn = {978-3-030-66842-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66843-3_21},
doi = {10.1007/978-3-030-66843-3_21},
abstract = {The aim of the systematic review was to assess recently published studies on diagnostic test accuracy of glioblastoma treatment response monitoring biomarkers in adults, developed through machine learning (ML). Articles published 09/2018–09/2020 were searched for using MEDLINE, EMBASE, and the Cochrane Register. Included study participants were adult patients with high grade glioma who had undergone standard treatment (maximal resection, radiotherapy with concomitant and adjuvant temozolomide) and subsequently underwent follow-up imaging to determine treatment response status (specifically, distinguishing progression/recurrence from progression/recurrence mimics - the target condition). Risk of bias and applicability was assessed with QUADAS 2 methodology. Contingency tables were created for hold-out test sets and recall, specificity, precision, F1-score, balanced accuracy calculated. Fifteen studies were included with 1038 patients in training sets and 233 in test sets. To determine whether there was progression or a mimic, the reference standard combination of follow-up imaging and histopathology at re-operation was applied in 67% (10/15) of studies. External hold-out test sets were used in 27% (4/15) to give ranges of diagnostic accuracy measures: recall = 0.70–1.00; specificity = 0.67–0.90; precision = 0.78–0.88; F1 score = 0.74–0.94; balanced accuracy = 0.74–0.83; AUC = 0.80–0.85. The small numbers of patient included in studies, the high risk of bias and concerns of applicability in the study designs (particularly in relation to the reference standard and patient selection due to confounding), and the low level of evidence, suggest that limited conclusions can be drawn from the data. There is likely good diagnostic performance of machine learning models that use MRI features to distinguish between progression and mimics. The diagnostic performance of ML using implicit features did not appear to be superior to ML using explicit features. There are a range of ML-based solutions poised to become treatment response monitoring biomarkers for glioblastoma. To achieve this, the development and validation of ML models require large, well-annotated datasets where the potential for confounding in the study design has been carefully considered. Therefore, multidisciplinary efforts and multicentre collaborations are necessary.},
booktitle = {Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-Oncology: Third International Workshop, MLCN 2020, and Second International Workshop, RNO-AI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {212–228},
numpages = {17},
keywords = {Biomarkers, Diagnostic monitoring, Machine learning, Neuro-oncology},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59728-3_35,
author = {Huynh, Khoi Minh and Wu, Ye and Thung, Kim-Han and Ahmad, Sahar and Taylor IV, Hoyt Patrick and Shen, Dinggang and Yap, Pew-Thian},
title = {Characterizing Intra-soma Diffusion with Spherical Mean Spectrum Imaging},
year = {2020},
isbn = {978-3-030-59727-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59728-3_35},
doi = {10.1007/978-3-030-59728-3_35},
abstract = {Most brain microstructure models are dedicated to the quantification of white matter microstructure, using for example sticks, cylinders, and zeppelins to model intra- and extra-axonal environments. Gray matter presents unique micro-architecture with cell bodies (somas) exhibiting diffusion characteristics that differ from axons in white matter. In this paper, we introduce a method to quantify soma microstructure, giving measures such as volume fraction, diffusivity, and kurtosis. Our method captures a spectrum of diffusion patterns and scales and does not rely on restrictive model assumptions. We show that our method yields unique and meaningful contrasts that are in agreement with histological data. We demonstrate its application in the mapping of the distinct spatial patterns of soma density in the cortex.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part VII},
pages = {354–363},
numpages = {10},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59722-1_9,
author = {Li, Hanyu and Januszewski, Micha\l{} and Jain, Viren and Li, Peter H.},
title = {Neuronal Subcompartment Classification and Merge Error Correction},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_9},
doi = {10.1007/978-3-030-59722-1_9},
abstract = {Recent advances in 3d electron microscopy are yielding ever larger reconstructions of brain tissue, encompassing thousands of individual neurons interconnected by millions of synapses. Interpreting reconstructions at this scale demands advances in the automated analysis of neuronal morphologies, for example by identifying morphological and functional subcompartments within neurons. We present a method that for the first time uses full 3d input (voxels) to automatically classify reconstructed neuron fragments as axon, dendrite, or somal subcompartments. Based on 3d convolutional neural networks, this method achieves a mean f1-score of 0.972, exceeding the previous state of the art of 0.955. The resulting predictions can support multiple analysis and proofreading applications. In particular, we leverage finely localized subcompartment predictions for automated detection and correction of merge errors in the volume reconstruction, successfully detecting 90.6% of inter-class merge errors with a false positive rate of only 2.7%.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {88–98},
numpages = {11},
keywords = {Merge error, 3d neural network, Connectomics},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59722-1_55,
author = {Lara, Juan S. and Contreras O., Victor H. and Ot\'{a}lora, Sebasti\'{a}n and M\"{u}ller, Henning and Gonz\'{a}lez, Fabio A.},
title = {Multimodal Latent Semantic Alignment for Automated Prostate Tissue Classification and Retrieval},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_55},
doi = {10.1007/978-3-030-59722-1_55},
abstract = {This paper presents an information fusion method for the automatic classification and retrieval of prostate histopathology whole-slide images (WSIs). The approach employs a weakly-supervised machine learning model that combines a bag-of-features representation, kernel methods, and deep learning. The primary purpose of the method is to incorporate text information during the model training to enrich the representation of the images. It automatically learns an alignment of the visual and textual space since each modality has different statistical properties. This alignment enriches the visual representation with complementary semantic information extracted from the text modality. The method was evaluated in both classification and retrieval tasks over a dataset of 235 prostate WSIs with their pathology report from the TCGA-PRAD dataset. The results show that the multimodal-enhanced model outperform unimodal models in both classification and retrieval. It outperforms state–of–the–art baselines by an improvement in WSI cancer detection of 4.74% achieving 77.01% in accuracy, and an improvement of 19.35% for the task of retrieving similar cases, obtaining 64.50% in mean average precision.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {572–581},
numpages = {10},
keywords = {Prostate cancer, Histopathology images, Multimodal fusion},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59722-1_50,
author = {Chikontwe, Philip and Kim, Meejeong and Nam, Soo Jeong and Go, Heounjeong and Park, Sang Hyun},
title = {Multiple Instance Learning with Center Embeddings for Histopathology Classification},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_50},
doi = {10.1007/978-3-030-59722-1_50},
abstract = {Histopathology image analysis plays an important role in the treatment and diagnosis of cancer. However, analysis of whole slide images (WSI) with deep learning is challenging given that the duration of pixel-level annotations is laborious and time consuming. To address this, recent methods have considered WSI classification as a Multiple Instance Learning (MIL) problem often with a multi-stage process for learning instance and slide level features. Currently, most methods focus on either instance-selection or instance prediction-aggregation that often fails to generalize and ignores instance relations. In this work, we propose a MIL-based method to jointly learn both instance- and bag-level embeddings in a single framework. In addition, we propose a center loss that maps embeddings of instances from the same bag to a single centroid and reduces intra-class variations. Consequently, our model can accurately predict instance labels and leverages robust hierarchical pooling of features to obtain bag-level features without sacrificing accuracy. Experimental results on curated colon datasets show the effectiveness of the proposed methods against recent state-of-the-art methods.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {519–528},
numpages = {10},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59722-1_40,
author = {Xie, Yutong and Zhang, Jianpeng and Liao, Zhibin and Verjans, Johan and Shen, Chunhua and Xia, Yong},
title = {Pairwise Relation Learning for Semi-supervised Gland Segmentation},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_40},
doi = {10.1007/978-3-030-59722-1_40},
abstract = {Accurate and automated gland segmentation on histology tissue images is an essential but challenging task in the computer-aided diagnosis of adenocarcinoma. Despite their prevalence, deep learning models always require a myriad number of densely annotated training images, which are difficult to obtain due to extensive labor and associated expert costs related to histology image annotations. In this paper, we propose the pairwise relation-based semi-supervised (PRS2) model for gland segmentation on histology images. This model consists of a segmentation network (S-Net) and a pairwise relation network (PR-Net). The S-Net is trained on labeled data for segmentation, and PR-Net is trained on both labeled and unlabeled data in an unsupervised way to enhance its image representation ability via exploiting the semantic consistency between each pair of images in the feature space. Since both networks share their encoders, the image representation ability learned by PR-Net can be transferred to S-Net to improve its segmentation performance. We also design the object-level Dice loss to address the issues caused by touching glands and combine it with other two loss functions for S-Net. We evaluated our model against five recent methods on the GlaS dataset and three recent methods on the CRAG dataset. Our results not only demonstrate the effectiveness of the proposed PR-Net and object-level Dice loss, but also indicate that our PRS2 model achieves the state-of-the-art gland segmentation performance on both benchmarks.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {417–427},
numpages = {11},
keywords = {Pairwise relation learning, Semi-supervised learning, Gland segmentation},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59722-1_3,
author = {Leahy, Brian D. and Jang, Won-Dong and Yang, Helen Y. and Struyven, Robbert and Wei, Donglai and Sun, Zhe and Lee, Kylie R. and Royston, Charlotte and Cam, Liz and Kalma, Yael and Azem, Foad and Ben-Yosef, Dalit and Pfister, Hanspeter and Needleman, Daniel},
title = {Automated Measurements of Key Morphological Features of Human Embryos for IVF},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_3},
doi = {10.1007/978-3-030-59722-1_3},
abstract = {A major challenge in clinical In-Vitro Fertilization (IVF) is selecting the highest quality embryo to transfer to the patient in the hopes of achieving a pregnancy. Time-lapse microscopy provides clinicians with a wealth of information for selecting embryos. However, the resulting movies of embryos are currently analyzed manually, which is time consuming and subjective. Here, we automate feature extraction of time-lapse microscopy of human embryos with a machine-learning pipeline of five convolutional neural networks (CNNs). Our pipeline consists of (1) semantic segmentation of the regions of the embryo, (2) regression predictions of fragment severity, (3) classification of the developmental stage, and object instance segmentation of (4) cells and (5) pronuclei. Our approach greatly speeds up the measurement of quantitative, biologically relevant features that may aid in embryo selection.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {25–35},
numpages = {11},
keywords = {In-vitro fertilization, Human embryos, Deep learning},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59722-1_19,
author = {Dong, Meng and Liu, Dong and Xiong, Zhiwei and Chen, Xuejin and Zhang, Yueyi and Zha, Zheng-Jun and Bi, Guoqiang and Wu, Feng},
title = {Towards Neuron Segmentation from Macaque Brain Images: A Weakly Supervised Approach},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_19},
doi = {10.1007/978-3-030-59722-1_19},
abstract = {The advance of microscopic imaging technology has enabled the observation of primate brain in its entirety and at single-neuron resolution. It is then an urgent need to develop means for automated analyses of these brain images, e.g. neuron segmentation. Deep learning is demonstrated an appealing approach for segmentation of natural images, but the success of deep learning is highly dependent on the large-scale and well-built training data that are costly to collect. In this paper, we take a step towards the goal of neuron segmentation from primate brain images, using a weakly supervised approach. We build – to our best knowledge – the first dual-channel three-dimensional image dataset of macaque brain for neuron segmentation. We propose two kinds of “weak” labels, i.e. central points and rough masks, to prepare training data with an affordable cost. Accordingly, we design a weakly supervised learning method for neuron instance segmentation where instances can be easily extracted from the predicted peak-shape probability maps. Experimental results have shown the effectiveness of our approach. We also verify the efficiency of the proposed method on a public nuclei dataset. Our dataset and code have been published at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {194–203},
numpages = {10},
keywords = {Weakly supervised learning, Macaque brain, Instance segmentation},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59722-1_14,
author = {Li, Jiabing and Artur, Camille and Eriksen, Jason and Roysam, Badrinath and Mayerich, David},
title = {Segmenting Continuous but Sparsely-Labeled Structures in Super-Resolution Microscopy Using Perceptual Grouping},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_14},
doi = {10.1007/978-3-030-59722-1_14},
abstract = {Super Resolution (SR) microscopy leverages a variety of optical and computational techniques for overcoming the optical diffraction limit to acquire additional spatial details. However, added spatial details challenge existing segmentation tools. Confounding features include protein distributions that form membranes and boundaries, such as cellular and nuclear surfaces. We present a segmentation pipeline that retains the benefits provided by SR in surface separation while providing a tensor field to overcome these confounding features. The proposed technique leverages perceptual grouping to generate a tensor field that enables robust evolution of active contours despite ill-defined membrane boundaries.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {141–150},
numpages = {10},
keywords = {Contours, Tensors, Perceptual grouping, Segmentation, Super resolution},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59722-1_11,
author = {Drenkow, Nathan and Joyce, Justin and Matelsky, Jordan and Heiko, Jennifer and Larabi, Reem and Wester, Brock and Kleissas, Dean and Gray-Roncal, William},
title = {Leveraging Tools from Autonomous Navigation for Rapid, Robust Neuron Connectivity},
year = {2020},
isbn = {978-3-030-59721-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59722-1_11},
doi = {10.1007/978-3-030-59722-1_11},
abstract = {As biological imaging datasets continue to grow in size, extracting information from large image volumes presents a computationally intensive challenge. State-of-the-art algorithms are almost entirely dominated by the use of convolutional neural network approaches that may be difficult to run at scale given schedule, cost, and resource limitations. We demonstrate a novel solution for high-resolution electron microscopy brain image volumes that permits the identification of individual neurons and synapses. Instead of conventional approaches where voxels are labelled according to the neuron or neuron segment to which they belong, we instead focus on extracting the underlying brain graph represented by synaptic connections between individual neurons, while also identifying key features like skeleton similarity and path length. This graph represents a critical step and scaffold for understanding the structure of neuronal circuitry. Our approach, which we call Agents, recasts the segmentation problem to one of path finding between keypoints (i.e., connectivity) in an information sharing framework using virtual agents. We create a family of sensors which follow local decision-making rules that perform computationally cheap operations on potential fields to perform tasks such as avoiding cell membranes and finding synapses. These enable a swarm of virtual agents to efficiently and robustly traverse three-dimensional datasets, create a sparse segmentation of pathways, and capture connectivity information. We achieve results that meet or exceed state-of-the-art performance at a substantially lower computational cost. Agents offers a categorically different approach to connectome estimation that can augment how we extract connectivity information at scale. Our method is generalizable and may be extended to biomedical imaging problems such as tracing the bronchial trees in lungs or road networks in natural images.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V},
pages = {109–118},
numpages = {10},
keywords = {Computer vision, Neuroscience, Connectomics},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59713-9_57,
author = {Gong, Lijun and Ma, Kai and Zheng, Yefeng},
title = {Distractor-Aware Neuron Intrinsic Learning for Generic 2D Medical Image Classifications},
year = {2020},
isbn = {978-3-030-59712-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59713-9_57},
doi = {10.1007/978-3-030-59713-9_57},
abstract = {Medical image analysis benefits Computer Aided Diagnosis (CADx). A fundamental analyzing approach is the classification of medical images, which serves for skin lesion diagnosis, diabetic retinopathy grading, and cancer classification on histological images. When learning these discriminative classifiers, we observe that the convolutional neural networks (CNNs) are vulnerable to distractor interference. This is due to the similar sample appearances from different categories (i.e., small inter-class distance). Existing attempts select distractors from input images by empirically estimating their potential effects to the classifier. The essences of how these distractors affect CNN classification are not known. In this paper, we explore distractors from the CNN feature space via proposing a neuron intrinsic learning method. We formulate a novel distractor-aware loss that encourages large distance between the original image and its distractor in the feature space. The novel loss is combined with the original classification loss to update network parameters by back-propagation. Neuron intrinsic learning first explores distractors crucial to the deep classifier and then uses them to robustify CNN inherently. Extensive experiments on medical image benchmark datasets indicate that the proposed method performs favorably against the state-of-the-art approaches.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part II},
pages = {591–601},
numpages = {11},
keywords = {Medical Image Classification, Distractor-awareness, Neuron Intrinsic Learning},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59710-8_76,
author = {Huang, Ziyi and Gan, Yu and Lye, Theresa and Zhang, Haofeng and Laine, Andrew and Angelini, Elsa D. and Hendon, Christine},
title = {Heterogeneity Measurement of Cardiac Tissues Leveraging Uncertainty Information from Image Segmentation},
year = {2020},
isbn = {978-3-030-59709-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59710-8_76},
doi = {10.1007/978-3-030-59710-8_76},
abstract = {Identifying arrhythmia substrates and quantifying their heterogeneity has great potential to provide critical guidance for radio frequency ablation. However, quantitative analysis of heterogeneity on cardiac optical coherence tomography (OCT) images is lacking. In this paper, we conduct the first study on quantifying cardiac tissue heterogeneity from human OCT images. Our proposed method applies a dropout-based Monte Carlo sampling technique to measure the model uncertainty. The heterogeneity information is extracted by decoupling the intra/inter-tissue heterogeneity and tissue boundary uncertainty from the uncertainty measurement. We empirically demonstrate that our model can highlight the subtle features from OCT images, and the heterogeneity information extracted is positively correlated with the tissue heterogeneity information from corresponding histology images.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I},
pages = {782–791},
numpages = {10},
keywords = {Heterogeneity., Deep learning, Optical coherence tomography},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-59710-8_6,
author = {Ye, Jiarong and Xue, Yuan and Long, L. Rodney and Antani, Sameer and Xue, Zhiyun and Cheng, Keith C. and Huang, Xiaolei},
title = {Synthetic Sample Selection via Reinforcement Learning},
year = {2020},
isbn = {978-3-030-59709-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59710-8_6},
doi = {10.1007/978-3-030-59710-8_6},
abstract = {Synthesizing realistic medical images provides a feasible solution to the shortage of training data in deep learning based medical image recognition systems. However, the quality control of synthetic images for data augmentation purposes is under-investigated, and some of the generated images are not realistic and may contain misleading features that distort data distribution when mixed with real images. Thus, the effectiveness of those synthetic images in medical image recognition systems cannot be guaranteed when they are being added randomly without quality assurance. In this work, we propose a reinforcement learning (RL) based synthetic sample selection method that learns to choose synthetic images containing reliable and informative features. A transformer based controller is trained via proximal policy optimization (PPO) using the validation classification accuracy as the reward. The selected images are mixed with the original training data for improved training of image recognition systems. To validate our method, we take the pathology image recognition as an example and conduct extensive experiments on two histopathology image datasets. In experiments on a cervical dataset and a lymph node dataset, the image classification performance is improved by 8.1% and 2.3%, respectively, when utilizing high-quality synthetic images selected by our RL framework. Our proposed synthetic sample selection method is general and has great potential to boost the performance of various medical image recognition systems given limited annotation.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I},
pages = {53–63},
numpages = {11},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-79478-1_19,
author = {Liu, Wanqi and Li, Yewen and Zang, Dawei and Tan, Guangming},
title = {FEB3D: An Efficient FPGA-Accelerated Compression Framework for Microscopy Images},
year = {2020},
isbn = {978-3-030-79477-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79478-1_19},
doi = {10.1007/978-3-030-79478-1_19},
abstract = {With the rapid development of fluorescence microscope technologies, high-content screening and light-sheet microscopy are producing ever-larger datasets that pose great challenges in data storing and data sharing. As a popular compression tool, B3D introduces a noise dependent compression algorithm for microscopy images to preserve the numerical intensities of all pixel within their uncertainties by exploiting the natural variability of each pixel value. Nevertheless, the high complexity of the processing flow restricts the deployment of the tool since the throughput and power consumption cannot satisfy the increasing demand. In this paper, we propose an efficient FPGA-accelerated data compression framework based on B3D. Following the co-design methodology, the compression processing flows are partitioned into different blocks to deploy on CPU or FPGA according to their computation characteristics. Also, we design a custom accelerator core that consists of multiple full on-chip pipelines using the channel function of the Intel OpenCL toolkit to implement data-flow driven computation. Our experiments show that the proposed framework achieves up&nbsp;to 32\texttimes{} throughput for a single pipeline compared with Intel Xeon E3-1220 v5 operating at 3.00&nbsp;GHz, and 6\texttimes{} energy-efficiency compared with GPU implementation.},
booktitle = {Network and Parallel Computing: 17th IFIP WG 10.3 International Conference, NPC 2020, Zhengzhou, China, September 28–30, 2020, Revised Selected Papers},
pages = {217–230},
numpages = {14},
keywords = {Throughput, FPGA, Data compression},
location = {Zhengzhou, China}
}

@inproceedings{10.1109/ITSC45102.2020.9294474,
author = {Henning, Sven Mertin Ne and Malena, Kevin and Link, Christopher and Gausemeier, Sandra and Ansgar},
title = {Macroscopic Traffic Flow Control using Consensus Algorithms},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ITSC45102.2020.9294474},
doi = {10.1109/ITSC45102.2020.9294474},
abstract = {In previous researches, a new approach in the field of traffic flow control using consensus algorithms was studied by using microscopic traffic simulations and led to promising results. However, these studies based on some assumptions and uncertainties within the microscopic traffic model since the control variables are defined within the domain of macroscopic traffic values and therefore have to be appropriately converted into the domain of microscopic traffic values for analysis. In contrast to this, in this work an analysis of the consensus-based traffic flow control approach within the domain of macroscopic values only is presented. Consequently, a second order macroscopic traffic flow model with multiple extensions is developed to model a road network and to study the consensus-based control approach without needing to consider microscopic traffic model characteristics.},
booktitle = {2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)},
pages = {1–8},
numpages = {8},
location = {Rhodes}
}

@inproceedings{10.1109/ITSC45102.2020.9294455,
author = {Yang, Huan and Wang, Yu and Zhao, Han and Zhu, Jinlin and Wang, Danwei},
title = {Real-time Traffic Incident Detection Using an Autoencoder Model},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ITSC45102.2020.9294455},
doi = {10.1109/ITSC45102.2020.9294455},
abstract = {Traffic flow data collected by loop detectors have been widely used for traffic incident detection. As traffic flow data have strong spatial-temporal correlations, this study tries to detect traffic incidents using an unsupervised learning approach. In this paper, a novel automatic incident detection (AID) method based on Autoencoder (AE) is proposed to detect the occurrence time and the location of traffic incidents in both freeway and urban networks. AE is an unsupervised machine learning model, which extracts nonlinear features of traffic flow data. A statistic named Squared Prediction Error (SPE) is constructed for incident detection. Meanwhile, the contribution plot technique is applied for incident localization. The experiments are conducted via a microscopic simulation platform Vissim and the test results verify the efficiency and effectiveness of the proposed method.},
booktitle = {2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)},
pages = {1–6},
numpages = {6},
location = {Rhodes}
}

@inproceedings{10.1145/3372224.3419208,
author = {Wang, Song and Huang, Jingqi and Zhang, Xinyu},
title = {Demystifying millimeter-wave V2X: towards robust and efficient directional connectivity under high mobility},
year = {2020},
isbn = {9781450370851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372224.3419208},
doi = {10.1145/3372224.3419208},
abstract = {Millimeter-wave (mmWave) networking represents a core technology to meet the demanding bandwidth requirements of emerging connected vehicles. However, the feasibility of mmWave vehicle-to-everything (V2X) connectivity has long been questioned. One major doubt lies in how the highly directional mmWave links can sustain under high mobility. In this paper, we present the first comprehensive reality check of mmWave V2X networks. We deploy an experimental testbed to mimic a typical mmWave V2X scenario, and customize a COTS mmWave radio to enable microscopic investigation of the channel and the link. We further construct a high-fidelity 3D ray-tracer to reproduce the mmWave characteristics at scale. With this toolset, we study the mmWave V2X coverage, mobility and blockage, codebook/beam management, and spatial multiplexing. Our measurement debunks some common misperceptions of mmWave V2X networks. In particular, due to the constrained roadway network structures, we find the beam management can be handled easily by the often-denounced beam scanning schemes, as long as the codebook is properly designed. Blockage can be almost eliminated through proper basestation deployment and cooperation. Highly effective spatial multiplexing can be realized even without sophisticated MIMO radios. Our work points to possible ways to realize efficient and reliable mmWave networks under high mobility, while maintaining the simplicity of standard network protocols.},
booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
articleno = {51},
numpages = {14},
keywords = {mmWave, millimeter-wave networks, beam management, V2X, MU-MIMO, 5G},
location = {London, United Kingdom},
series = {MobiCom '20}
}

@inproceedings{10.1145/3397391.3397409,
author = {Yumang, Analyn N. and Lazaro, Jose B. and Almiranez, Denise Daine C.},
title = {Detection and Classification of Gram-negative Bacteria in Water Samples Using Viola-Jones Algorithm},
year = {2020},
isbn = {9781450377249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397391.3397409},
doi = {10.1145/3397391.3397409},
abstract = {Enterobacteriaceae is a family of gram-negative bacteria which can be found in water samples. Some of its strains causes diseases to human body. In this study, the researcher employed the Viola-Jones algorithm to detect the shape and classify Enterobacteriaceae in a sample. A Raspberry Pi camera attached to a compound microscope was used to capture the image; the software utilized was Python, a powerful programming language optimized for programmers for high quality use. Confusion matrix was used to check the accuracy of the data obtained from the proposed system with the actual system (laboratory). Under random sampling, three trials were made in 10 samples. The results showed an accuracy of 84.44% for detecting the shape of the bacteria, and 84.44% accuracy in classifying the family. Therefore, the proposed system can detect the shape of bacteria and classify its family using Viola-Jones Algorithm.},
booktitle = {Proceedings of the 2020 10th International Conference on Biomedical Engineering and Technology},
pages = {6–10},
numpages = {5},
keywords = {Viola-Jones algorithm, Raspberry Pi camera, Enterobacteriaceae, Confusion matrix},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {ICBET '20}
}

@inproceedings{10.1007/978-3-030-61616-8_67,
author = {Axenie, Cristian and Kurz, Daria},
title = {Tumor Characterization Using Unsupervised Learning of Mathematical Relations Within Breast Cancer Data},
year = {2020},
isbn = {978-3-030-61615-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61616-8_67},
doi = {10.1007/978-3-030-61616-8_67},
abstract = {Despite the variety of imaging, genetic and histopathological data used to assess tumors, there is still an unmet need for patient-specific tumor growth profile extraction and tumor volume prediction, for use in surgery planning. Models of tumor growth predict tumor size and require tumor biology-dependent parametrization, which hardly generalizes to cope with tumor variability among patients. In addition, the datasets are limited in size, owing to the restricted or single-time measurements. In this work, we address the shortcomings that incomplete biological specifications, the inter-patient variability of tumors, and the limited size of the data bring to mechanistic tumor growth models. We introduce a machine learning model that alleviates these shortcomings and is capable of characterizing a tumor’s growth pattern, phenotypical transitions, and volume. The model learns without supervision, from different types of breast cancer data the underlying mathematical relations describing tumor growth curves more accurate than three state-of-the-art models. Experiments performed on publicly available clinical breast cancer datasets, demonstrate the versatility of the approach among breast cancer types. Moreover, the model can also, without modification, learn the mathematical relations among, for instance, histopathological and morphological parameters of the tumor and, combined with the growth curve, capture the (phenotypical) growth transitions of the tumor from a small amount of data. Finally, given the tumor growth curve and its transitions, our model can learn the relation among tumor proliferation-to-apoptosis ratio, tumor radius, and tumor nutrient diffusion length, used to estimate tumor volume. Such a quantity can be readily incorporated within current clinical practice, for surgery planning.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15–18, 2020, Proceedings, Part II},
pages = {838–849},
numpages = {12},
keywords = {Prediction algorithms, Unsupervised learning, Breast cancer, Artificial neural networks},
location = {Bratislava, Slovakia}
}

@inproceedings{10.1145/3411408.3411435,
author = {Kallipolitis, Athanasios and Stratigos, Alexandros and Zarras, Alexios and Maglogiannis, Ilias},
title = {Explainable Fully Connected Visual Words for the Classification of Skin Cancer Confocal Images: Interpreting the influence of visual words in classifying benign vs malignant pattern},
year = {2020},
isbn = {9781450388788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411408.3411435},
doi = {10.1145/3411408.3411435},
abstract = {Skin cancer is affecting the lives of million people worldwide. Early detection and treatment of the cause can reduce drastically morbidity. Although the main workflow in dermatology clinics includes invasive skin removal procedures for diagnostic purposes, Reflectance Confocal Microscopy (RCM) provides an ancillary, non-invasive methodology for reviewing areas of interest of the human skin at a high resolution. In this paper, we propose a method for the classification and the interpretation of visual patterns in skin cancer confocal images. Both tasks are based on the formation of a visual vocabulary from Speeded up Robust Features (SURF) and the utilization of simple shallow artificial neural network with fully connected layers. Interpretability of the predictive models is also quite important, since it improves their reliability, accountability, transparency and provides useful insight of how to evolve the predictive model towards better performance. The paper discusses the technical details of both approaches along with some initial results.},
booktitle = {11th Hellenic Conference on Artificial Intelligence},
pages = {67–73},
numpages = {7},
keywords = {Skin Cancer, Reflectance Confocal Microscopy, Interpretability, Bag of Visual Words},
location = {Athens, Greece},
series = {SETN 2020}
}

@inproceedings{10.1007/978-3-030-58595-2_18,
author = {Mais, Lisa and Hirsch, Peter and Kainmueller, Dagmar},
title = {PatchPerPix for Instance Segmentation},
year = {2020},
isbn = {978-3-030-58594-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58595-2_18},
doi = {10.1007/978-3-030-58595-2_18},
abstract = {We present a novel method for proposal free instance segmentation that can handle sophisticated object shapes which span large parts of an image and form dense object clusters with crossovers. Our method is based on predicting dense local shape descriptors, which we assemble to form instances. All instances are assembled simultaneously in one go. To our knowledge, our method is the first non-iterative method that yields instances that are composed of learnt shape patches. We evaluate our method on a diverse range of data domains, where it defines the new state of the art on four benchmarks, namely the ISBI 2012 EM segmentation benchmark, the BBBC010 C. elegans dataset, and 2d as well as 3d fluorescence microscopy data of cell nuclei. We show furthermore that our method also applies to 3d light microscopy data of Drosophila neurons, which exhibit extreme cases of complex shape clusters.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV},
pages = {288–304},
numpages = {17},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1007/978-3-030-58539-6_13,
author = {Wolf, Steffen and Li, Yuyan and Pape, Constantin and Bailoni, Alberto and Kreshuk, Anna and Hamprecht, Fred A.},
title = {The Semantic Mutex Watershed for Efficient Bottom-Up Semantic Instance Segmentation},
year = {2020},
isbn = {978-3-030-58538-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58539-6_13},
doi = {10.1007/978-3-030-58539-6_13},
abstract = {Semantic instance segmentation is the task of simultaneously partitioning an image into distinct segments while associating each pixel with a class label. In commonly used pipelines, segmentation and label assignment are solved separately since joint optimization is computationally expensive. We propose a greedy algorithm for joint graph partitioning and labeling derived from the efficient Mutex Watershed partitioning algorithm. It optimizes an objective function closely related to the Asymmetric Multiway Cut objective and empirically shows efficient scaling behavior. Due to the algorithm’s efficiency it can operate directly on pixels without prior over-segmentation of the image into superpixels. We evaluate the performance on the Cityscapes dataset (2D urban scenes) and on a 3D microscopy volume. In urban scenes, the proposed algorithm combined with current deep neural networks outperforms the strong baseline of ‘Panoptic Feature Pyramid Networks’ by Kirillov et al. (2019). In the 3D electron microscopy images, we show explicitly that our joint formulation outperforms a separate optimization of the partitioning and labeling problems.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI},
pages = {208–224},
numpages = {17},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3408066.3408070,
author = {Fa, Zhang and Song-Chun, Wang and Zhi-Hua, Song},
title = {Civilian Characteristics and Casualties Of Terrorist Attacks in Public Places},
year = {2020},
isbn = {9781450377034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408066.3408070},
doi = {10.1145/3408066.3408070},
abstract = {The crowds in public gathering places are densely populated. Once a violent terrorist attack occurs, it may cause a large number of civilian casualties. There are few researches on the dynamics of crowd in terrorist attacks, and the relationship between civilian characteristics and the consequences of attacks is unclear. We built an agent-based model of terrorist attacks in public gathering places, and explore the impact of the microscopic characteristics of civilians on the consequences of events. Firstly, we proposed a conceptual framework of terrorist attacks event occurred in public places. Then we constructed the agent models of two main groups: civilians and terrorists. We developed a multi-agent simulation system to investigate the dynamic of crowd under attacks in public space. Finally, we simulated the terrorist attacks in a city square to explore the relationship between civilian characteristics and the consequences of the event. The simulation results show that the observation range, risk sensitivity and emergency movement speed of civilians have a significant impact on the number of casualties.},
booktitle = {Proceedings of the 12th International Conference on Computer Modeling and Simulation},
pages = {109–114},
numpages = {6},
keywords = {Terrorist attack, Simulation, Public space, Civilian},
location = {Brisbane, QLD, Australia},
series = {ICCMS '20}
}

@inproceedings{10.1007/978-3-030-58814-4_65,
author = {Dirvanauskas, Darius and Maskeli\={u}nas, Rytis and Raudonis, Vidas and Misra, Sanjay},
title = {Embryo Spatial Model Reconstruction},
year = {2020},
isbn = {978-3-030-58813-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58814-4_65},
doi = {10.1007/978-3-030-58814-4_65},
abstract = {Time lapse microscopy offered new solutions to study embryo development process. It allows embryologist to monitor embryo growth in real time and evaluate them without interfering into their growth environment. Embryo evaluation during growth process is one of the key criteria in embryo selection for fertilization. Live embryo monitoring is time consuming and new tools are offered to automate part of process. Our proposed algorithm gives new possibilities for embryo monitoring. It uses embryo images which are taken from different embryo layers, extracts embryo cell features and returns metrical evaluation to compare different embryos. High number of extracted features shows embryo fragmentation. Other tool which we present is spatial embryo model. Features extracted from embryo layers are combined together to spatial model. It allows embryologist to examine embryo model and compare different layers in one space. The obtained spatial embryo model will be later used to develop new algorithms for embryo analysis tasks.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part V},
pages = {772–780},
numpages = {9},
keywords = {Feature extraction, Spatial model, Image analysis},
location = {Cagliari, Italy}
}

@inproceedings{10.5555/3408352.3408737,
author = {Chang, Wanli and Roy, Debayan and Zhao, Shuai and Annaswamy, Anuradha and Chakraborty, Samarjit},
title = {CPS-oriented modeling and control of traffic signals using adaptive back pressure},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Modeling and design of automotive systems from a cyber-physical system (CPS) perspective have lately attracted extensive attention. As the trend towards automated driving and connectivity accelerates, strong interactions between vehicles and the infrastructure are expected. This requires modeling and control of the traffic network in a similarly formal manner. Modeling of such networks involves a tradeoff between expressivity of the appropriate features and tractability of the control problem. Back-pressure control of traffic signals is gaining ground due to its decentralized implementation, low computational complexity, and no requirements on prior traffic information. It guarantees maximum stability under idealistic assumptions. However, when deployed in real traffic intersections, the existing back-pressure control algorithms may result in poor junction utilization due to (i) fixed-length control phases; (ii) stability as the only objective; and (iii) obliviousness to finite road capacities and empty roads. In this paper, we propose a CPS-oriented model of traffic intersections and control of traffic signals, aiming to address the utilization issue of the back-pressure algorithms. We consider a more realistic model with transition phases and dedicated turning lanes, the latter influencing computation of the pressure and subsequently the utilization. The main technical contribution is an adaptive controller that enables varying-length control phases and considers both stability and utilization, while taking both cases of full roads and empty roads into account. We implement a mechanism to prevent frequent changes of control phases and thus limit the number of transition phases, which have negative impact on the junction utilization. Microscopic simulation results with SUMO on a 3 \texttimes{} 3 traffic network under various traffic patterns show that the proposed algorithm is at least about 13% better in performance than the existing fixed-length backpressure control algorithms reported in previous works. This is a significant improvement in the context of traffic signal control.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {1686–1691},
numpages = {6},
location = {<conf-loc>, <city>Grenoble</city>, <country>France</country>, </conf-loc>},
series = {DATE '20}
}

@inproceedings{10.1145/3390557.3394124,
author = {Peng, Zhongbo and Shi, Peng and Deng, Yu},
title = {Corrosion Rate of Ni-ZrO2 Nano-coatings Forecasted by GRNN},
year = {2020},
isbn = {9781450376587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3390557.3394124},
doi = {10.1145/3390557.3394124},
abstract = {The Ni-ZrO2 nano-coating was prepared on the surface of a cobalt-nickel-based model alloy sample by the magnetic stirring-assisted electrodeposition. Based on the orthogonal experiment, the electrochemical free corrosion current density of the Ni-ZrO2 nano-coating was performed by GRNN to predict the corrosion rate of Ni-ZrO2 nano-coating. The surface morphology and the composition of elements of the Ni-ZrO2 nano-coating were characterized using S-3400 scanning electron microscopy (SEM) equipped with an energy dispersive spectrometer (EDS) and MFP-3D Origin atomic force microscopy (AFM). The results show that: (1) The performance of the Ni-ZrO2 nano-coating is better and shows a lower corrosion rate when the mass concentration of ZrO2 particles is 6 g/L, the plating bath temperature is 60 °C, and the plating current density is 5 A/dm2; (2) The factors affecting the free corrosion current density of Ni-ZrO2 coatings are as follows: ZrO2 particles mass concentration &gt; plating bath temperature &gt; plating current density; (3) The maximum and minimum relative errors of the four sets of non-orthogonal experiments predicted by the GRNN are 7.33% and 3.91%, respectively, which attests to the accuracy of GRNN prediction.},
booktitle = {Proceedings of the 2020 the 4th International Conference on Innovation in Artificial Intelligence},
pages = {236–241},
numpages = {6},
keywords = {Ni-ZrO2 nano-coating, GRNN, Free corrosion current density, Electrodeposition},
location = {Xiamen, China},
series = {ICIAI '20}
}

@inproceedings{10.1109/MeMeA49120.2020.9137212,
author = {Mencattini, Arianna and Di Giuseppe, Davide and D'Orazio, Michele and Rizzuto, Valeria and Ma\~{n}\'{u} Pereira, M. M. and Colomba Comes, Maria and Lopez-Martinez, Maria Jos\'{e} and Samitier, Josep and Martinelli, Eugenio},
title = {A microfluidic device for shape measurement in red blood cells (RBCs)},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MeMeA49120.2020.9137212},
doi = {10.1109/MeMeA49120.2020.9137212},
abstract = {Modern optical sensors coupled with time-lapse microscopy devices and dedicated software tools allow the miniaturization of laboratories for biological experiments leading to the Organ-On-Chip (OoC) framework. OoCs allow performing massive measurements on a large number of cells under the assumption of reproducibility conditions, permitting to investigate the cell dynamics in terms of motility and shape changes over time. In this work, we present the OoC platform used in a preliminary study of the Rare Haemolytic Anaemia (RHA) disease, a group of rare diseases characterized by haemolysis, which is the premature loss of red blood cells (RBCs). Preliminary results demonstrate the effectiveness of shape measurement for the diagnosis of RHA.},
booktitle = {2020 IEEE International Symposium on Medical Measurements and Applications (MeMeA)},
pages = {1–5},
numpages = {5},
location = {Bari, Italy}
}

@inproceedings{10.1145/3396743.3396769,
author = {Yuan, Gao and Xiao, Bai and Lianfang, Lu},
title = {Analysis of Influencing Factors of Farmers' Satisfaction with Industrial Poverty Alleviation Effect Based on Ordinal Logistic Model: Take the three prefectures of southern Xinjiang in China as an example},
year = {2020},
isbn = {9781450377065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396743.3396769},
doi = {10.1145/3396743.3396769},
abstract = {Based on a sample survey of 600 farmers in the three prefectures of southern Xinjiang, China, this paper uses questionnaire survey and ordinal Logistic model to study the influencing factors of farmers' satisfaction with the effect of industrial poverty alleviation from a microscopic perspective. The research shows that the cumulative percentage of farmers' satisfaction with the effect of industrial poverty alleviation is 90.7%, and the factors that influence the evaluation of farmers' satisfaction with industrial poverty alleviation are the educational level, the amount of household labor, the annual household income, the types of roads in front of farmers' doors, household fuel, whether rural cadres help farmers to alleviate poverty and the main poverty alleviation methods of the government. Based on this, the corresponding countermeasures are put forward: to continue to improve the embodiment of precision poverty alleviation, to strengthen the intensity of industrial precision poverty alleviation, to increase vocational skills training, to speed up infrastructure construction, based on its own advantages, to develop characteristic industries, to realize the precision poverty alleviation of industries, and to enhance the satisfaction of farmers with industrial poverty alleviation.},
booktitle = {Proceedings of the 2020 2nd International Conference on Management Science and Industrial Engineering},
pages = {145–152},
numpages = {8},
keywords = {Ordinal Logistic Model, Influencing Factors, Industrial Poverty Alleviation},
location = {Osaka, Japan},
series = {MSIE '20}
}

@inproceedings{10.1109/I2MTC43012.2020.9128388,
author = {Tang, Jiawei and Lu, Mingyang and Yin, Wuliang and Xie, Yuedong and Zhang, Zhijie and Zhao, Qian},
title = {Effect of frozen-thaw injury on cell membrane and bio-impedance},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/I2MTC43012.2020.9128388},
doi = {10.1109/I2MTC43012.2020.9128388},
abstract = {Biological samples exhibit frequency dependent spectra caused by a dispersion mechanism. This dispersion mechanism demonstrates dielectric relaxation due to the interaction between electromagnetic field and biological samples at cellular levels. Hence, biological impedance spectroscopy may be used to reveal the electrical and geometrical properties of biological samples, in particular, frozen-thaw injury. Frozen-thaw injury is known as one of the most common factors that can influence the bio-impedance spectroscopy of biological samples. However, the mechanism of how frozen-thaw injury influences the bio-impedance spectroscopy at cellular levels has not been analysed. In this paper, the influence of frozen-thaw injury on beta dispersion was experimentally investigated using the AC conduction (contact electrode) method on potato and pork samples. From the results of the experiment, we assumed that frozen-thaw injury mainly influences the impedance spectroscopy of a potato and pork by breaking their cell membranes. In light of this assumption, a novel FEM model to simulate membrane breakage was developed and a microscopic experiment was then carried out to identify the membrane integrity. In this paper, the influence of frozen-thaw injury on dielectric properties of biological cells suspension was simulated using a custom developed FEM solver and an originally designed cell model. In its 2D version, the AC conduction case was simulated. Then, in the attempt to confirm the assumption, a microscopic experiment was conducted to determine if the cell membrane was broken or not. The measurement and simulation results suggest that bio-impedance measurements provide an indication of cellular structural changes of biological samples, which could be useful for biomedical, pharmaceutical and food inspection applications.},
booktitle = {2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)},
pages = {1–6},
numpages = {6},
location = {Dubrovnik, Croatia}
}

@inproceedings{10.1007/978-3-030-45385-5_18,
author = {Ulag, Songul and Ilhan, Elif and Aksu, Burak and Sengor, Mustafa and Ekren, Nazmi and Kilic, Osman and Gunduz, Oguzhan},
title = {Patch-Based Technology for Corneal Microbial Keratitis},
year = {2020},
isbn = {978-3-030-45384-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45385-5_18},
doi = {10.1007/978-3-030-45385-5_18},
abstract = {Corneal opacities, which happened mainly due to microbial keratitis, are the fourth cause of blindness worldwide. Antimicrobial therapy is an alternative solution for microbial keratitis caused by Staphylococcus aureus and Pseudomonas Aeruginosa. The aim of this study, to develop patches for the treatment of corneal keratitis which caused significant corneal blindness by using electrospinning method. Polyvinyl-alcohol (PVA) patches with Gelatine (GEL) studied in various ratios. Different amounts of gelatine added to PVA to resemble the collagen fibril structure of the cornea. To enable the patches to the antimicrobial effect against the bacterias, the special plant extract was used. The produced corneal patches were examined separately for chemical, morphological, and antimicrobial properties. Scanning electron microscope (SEM), Fourier-transform infrared (FT-IR) spectroscopy were performed to observe the surface morphology and chemical structure of the patches, respectively.},
booktitle = {Bioinformatics and Biomedical Engineering: 8th International Work-Conference, IWBBIO 2020, Granada, Spain, May 6–8, 2020, Proceedings},
pages = {194–200},
numpages = {7},
keywords = {Propolis, Nanofiber patches, Electrospinning, Corneal distropy, Bacterial keratitis},
location = {Granada, Spain}
}

@inproceedings{10.1007/978-3-030-45385-5_17,
author = {Cesur, Sumeyye and Cam, Muhammet Emin and Say\i{}n, Fatih Serdar and Su, Sena and Gunduz, Oguzhan},
title = {Controlled Release of Metformin Loaded Polyvinyl Alcohol (PVA) Microbubble/Nanoparticles Using Microfluidic Device for the Treatment of Type 2 Diabetes Mellitus},
year = {2020},
isbn = {978-3-030-45384-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45385-5_17},
doi = {10.1007/978-3-030-45385-5_17},
abstract = {Nowadays it became obvious that a relentless increase in Type 2 diabetes mellitus (T2DM), affecting the economically affluent countries, is gradually afflicting also the developing world. The currently used drugs in the treatment of T2DM have inefficient glucose control and carry serious side effects. In this study, nano-sized uniform particles were produced by microfluidic method by the explosion of microbubbles. Morphological (SEM), molecular interactions between the components (FT-IR), drug release test by UV spectroscopy measurement were carried out after production process. When microbubbles and nanoparticles, optical microscope and SEM images obtained were examined, it was observed that metformin was successfully loaded into nanoparticles. The diameter of the microbubbles and nanoparticles was 104 ± 91&nbsp;µm and 116 ± 13&nbsp;nm, respectively. Metformin was released in a controlled manner at pH 1.2 for 390&nbsp;min. It is promising in the treatment of T2DM with the controlled release ability of metformin loaded nonoparticles.},
booktitle = {Bioinformatics and Biomedical Engineering: 8th International Work-Conference, IWBBIO 2020, Granada, Spain, May 6–8, 2020, Proceedings},
pages = {185–193},
numpages = {9},
keywords = {Drug delivery, T-junction, Nanoparticle, Microbubble, Diabetes mellitus},
location = {Granada, Spain}
}

@inproceedings{10.1145/3380678.3380687,
author = {Yin, Da and Ma, Jun and Yuan, Caojin},
title = {The PCA Algorithm in Digital Holographic Microscopy under Structured Illumination},
year = {2020},
isbn = {9781450376396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380678.3380687},
doi = {10.1145/3380678.3380687},
abstract = {Principal component analysis (PCA) is a method of processing high-dimensional feature data, which can be decomposed into a set of unrelated variables called principal components. Digital holographic microscopy (DHM) is a powerful tool in the biomedical imaging for recording the amplitude and phase information of object simultaneously. Structured illumination (SI) has been introduced in DHM to improve the resolution, by which the resolution can be doubled. However, accurate phase-shifting is required to retrieve the low and high frequency information. Besides that, the aberration of imaging system makes DHM under SI cumbersome. This paper presents an algorithm based on PCA for DHM under SI. The aberration terms can be extracted from the first principal component of the exponential term of filtered hologram. Moreover, the low and high frequency information can be achieved from three images without prior knowledge of phase shift values. In synthesizing process, the spectrums are precisely shifted to the correct position in the spatial-frequency domain also based on the PCA. This paper verifies the feasibility of the PCA algorithm the experiment. It is an attractive and promising technology for DHM under SI.},
booktitle = {Proceedings of the 2019 International Communication Engineering and Cloud Computing Conference},
pages = {46–48},
numpages = {3},
keywords = {spectrum shift, aberration compensation, SI, PCA, DHM},
location = {Prague, Czech Republic},
series = {CECCC 2019}
}

@inproceedings{10.1145/3341161.3342900,
author = {Top\^{\i}rceanu, Alexandru and Precup, Radu-Emil},
title = {A novel methodology for improving election poll prediction using time-aware polling},
year = {2020},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3342900},
doi = {10.1145/3341161.3342900},
abstract = {Multiple poll forecasting solutions, based on statistics and economic indices, have been proposed over time, but, as we better understand diffusion phenomena, we know that temporal characteristics provide even more uncertainty. As such, current literature is not yet able to define truly reliable models for the evolution of political opinion, marketing preferences, or social unrest. Inspired by micro-scale opinion dynamics, we develop an original time-aware (TA) methodology which is able to improve the prediction of opinion distribution, by modeling opinion as a function which spikes up when opinion is expressed, and slowly dampens down otherwise. After a parametric analysis, we validate our TA method on survey data from the US presidential elections of 2012 and 2016. By comparing our time-aware method (TA) with classic survey averaging (SA), and cumulative vote counting (CC), we find our method is substantially closer to the real election outcomes. On average, we measure that SA is 6.3% off, CC is 5.6% off, while TA is only 1.5% off from the final registered election outcomes; this difference translates into an ≈ 75% prediction improvement of our TA method. As our work falls in line with studies on the microscopic temporal dynamics of social networks, we find evidence of how macroscopic prediction can be improved using time-awareness.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {282–285},
numpages = {4},
keywords = {time-aware, social network mining, opinion prediction, election polls},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1109/ITSC55140.2022.9922249,
author = {Naderi, Mehdi and Papageorgiou, Markos and Karafyllis, Iasson and Papamichail, Ioannis},
title = {Automated vehicle driving on large lane-free roundabouts&lt;sup&gt;*&lt;/sup&gt;},
year = {2022},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ITSC55140.2022.9922249},
doi = {10.1109/ITSC55140.2022.9922249},
abstract = {Automated vehicle driving on large, complex, lane-free roundabouts is a major challenge. As a striking example for this challenge, we consider the famous roundabout of Place Charles de Gaulle in Paris, featuring a width of 38 &lt;tex&gt;$m$&lt;/tex&gt; and comprising a dozen of entering/exiting radial streets. The paper proposes a complete generic methodology to control the lane-free paths of automated vehicles. The developed real-time vehicle movement control strategy relies on appropriate automated offline computation of: (a) wide overlapping movement corridors, one for each Origin-Destination (OD) movement, which delineate the admissible movement zones of corresponding OD vehicles; (b) desired vehicle orientation at each location within each OD corridor. Real-time vehicle movement within the respective corridor is effectuated by a distributed (per vehicle) nonlinear feedback control strategy, such that vehicles can move forward efficiently, accounting, when possible, for the pre-specified desired orientation, while avoiding collisions with other vehicles. Boundary controllers, developed based on linear state-feedback approaches, are used as safety filters defining upper and lower bounds for the vehicle steering angle, such that it is guaranteed that: a vehicle never violates its admissible corridor and roundabout boundaries; and never misses its exit. Microscopic simulation testing results demonstrate the pertinence and effectiveness of the suggested approach.},
booktitle = {2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)},
pages = {1528–1535},
numpages = {8},
location = {Macau, China}
}

@inproceedings{10.1007/978-3-030-58595-2_8,
author = {Cheng, Hsien-Tzu and Yeh, Chun-Fu and Kuo, Po-Chen and Wei, Andy and Liu, Keng-Chi and Ko, Mong-Chi and Chao, Kuan-Hua and Peng, Yu-Ching and Liu, Tyng-Luh},
title = {Self-similarity Student for Partial Label Histopathology Image Segmentation},
year = {2020},
isbn = {978-3-030-58594-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58595-2_8},
doi = {10.1007/978-3-030-58595-2_8},
abstract = {Delineation of cancerous regions in gigapixel whole slide images (WSIs) is a crucial diagnostic procedure in digital pathology. This process is time-consuming because of the large search space in the gigapixel WSIs, causing chances of omission and misinterpretation at indistinct tumor lesions. To tackle this, the development of an automated cancerous region segmentation method is imperative. We frame this issue as a modeling problem with partial label WSIs, where some cancerous regions may be misclassified as benign and vice versa, producing patches with noisy labels. To learn from these patches, we propose Self-similarity Student, combining teacher-student model paradigm with similarity learning. Specifically, for each patch, we first sample its similar and dissimilar patches according to spatial distance. A teacher-student model is then introduced, featuring the exponential moving average on both student model weights and teacher predictions ensemble. While our student model takes patches, teacher model takes all their corresponding similar and dissimilar patches for learning robust representation against noisy label patches. Following this similarity learning, our similarity ensemble merges similar patches’ ensembled predictions as the pseudo-label of a given patch to counteract its noisy label. On the CAMELYON16 dataset, our method substantially outperforms state-of-the-art noise-aware learning methods by 5% and the supervised-trained baseline by 10% in various degrees of noise. Moreover, our method is superior to the baseline on our TVGH TURP dataset with 2% improvement, demonstrating the generalizability to more clinical histopathology segmentation tasks.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV},
pages = {117–132},
numpages = {16},
keywords = {Noisy label, Histopathology, Whole slide image},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1109/AIM43001.2020.9158963,
author = {Kaveh, Orod and Coskun, M. Bulut and Mahdavi, Mohammad and Reza Moheimani, S. O.},
title = {FPGA-Based Characterization and Q-Control of an Active AFM Cantilever},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AIM43001.2020.9158963},
doi = {10.1109/AIM43001.2020.9158963},
abstract = {The slow transient response of Si cantilevers oscillating at resonance is a significant limiting factor to achieving higher scan rates in tapping-mode atomic force microscopy (TM-AFM). During a typical tapping-mode operation in air, when the cantilever encounters a steep drop in the topography, it experiences a transient response whose time constant is a function of the resonant frequency and quality factor (Q) of the cantilever. In order to achieve accurate surface tracking at higher scan rates, it is desirable to have a high resonance frequency and a relatively low Q factor. In this work, we demonstrate an active cantilever with resonant frequency of 46 kHz, featuring a piezoelectric layer for simultaneous actuation and sensing of cantilever’s vibrations. A field programmable gate array (FPGA) is employed to mitigate the electrical feedthrough from the on-chip actuation to sensing lines for achieving a high dynamic range and also controlling the quality factor of the AFM microcantilever for higher scanning rate dynamic atomic force microscopy. After the recovery of cantilever dynamics and system identification, a dynamic range of 19.8 dB is achieved. For actively controlling the Qfactor a positive position feedback controller is implemented that results in a reduction in quality factor of the cantilever from 268 to 81.7. AFM imaging results with both uncontrolled and controlled Q factor are exhibited, demonstrating a faster response time to image surface topographies.},
booktitle = {2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)},
pages = {2062–2067},
numpages = {6},
location = {Boston, MA, USA}
}

